\documentclass{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{amsfonts}

% Custom command for the expectation operator for consistency
\newcommand{\E}{\mathbb{E}}

\title{Appendix: Sensitivity Analysis for Unmeasured Confounding}
\author{A Synthesis of Modern Bounding Factor Methods}
\date{}

\begin{document}

\maketitle

\section{Introduction and Formal Setup}

This appendix provides a formal derivation of modern sensitivity analysis techniques for unmeasured confounding. After adjusting for a vector of measured covariates $X$, an association between a treatment $D$ and an outcome $Y$ may still be biased due to one or more unmeasured confounders, which we denote collectively by $U$. Sensitivity analysis aims to quantify how strong the associations between $U$, $D$, and $Y$ would need to be to alter the study's conclusions. We synthesize the core contributions of Ding and VanderWeele (2016) and VanderWeele and Ding (2017).

\subsection{Problem Setting and Notation}

Let $D$ be a binary treatment ($D=1$ for treated, $D=0$ for control) and $Y$ be a binary outcome ($Y=1$ for event occurs, $Y=0$ for no event). Let $X$ be a vector of measured covariates, and let $U$ be a vector representing one or more unmeasured confounders.

The observed risk ratio, adjusted for measured covariates $X$, is defined within a stratum $X=x$ as:
\begin{equation}
RR_{DY|X=x}^{\text{obs}} = \frac{P(Y=1|D=1, X=x)}{P(Y=1|D=0, X=x)}
\end{equation}
For notational simplicity, we will omit the conditioning on $X=x$ in the following sections, with the understanding that all derivations are conditional on the measured covariates. The notation $RR_{DY}^{\text{obs}}$ will refer to this stratum-specific observed risk ratio.

The true causal risk ratio, adjusted for both measured ($X$) and unmeasured ($U$) confounders, is what we wish to estimate. If we assume $U$ is categorical with levels $k=1, \dots, K$, the standardized true risk ratio is:
\begin{equation}
RR_{DY}^{\text{true}} = \frac{\sum_{k=1}^{K} P(Y=1|D=1, U=k) P(U=k)}{\sum_{k=1}^{K} P(Y=1|D=0, U=k) P(U=k)}
\end{equation}

\noindent\textbf{Numerical Example:} Consider a hypothetical study examining the effect of a treatment (D) on an outcome (Y), adjusted for age group (X). Within age group $X=x$, we observe that among treated individuals ($D=1$), 120 out of 600 experienced the outcome, while among control individuals ($D=0$), 50 out of 500 experienced the outcome. This gives $P(Y=1|D=1, X=x) = 120/600 = 0.20$ and $P(Y=1|D=0, X=x) = 50/500 = 0.10$. The observed risk ratio is:
\[
RR_{DY}^{\text{obs}} = \frac{0.20}{0.10} = 2.0
\]
This observed association of $RR=2.0$ could be causal, or it could be explained (partially or fully) by an unmeasured confounder $U$. \textit{Interpretation:} The treatment is associated with a 2-fold increase in outcome risk after adjusting for age, but unmeasured confounding remains a concern.

\section{The Bounding Factor Without Assumptions}

The key innovation of modern sensitivity analysis is the derivation of a bound on the bias that does not require making assumptions about the nature of $U$ (e.g., that it is binary, has no interaction with $D$, etc.). This is achieved by parameterizing the strength of confounding in terms of maximal associations.

\subsection{Defining the Sensitivity Parameters}

We define two sensitivity parameters that capture the maximal strength of the relationships involving the unmeasured confounder(s) $U$.

\begin{enumerate}
    \item \textbf{The maximal risk ratio for the treatment-confounder relationship ($RR_{DU}$):} This parameter quantifies the largest factor by which the prevalence of any level of the unmeasured confounder(s) $U$ differs between the treated and control groups.
    \begin{equation}
    RR_{DU} = \max_{u} \frac{P(U=u | D=1)}{P(U=u | D=0)}
    \end{equation}
    This parameter captures the degree of imbalance in the unmeasured confounder across treatment groups.

    \item \textbf{The maximal risk ratio for the confounder-outcome relationship ($RR_{UY}$):} This parameter quantifies the largest factor by which any level of the unmeasured confounder(s) $U$ can increase the risk of the outcome $Y$, comparing any two levels of $U$. This maximum is taken across both the treated and control groups.
    \begin{equation}
    RR_{UY} = \max_{d, u, u'} \frac{P(Y=1 | D=d, U=u)}{P(Y=1 | D=d, U=u')}
    \end{equation}
    This parameter captures the maximum possible causal effect of the unmeasured confounder on the outcome.
\end{enumerate}

\noindent\textbf{Numerical Example:} Suppose the unmeasured confounder $U$ has 3 levels ($u_1, u_2, u_3$) with the following distributions: $P(U=u_1|D=1)=0.20$, $P(U=u_2|D=1)=0.50$, $P(U=u_3|D=1)=0.30$; and $P(U=u_1|D=0)=0.50$, $P(U=u_2|D=0)=0.40$, $P(U=u_3|D=0)=0.10$. The risk ratios for each level are: $P(U=u_1|D=1)/P(U=u_1|D=0)=0.40$, $P(U=u_2|D=1)/P(U=u_2|D=0)=1.25$, and $P(U=u_3|D=1)/P(U=u_3|D=0)=3.00$. Thus $RR_{DU} = \max(0.40, 1.25, 3.00) = 3.00$.

For the confounder-outcome relationship, suppose outcome risks are: $P(Y=1|D=1,u_1)=0.15$, $P(Y=1|D=1,u_2)=0.20$, $P(Y=1|D=1,u_3)=0.25$ for the treated, and $P(Y=1|D=0,u_1)=0.08$, $P(Y=1|D=0,u_2)=0.10$, $P(Y=1|D=0,u_3)=0.13$ for the control. Comparing all pairs within each treatment group, the maximum risk ratio is $P(Y=1|D=1,u_3)/P(Y=1|D=1,u_1) = 0.25/0.15 = 1.67$. Thus $RR_{UY} = 1.67$. \textit{Interpretation:} Level $u_3$ of the confounder is 3 times more prevalent among treated individuals, and moving from the lowest to highest confounder level increases outcome risk by 67\%.

\subsection{Derivation of the Bounding Factor}

As proven by Ding and VanderWeele (2016), the ratio of the observed risk ratio to the true causal risk ratio is bounded by a function of these two sensitivity parameters.
\begin{equation}
\frac{RR_{DY}^{\text{obs}}}{RR_{DY}^{\text{true}}} \le \frac{RR_{DU} \times RR_{UY}}{RR_{DU} + RR_{UY} - 1}
\end{equation}
This inequality holds regardless of the dimensionality or nature of $U$. The term on the right is the **joint bounding factor** for the bias.

Rearranging this gives a lower bound for the true causal risk ratio:
\begin{equation}
RR_{DY}^{\text{true}} \ge \frac{RR_{DY}^{\text{obs}}}{\left( \frac{RR_{DU} \times RR_{UY}}{RR_{DU} + RR_{UY} - 1} \right)}
\end{equation}

\noindent\textbf{Numerical Example:} Using the observed $RR_{DY}^{\text{obs}} = 2.00$ from the first example and the sensitivity parameters $RR_{DU} = 3.00$ and $RR_{UY} = 1.67$ from the second example, the bounding factor is:
\[
BF = \frac{RR_{DU} \times RR_{UY}}{RR_{DU} + RR_{UY} - 1} = \frac{3.00 \times 1.67}{3.00 + 1.67 - 1} = \frac{5.00}{3.67} = 1.36
\]
The bias-corrected lower bound for the true causal risk ratio is:
\[
RR_{DY}^{\text{true}} \ge \frac{2.00}{1.36} = 1.47
\]
\textit{Interpretation:} Even in the presence of an unmeasured confounder with strengths $RR_{DU}=3.00$ and $RR_{UY}=1.67$, the true causal effect cannot be less than 1.47, meaning the association remains substantially elevated above the null ($RR=1$) and cannot be entirely explained by this degree of confounding.

\subsection*{Implications}
This powerful result allows a researcher to calculate a ``worst-case'' corrected estimate for the causal effect, given assumed maximal strengths of unmeasured confounding. For any specified values of $RR_{DU}$ and $RR_{UY}$, an unmeasured confounder of that strength cannot possibly reduce the true risk ratio below this bound. This holds true without any simplifying assumptions about the confounder.

\section{The E-Value: A Standardized Metric for Sensitivity Analysis}

The E-value, proposed by VanderWeele and Ding (2017), translates the bounding factor into a single, intuitive metric that is easy to calculate and report.

\subsection{Defining the E-Value}

The E-value answers the question: ``What is the minimum strength of association (on the risk ratio scale) that an unmeasured confounder would need to have with both the treatment and the outcome to fully explain away the observed association?''

To ``fully explain away'' an association means to reduce the true causal effect to the null, i.e., $RR_{DY}^{\text{true}} = 1$.

\noindent\textbf{Numerical Example:} Given an observed risk ratio of $RR_{DY}^{\text{obs}} = 2.00$, we ask: what is the minimum strength of association that an unmeasured confounder would need to have with both the treatment and outcome to fully explain away this association? Using the E-value formula (derived in the next subsection), we find:
\[
\text{E-value} = RR + \sqrt{RR(RR-1)} = 2.00 + \sqrt{2.00(2.00-1)} = 2.00 + 1.41 = 3.41
\]
\textit{Interpretation:} To explain away the observed $RR=2.0$ as entirely due to confounding, an unmeasured confounder would need to be associated with both the treatment and outcome by a risk ratio of 3.41-fold each, above and beyond the measured covariatesâ€”a fairly strong requirement that increases confidence in the causal claim.

\subsection{Derivation of the E-Value Formula}

We start with the bounding factor inequality and set the condition for explaining away the effect. This requires the bounding factor to be at least as large as the observed risk ratio.
\begin{equation}
\frac{RR_{DU} \times RR_{UY}}{RR_{DU} + RR_{UY} - 1} \ge RR_{DY}^{\text{obs}}
\end{equation}
The E-value is defined as the minimum value that both $RR_{DU}$ and $RR_{UY}$ would need to take to satisfy this inequality. This minimum occurs when the two parameters are equal. Let this minimum value be the E-value, such that $RR_{DU} = RR_{UY} = \text{E-value}$.

Substituting this into the inequality gives:
\begin{equation}
\frac{(\text{E-value})^2}{2 \cdot \text{E-value} - 1} = RR_{DY}^{\text{obs}}
\end{equation}
We now solve this equation for the E-value.
\begin{align*}
(\text{E-value})^2 &= RR_{DY}^{\text{obs}} \cdot (2 \cdot \text{E-value} - 1) \\
(\text{E-value})^2 - 2 \cdot RR_{DY}^{\text{obs}} \cdot (\text{E-value}) + RR_{DY}^{\text{obs}} &= 0
\end{align*}
This is a quadratic equation of the form $ax^2 + bx + c = 0$, where $x = \text{E-value}$, $a=1$, $b=-2 \cdot RR_{DY}^{\text{obs}}$, and $c=RR_{DY}^{\text{obs}}$. Using the quadratic formula, $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$:
\begin{align*}
\text{E-value} &= \frac{2 \cdot RR_{DY}^{\text{obs}} \pm \sqrt{(-2 \cdot RR_{DY}^{\text{obs}})^2 - 4(1)(RR_{DY}^{\text{obs}})}}{2} \\
&= \frac{2 \cdot RR_{DY}^{\text{obs}} \pm \sqrt{4(RR_{DY}^{\text{obs}})^2 - 4RR_{DY}^{\text{obs}}}}{2} \\
&= RR_{DY}^{\text{obs}} \pm \sqrt{(RR_{DY}^{\text{obs}})^2 - RR_{DY}^{\text{obs}}} \\
&= RR_{DY}^{\text{obs}} \pm \sqrt{RR_{DY}^{\text{obs}}(RR_{DY}^{\text{obs}} - 1)}
\end{align*}
We take the positive root because the E-value represents the minimum strength required, and the negative root would yield a value smaller than the observed risk ratio, which would not satisfy the original condition. Thus, the final formula is:
\begin{equation}
\text{E-value} = RR_{DY}^{\text{obs}} + \sqrt{RR_{DY}^{\text{obs}}(RR_{DY}^{\text{obs}} - 1)}
\end{equation}
This formula applies for an observed risk ratio $RR_{DY}^{\text{obs}} > 1$. If the observed risk ratio is protective ($RR_{DY}^{\text{obs}} < 1$), one first inverts it to $1/RR_{DY}^{\text{obs}}$ and then applies the same formula.

\noindent\textbf{Numerical Example:} We solve the quadratic equation for $RR_{DY}^{\text{obs}} = 2.00$. Starting from $(E\text{-value})^2 - 2 \cdot RR_{DY}^{\text{obs}} \cdot (E\text{-value}) + RR_{DY}^{\text{obs}} = 0$, we get:
\[
(E\text{-value})^2 - 4.00 \cdot (E\text{-value}) + 2.00 = 0
\]
This is a quadratic equation with $a=1$, $b=-4.00$, and $c=2.00$. The discriminant is $b^2 - 4ac = 16.00 - 8.00 = 8.00$. Using the quadratic formula:
\[
E\text{-value} = \frac{-b + \sqrt{b^2-4ac}}{2a} = \frac{4.00 + \sqrt{8.00}}{2} = \frac{4.00 + 2.83}{2} = \frac{6.83}{2} = 3.41
\]
This matches the closed-form E-value formula: $E\text{-value} = RR + \sqrt{RR(RR-1)} = 2.00 + \sqrt{2.00} = 3.41$. \textit{Interpretation:} The quadratic derivation confirms that the closed-form E-value formula is algebraically exact, providing a simple calculation tool without requiring iterative solving.

\subsection*{Implications}
The E-value provides a single, standardized metric to assess the robustness of an observed association to unmeasured confounding. A large E-value implies that a very strong unmeasured confounder would be required to explain away the result, thus increasing confidence in the causal interpretation. A small E-value indicates that even weak confounding could be responsible for the observed association. It is recommended to report the E-value for both the point estimate and the limit of the confidence interval closest to the null.

\section{The High Threshold Condition}

A further implication of the bounding factor is a strengthening of the classic Cornfield conditions for confounding.

\subsection{Derivation of the High Threshold}

The classic Cornfield conditions state that to explain away an observed $RR_{DY}^{\text{obs}}$, an unmeasured confounder must satisfy both $RR_{DU} \ge RR_{DY}^{\text{obs}}$ and $RR_{UY} \ge RR_{DY}^{\text{obs}}$.

The bounding factor inequality provides a stronger, joint condition. To fully explain away the effect ($RR_{DY}^{\text{true}} = 1$), we need $\frac{RR_{DU} \times RR_{UY}}{RR_{DU} + RR_{UY} - 1} \ge RR_{DY}^{\text{obs}}$.

Ding and VanderWeele (2016) show that this implies a stronger condition on the maximum of the two parameters. To explain away the effect, it must be true that:
\begin{equation}
\max(RR_{DU}, RR_{UY}) \ge RR_{DY}^{\text{obs}} + \sqrt{RR_{DY}^{\text{obs}}(RR_{DY}^{\text{obs}} - 1)}
\end{equation}
The term on the right is exactly the E-value.

\noindent\textbf{Numerical Example:} For the observed $RR_{DY}^{\text{obs}} = 2.00$, we compare the classic Cornfield conditions (both $RR_{DU} \ge 2.00$ and $RR_{UY} \ge 2.00$) with the high threshold condition ($\max(RR_{DU}, RR_{UY}) \ge 3.41$).

Suppose $RR_{DU} = RR_{UY} = 2.0$ (meeting Cornfield conditions). The bounding factor is:
\[
BF = \frac{2.0 \times 2.0}{2.0 + 2.0 - 1} = \frac{4.0}{3.0} = 1.33
\]
Thus $RR_{DY}^{\text{true}} \ge 2.00/1.33 = 1.50$. The effect is not explained away ($RR^{\text{true}} = 1.50 > 1$).

Now suppose $\max(RR_{DU}, RR_{UY}) = E\text{-value} = 3.414$. Let $RR_{DU} = RR_{UY} = 3.414$. The bounding factor is:
\[
BF = \frac{3.414 \times 3.414}{3.414 + 3.414 - 1} = \frac{11.655}{5.828} = 2.00
\]
Thus $RR_{DY}^{\text{true}} \ge 2.00/2.00 = 1.00$. The effect is now explained away ($RR^{\text{true}} \approx 1$). \textit{Interpretation:} Meeting the classic Cornfield conditions (both parameters $\ge 2.0$) is insufficient to nullify the observed association; at least one confounding parameter must reach the E-value threshold of 3.414, demonstrating that the high threshold condition sets a substantially more stringent bar for confounding explanations.

\subsection*{Implications}
This ``high threshold'' condition demonstrates that it is not sufficient for both confounder associations to be as strong as the observed effect. At least one of the confounder associations must be as large as the E-value, which is always substantially larger than the observed risk ratio itself. This raises the bar for a plausible confounding explanation, showing that explaining away moderate-to-strong associations requires the existence of a very powerful unmeasured confounder.

\vspace{1cm}

\section*{References}

\noindent Ding, P., \& VanderWeele, T. J. (2016). Sensitivity Analysis Without Assumptions. \textit{Epidemiology}, 27(3), 368-377.

\vspace{0.2cm}

\noindent VanderWeele, T. J., \& Ding, P. (2017). Sensitivity Analysis in Observational Research: Introducing the E-Value. \textit{Annals of Internal Medicine}, 167(4), 268-274.

\vspace{0.2cm}

\noindent VanderWeele, T. J., \& Arah, O. A. (2011). Bias formulas for sensitivity analysis of unmeasured confounding for general outcomes, treatments, and confounders. \textit{Epidemiology}, 22(1), 42-52.

\end{document}
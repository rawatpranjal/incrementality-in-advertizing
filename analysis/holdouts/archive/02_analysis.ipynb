{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 02_analysis.ipynb: Bounded Causal Effect Estimation\n\n## Important Context: This is Observational Data, Not an Experiment\n\nWe observe:\n- **Ad-seers**: Users who were shown advertisements (treated group)\n- **Secret shoppers**: Organic buyers who purchased without seeing ads (NOT a control group)\n\nThe \"secret shoppers\" are a highly selected subset - they're users who managed to purchase despite never seeing ads. They are NOT representative of what would happen to ad-seers without treatment.\n\n## Mathematical Framework\n\n### Target Parameter\nAverage treatment effect (ATE) within the eligible population:\n$$\\theta = \\mathbb{E}[Y_{i}(1) - Y_{i}(0) \\mid \\text{eligible}]$$\n\n### Observable Quantities\nFor each segment $g$:\n- $N_1^g$ = Number of ad-seers (treated users)\n- $n_{11}^g$ = Ad-seers who purchased in Period 2\n- $n_{01}^g$ = Secret shoppers (organic buyers without ad exposure)\n- $N_0^g$ = Number in \"control\" (UNKNOWN - we cannot observe the true denominator)\n\n### Key Calculated Metrics\n- $p_1^g = n_{11}^g / N_1^g$ = Purchase rate among ad-seers\n- $c^g = n_{01}^g / N_1^g$ = Contamination ratio (organic buyers per ad-seer)\n- $\\pi \\in [\\pi_L, \\pi_U]$ = Assumed bounds on treatment assignment probability\n\n### Bounded Estimation Formula\nSince we cannot observe the true control group size:\n$$\\text{ATE}_g \\in \\left[ p_1^g - \\frac{\\pi_U}{1-\\pi_U} c^g, \\; p_1^g - \\frac{\\pi_L}{1-\\pi_L} c^g \\right]$$\n\nThis gives us a range, not a point estimate, acknowledging the fundamental uncertainty in observational data."
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate fake data for testing the analysis pipeline\nimport pandas as pd\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create fake aggregated counts data\nfake_data = []\n\n# Overall cohort\nfake_data.append({\n    'segment_type': 'Overall_Cohort',\n    'segment_name': 'All_Users',\n    'N_1': 2500000,  # 2.5M treated users\n    'n_11': 750000,   # 30% purchase rate for treated\n    'N_0': 500000,    # 500k control users (observed, not true size)\n    'n_01': 125000    # Secret shoppers (organic buyers)\n})\n\n# Spend quintiles\nspend_labels = ['Q1_Low', 'Q2', 'Q3', 'Q4', 'Q5_High']\nbase_rates_treated = [0.20, 0.25, 0.30, 0.35, 0.45]  # Increasing with spend\nbase_rates_secret = [0.15, 0.20, 0.25, 0.30, 0.40]    # Secret shopper rates\n\nfor i, label in enumerate(spend_labels):\n    N_1 = np.random.randint(400000, 600000)\n    N_0 = np.random.randint(80000, 120000)\n    fake_data.append({\n        'segment_type': 'Spend_Quintile',\n        'segment_name': label,\n        'N_1': N_1,\n        'n_11': int(N_1 * base_rates_treated[i]),\n        'N_0': N_0,\n        'n_01': int(N_0 * base_rates_secret[i])\n    })\n\n# Purchase frequency groups\nfreq_labels = ['1_purchase', '2-3_purchases', '4-10_purchases', '11+_purchases']\nfreq_rates_treated = [0.22, 0.28, 0.35, 0.50]\nfreq_rates_secret = [0.18, 0.23, 0.30, 0.45]\n\nfor i, label in enumerate(freq_labels):\n    N_1 = np.random.randint(300000, 900000)\n    N_0 = np.random.randint(60000, 180000)\n    fake_data.append({\n        'segment_type': 'Purchase_Frequency',\n        'segment_name': label,\n        'N_1': N_1,\n        'n_11': int(N_1 * freq_rates_treated[i]),\n        'N_0': N_0,\n        'n_01': int(N_0 * freq_rates_secret[i])\n    })\n\n# Tenure groups\ntenure_labels = ['0-30_days', '31-60_days', '61-90_days', '91+_days']\ntenure_rates_treated = [0.35, 0.32, 0.28, 0.25]\ntenure_rates_secret = [0.30, 0.27, 0.23, 0.20]\n\nfor i, label in enumerate(tenure_labels):\n    N_1 = np.random.randint(500000, 700000)\n    N_0 = np.random.randint(100000, 140000)\n    fake_data.append({\n        'segment_type': 'Tenure',\n        'segment_name': label,\n        'N_1': N_1,\n        'n_11': int(N_1 * tenure_rates_treated[i]),\n        'N_0': N_0,\n        'n_01': int(N_0 * tenure_rates_secret[i])\n    })\n\n# AOV quintiles\naov_labels = ['AOV_Q1_Low', 'AOV_Q2', 'AOV_Q3', 'AOV_Q4', 'AOV_Q5_High']\naov_rates_treated = [0.25, 0.28, 0.30, 0.32, 0.35]\naov_rates_secret = [0.20, 0.23, 0.25, 0.27, 0.30]\n\nfor i, label in enumerate(aov_labels):\n    N_1 = np.random.randint(400000, 600000)\n    N_0 = np.random.randint(80000, 120000)\n    fake_data.append({\n        'segment_type': 'AOV_Quintile',\n        'segment_name': label,\n        'N_1': N_1,\n        'n_11': int(N_1 * aov_rates_treated[i]),\n        'N_0': N_0,\n        'n_01': int(N_0 * aov_rates_secret[i])\n    })\n\n# Create DataFrame\ndf_counts = pd.DataFrame(fake_data)\n\n# Save to CSV for testing\ntest_file = 'causal_estimation_counts_TEST.csv'\ndf_counts.to_csv(test_file, index=False)\n\nprint(\"ðŸŽ­ FAKE DATA GENERATED FOR TESTING\")\nprint(\"=\" * 60)\nprint(f\"Created test file: {test_file}\")\nprint(f\"Total segments: {len(df_counts)}\")\nprint(\"\\nData preview:\")\nprint(df_counts.head(10))\n\nprint(\"\\nSummary by segment type:\")\nsummary = df_counts.groupby('segment_type').agg({\n    'N_1': 'sum',\n    'n_11': 'sum', \n    'N_0': 'sum',\n    'n_01': 'sum'\n})\nsummary['p_1'] = summary['n_11'] / summary['N_1']\nsummary['p_0_observed'] = summary['n_01'] / summary['N_0']\nsummary['observed_diff'] = summary['p_1'] - summary['p_0_observed']\nprint(summary)\n\nprint(\"\\nâœ… You can now run the rest of the notebook with this test data!\")\nprint(\"   Just change 'causal_estimation_counts.csv' to 'causal_estimation_counts_TEST.csv'\")\nprint(\"   in the next cell to use fake data instead of real data.\")\nprint(\"\\nâš ï¸ Note: The 'observed_diff' is NOT a valid causal estimate\")\nprint(\"   Secret shoppers are a selected subset, not a control group!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Aggregated Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Using TEST data for analysis\n",
      "Data file: causal_estimation_counts_TEST.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Switch between TEST and REAL data\n",
    "USE_TEST_DATA = True  # Set to False when real data is ready\n",
    "\n",
    "if USE_TEST_DATA:\n",
    "    counts_file = 'causal_estimation_counts_TEST.csv'\n",
    "    print(\"ðŸ“Š Using TEST data for analysis\")\n",
    "else:\n",
    "    counts_file = 'causal_estimation_counts.csv'\n",
    "    print(\"ðŸ“Š Using REAL data for analysis\")\n",
    "    \n",
    "print(f\"Data file: {counts_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 segment aggregates from causal_estimation_counts_TEST.csv\n",
      "\n",
      "Data overview:\n",
      "         segment_type    segment_name      N_1    n_11     N_0    n_01\n",
      "0      Overall_Cohort       All_Users  2500000  750000  500000  125000\n",
      "1      Spend_Quintile          Q1_Low   521958  104391   95795   14369\n",
      "2      Spend_Quintile              Q2   531932  132983  118158   23631\n",
      "3      Spend_Quintile              Q3   519879  155963   91284   22821\n",
      "4      Spend_Quintile              Q4   454886  159210   86265   25879\n",
      "5      Spend_Quintile         Q5_High   568266  255719  101962   40784\n",
      "6  Purchase_Frequency      1_purchase   475203  104544  120263   21647\n",
      "7  Purchase_Frequency   2-3_purchases   578167  161886  101090   23250\n",
      "8  Purchase_Frequency  4-10_purchases   629365  220277  124820   37446\n",
      "9  Purchase_Frequency   11+_purchases   621879  310939  122955   55329\n",
      "\n",
      "Segment types: ['Overall_Cohort' 'Spend_Quintile' 'Purchase_Frequency' 'Tenure'\n",
      " 'AOV_Quintile']\n"
     ]
    }
   ],
   "source": [
    "# Load the aggregated counts from data extraction\n",
    "# Using the configured filename from the cell above\n",
    "df_counts = pd.read_csv(counts_file)\n",
    "\n",
    "print(f\"Loaded {len(df_counts)} segment aggregates from {counts_file}\\n\")\n",
    "print(\"Data overview:\")\n",
    "print(df_counts.head(10))\n",
    "print(f\"\\nSegment types: {df_counts['segment_type'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Observed Purchase Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate observed rates\n# p_1: Purchase rate among ad-seers (treated group)\ndf_counts['p_1'] = df_counts['n_11'] / df_counts['N_1']\n\n# c: Contamination ratio - ratio of organic buyers (secret shoppers) to ad-seers\n# This is NOT a purchase rate, but a simple ratio as specified in the math\ndf_counts['c'] = df_counts['n_01'] / df_counts['N_1']\n\n# For reference only - the observed rate among secret shoppers\n# Note: This is NOT used in bounded estimation as we don't know the true N_0\ndf_counts['p_0_observed'] = df_counts['n_01'] / df_counts['N_0']\n\n# The difference between observed rates (for reference - this is NOT a valid comparison)\n# Secret shoppers are a selected subset, not a control group\ndf_counts['observed_difference_biased'] = df_counts['p_1'] - df_counts['p_0_observed']\n\nprint(\"Key Metrics by Segment:\\n\")\nprint(\"=\" * 80)\ndisplay_cols = ['segment_type', 'segment_name', 'N_1', 'n_11', 'n_01', 'p_1', 'c']\nprint(df_counts[display_cols].to_string(index=False, float_format='%.4f'))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DEFINITIONS:\")\nprint(\"-\" * 40)\nprint(\"p_1 = n_11/N_1 = Purchase rate of ad-seers\")\nprint(\"c = n_01/N_1 = Ratio of organic buyers to ad-seers\")\nprint(\"\\nNOTE: We do NOT calculate a 'control purchase rate' because we don't know\")\nprint(\"the true size of the control population (N_0). The 'secret shoppers' are\")\nprint(\"a selected subset, not a representative control group.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Bounded Estimation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_ate_bounds(p_1, c, pi_lower, pi_upper):\n    \"\"\"\n    Calculate ATE bounds for given parameters using the correct formula.\n    \n    From the math specification:\n    ATE_g âˆˆ [p_1 - (Ï€_U/(1-Ï€_U)) * c,  p_1 - (Ï€_L/(1-Ï€_L)) * c]\n    \n    Where:\n    - p_1 = n_11 / N_1 (purchase rate among treated)\n    - c = n_01 / N_1 (control purchases scaled by treated group size)\n    \n    Parameters:\n    - p_1: Purchase rate among treated\n    - c: Scaled control purchase rate (n_01 / N_1)\n    - pi_lower: Lower bound of treatment probability\n    - pi_upper: Upper bound of treatment probability\n    \n    Returns:\n    - (lower_bound, upper_bound, point_estimate)\n    \"\"\"\n    # Calculate adjustment factors\n    factor_upper = pi_upper / (1 - pi_upper)\n    factor_lower = pi_lower / (1 - pi_lower)\n    \n    # ATE bounds (note the order - larger factor gives lower bound)\n    lower_bound = p_1 - factor_upper * c\n    upper_bound = p_1 - factor_lower * c\n    \n    # Point estimate using midpoint of pi bounds\n    pi_mid = (pi_lower + pi_upper) / 2\n    factor_mid = pi_mid / (1 - pi_mid)\n    point_estimate = p_1 - factor_mid * c\n    \n    return lower_bound, upper_bound, point_estimate\n\ndef apply_monotonicity_constraint(lower_bound, upper_bound, point_estimate):\n    \"\"\"\n    Apply monotonicity constraint: Y_i(1) >= Y_i(0)\n    This implies ATE >= 0\n    \"\"\"\n    lower_bound_constrained = max(0, lower_bound)\n    upper_bound_constrained = max(0, upper_bound)\n    point_estimate_constrained = max(0, point_estimate)\n    \n    return lower_bound_constrained, upper_bound_constrained, point_estimate_constrained\n\nprint(\"Bounded estimation functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Bounded ATE Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define treatment probability bounds\nPI_LOWER = 0.95\nPI_UPPER = 0.99\n\nprint(f\"Treatment probability bounds: Ï€ âˆˆ [{PI_LOWER}, {PI_UPPER}]\\n\")\n\n# Calculate bounds for each segment\nresults = []\n\nfor idx, row in df_counts.iterrows():\n    # Use the simplified c calculation (just n_01 / N_1)\n    c_value = row['c']\n    \n    lower, upper, point = calculate_ate_bounds(\n        row['p_1'], c_value, PI_LOWER, PI_UPPER\n    )\n    \n    # Also calculate with monotonicity constraint\n    lower_mono, upper_mono, point_mono = apply_monotonicity_constraint(lower, upper, point)\n    \n    results.append({\n        'segment_type': row['segment_type'],\n        'segment_name': row['segment_name'],\n        'N_1': row['N_1'],\n        'n_01': row['n_01'],\n        'n_11': row['n_11'],\n        'p_1': row['p_1'],\n        'p_0_observed': row['p_0_observed'],\n        'c': c_value,\n        'observed_diff_biased': row['observed_difference_biased'],\n        'ate_lower': lower,\n        'ate_upper': upper,\n        'ate_point': point,\n        'ate_lower_mono': lower_mono,\n        'ate_upper_mono': upper_mono,\n        'ate_point_mono': point_mono\n    })\n\ndf_results = pd.DataFrame(results)\n\nprint(\"Bounded ATE Estimates (without monotonicity):\")\nprint(\"=\"*80)\ndisplay_cols = ['segment_type', 'segment_name', 'ate_lower', 'ate_upper', 'ate_point']\nprint(df_results[display_cols].to_string(index=False, float_format='%.4f'))\n\nprint(\"\\nBounded ATE Estimates (with monotonicity constraint):\")\nprint(\"=\"*80)\ndisplay_cols_mono = ['segment_type', 'segment_name', 'ate_lower_mono', 'ate_upper_mono', 'ate_point_mono']\nprint(df_results[display_cols_mono].to_string(index=False, float_format='%.4f'))\n\nprint(\"\\nNote: Observed difference (p_1 - p_0_observed) is shown for reference but is NOT\")\nprint(\"a valid causal estimate as secret shoppers are a selected subset.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Cohort-Level ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get overall cohort estimate\noverall = df_results[df_results['segment_name'] == 'All_Users'].iloc[0]\n\nprint(\"OVERALL COHORT-LEVEL RESULTS\")\nprint(\"=\"*60)\nprint(f\"Total ad-seers (treated): {overall['N_1']:,.0f}\")\nprint(f\"Secret shoppers (organic buyers): {overall['n_01']:,.0f}\")\nprint(f\"Observed N_0 (NOT the true control size): {overall['N_0']:,.0f}\")\nprint(f\"\\nObserved metrics:\")\nprint(f\"  Ad-seers purchase rate (p_1): {overall['p_1']:.4f}\")\nprint(f\"  Contamination ratio (c): {overall['c']:.4f}\")\nprint(f\"  Secret shopper rate (biased): {overall['p_0_observed']:.4f}\")\n\nprint(f\"\\nâš ï¸ WARNING: The 'secret shopper rate' is NOT a valid control\")\nprint(f\"   These are hand-selected expert buyers, not a random sample\")\n\nprint(f\"\\nBounded ATE estimates (assuming Ï€ âˆˆ [{PI_LOWER}, {PI_UPPER}]):\")\nprint(f\"  Without monotonicity: [{overall['ate_lower']:.4f}, {overall['ate_upper']:.4f}]\")\nprint(f\"  Point estimate: {overall['ate_point']:.4f}\")\nprint(f\"\\n  With monotonicity: [{overall['ate_lower_mono']:.4f}, {overall['ate_upper_mono']:.4f}]\")\nprint(f\"  Point estimate: {overall['ate_point_mono']:.4f}\")\n\n# Calculate percentage lifts relative to p_1 baseline\nbaseline = overall['p_1']\nif baseline > 0:\n    print(f\"\\nPercentage lift (relative to ad-seers baseline):\")\n    print(f\"  Bounded (point): {100 * overall['ate_point_mono'] / baseline:.2f}%\")\n    print(f\"  Bounded range: [{100 * overall['ate_lower_mono'] / baseline:.2f}%, {100 * overall['ate_upper_mono'] / baseline:.2f}%]\")\n    \nprint(\"\\n\" + \"=\"*60)\nprint(\"INTERPRETATION:\")\nprint(\"-\"*40)\nprint(\"These bounds represent our best estimate of the true causal effect\")\nprint(\"given that we cannot observe the full control population.\")\nprint(\"The range acknowledges the fundamental uncertainty in observational data.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Heterogeneous Treatment Effects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze heterogeneous effects by segment type\nsegment_types = df_results[df_results['segment_name'] != 'All_Users']['segment_type'].unique()\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\naxes = axes.flatten()\n\nfor idx, seg_type in enumerate(segment_types):\n    ax = axes[idx]\n    \n    # Filter data for this segment type\n    seg_data = df_results[df_results['segment_type'] == seg_type].sort_values('segment_name')\n    \n    # Plot bounds and point estimates\n    x_pos = np.arange(len(seg_data))\n    \n    # Plot error bars for bounds\n    yerr = np.array([\n        seg_data['ate_point_mono'] - seg_data['ate_lower_mono'],\n        seg_data['ate_upper_mono'] - seg_data['ate_point_mono']\n    ])\n    \n    ax.errorbar(x_pos, seg_data['ate_point_mono'], yerr=yerr, \n                fmt='o', capsize=5, capthick=2, label='Bounded ATE', markersize=8)\n    \n    # Plot observed difference for reference (not a valid comparison)\n    ax.scatter(x_pos, seg_data['observed_diff_biased'], s=40, marker='x', color='gray',\n            label='Observed Diff (biased)', alpha=0.5)\n    \n    # Add zero line\n    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    \n    # Formatting\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(seg_data['segment_name'], rotation=45, ha='right')\n    ax.set_ylabel('ATE (Purchase Probability)')\n    ax.set_title(f'Treatment Effects by {seg_type}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle(f'Heterogeneous Treatment Effects\\n(Ï€ âˆˆ [{PI_LOWER}, {PI_UPPER}], with monotonicity)', \n             fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Print detailed heterogeneity summary\nprint(\"\\nHETEROGENEITY SUMMARY\")\nprint(\"=\"*60)\n\nfor seg_type in segment_types:\n    seg_data = df_results[df_results['segment_type'] == seg_type]\n    print(f\"\\n{seg_type}:\")\n    print(\"-\"*40)\n    \n    # Find segments with highest and lowest effects\n    max_effect_idx = seg_data['ate_point_mono'].idxmax()\n    min_effect_idx = seg_data['ate_point_mono'].idxmin()\n    \n    max_seg = seg_data.loc[max_effect_idx]\n    min_seg = seg_data.loc[min_effect_idx]\n    \n    print(f\"  Highest effect: {max_seg['segment_name']} = {max_seg['ate_point_mono']:.4f}\")\n    print(f\"  Lowest effect: {min_seg['segment_name']} = {min_seg['ate_point_mono']:.4f}\")\n    print(f\"  Range: {max_seg['ate_point_mono'] - min_seg['ate_point_mono']:.4f}\")\n\nprint(\"\\nâš ï¸ Note: The 'Observed Diff' is shown for reference but is NOT a valid causal estimate\")\nprint(\"   as secret shoppers are a highly selected subset of users.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sensitivity analysis: vary Ï€ bounds\npi_scenarios = [\n    (0.90, 0.95, 'Conservative'),\n    (0.95, 0.99, 'Baseline'),\n    (0.98, 0.995, 'Aggressive'),\n    (0.99, 0.999, 'Extreme')\n]\n\nsensitivity_results = []\n\n# Focus on overall cohort for sensitivity\noverall_row = df_counts[df_counts['segment_name'] == 'All_Users'].iloc[0]\nc_overall = overall_row['n_01'] / overall_row['N_1'] if overall_row['N_1'] > 0 else 0\n\nfor pi_low, pi_high, scenario in pi_scenarios:\n    lower, upper, point = calculate_ate_bounds(\n        overall_row['p_1'], c_overall, pi_low, pi_high\n    )\n    lower_mono, upper_mono, point_mono = apply_monotonicity_constraint(lower, upper, point)\n    \n    sensitivity_results.append({\n        'scenario': scenario,\n        'pi_range': f\"[{pi_low}, {pi_high}]\",\n        'ate_lower': lower,\n        'ate_upper': upper,\n        'ate_point': point,\n        'ate_lower_mono': lower_mono,\n        'ate_upper_mono': upper_mono,\n        'ate_point_mono': point_mono,\n        'width': upper - lower,\n        'width_mono': upper_mono - lower_mono\n    })\n\ndf_sensitivity = pd.DataFrame(sensitivity_results)\n\nprint(\"SENSITIVITY ANALYSIS: Overall Cohort ATE\")\nprint(\"=\"*80)\nprint(\"\\nWithout monotonicity constraint:\")\ncols = ['scenario', 'pi_range', 'ate_lower', 'ate_upper', 'ate_point', 'width']\nprint(df_sensitivity[cols].to_string(index=False, float_format='%.5f'))\n\nprint(\"\\nWith monotonicity constraint:\")\ncols_mono = ['scenario', 'pi_range', 'ate_lower_mono', 'ate_upper_mono', 'ate_point_mono', 'width_mono']\nprint(df_sensitivity[cols_mono].to_string(index=False, float_format='%.5f'))\n\n# Visualize sensitivity\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: ATE bounds across scenarios\nx_pos = np.arange(len(df_sensitivity))\nwidth = 0.35\n\nax1.bar(x_pos - width/2, df_sensitivity['ate_point'], width, \n        label='Without Monotonicity', alpha=0.7)\nax1.bar(x_pos + width/2, df_sensitivity['ate_point_mono'], width, \n        label='With Monotonicity', alpha=0.7)\n\n# Add error bars for bounds\nfor i, row in df_sensitivity.iterrows():\n    ax1.plot([i - width/2, i - width/2], \n             [row['ate_lower'], row['ate_upper']], \n             'k-', linewidth=2)\n    ax1.plot([i + width/2, i + width/2], \n             [row['ate_lower_mono'], row['ate_upper_mono']], \n             'k-', linewidth=2)\n\nax1.set_xticks(x_pos)\nax1.set_xticklabels(df_sensitivity['scenario'])\nax1.set_ylabel('ATE')\nax1.set_title('ATE Estimates Across Ï€ Scenarios')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Width of confidence intervals\nax2.plot(x_pos, df_sensitivity['width'], 'o-', label='Without Monotonicity', markersize=8)\nax2.plot(x_pos, df_sensitivity['width_mono'], 's-', label='With Monotonicity', markersize=8)\nax2.set_xticks(x_pos)\nax2.set_xticklabels(df_sensitivity['scenario'])\nax2.set_ylabel('Width of Bounds')\nax2.set_title('Uncertainty (Width) Across Scenarios')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.suptitle('Sensitivity Analysis: Impact of Ï€ Bounds', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate comprehensive report\nreport_file = 'bounded_ate_results.txt'\n\nwith open(report_file, 'w') as f:\n    f.write(\"BOUNDED CAUSAL EFFECT ESTIMATION RESULTS\\n\")\n    f.write(\"=\" * 80 + \"\\n\\n\")\n    \n    f.write(\"IMPORTANT: THIS IS OBSERVATIONAL DATA, NOT AN EXPERIMENT\\n\")\n    f.write(\"-\" * 40 + \"\\n\")\n    f.write(\"We observe:\\n\")\n    f.write(\"- Ad-seers: Users shown advertisements (treated group)\\n\")\n    f.write(\"- Secret shoppers: Organic buyers without ad exposure (NOT a control group)\\n\")\n    f.write(\"Secret shoppers are a highly selected subset who purchase despite no ads.\\n\\n\")\n    \n    f.write(\"METHODOLOGY\\n\")\n    f.write(\"-\" * 40 + \"\\n\")\n    f.write(\"Approach: Bounded estimation under partial identification\\n\")\n    f.write(\"Key assumption: Treatment assignment probability Ï€ is bounded but unknown\\n\")\n    f.write(f\"Baseline bounds: Ï€ âˆˆ [{PI_LOWER}, {PI_UPPER}]\\n\")\n    f.write(\"Optional: Monotonicity constraint (Y(1) â‰¥ Y(0)) applied\\n\\n\")\n    \n    f.write(\"OVERALL COHORT RESULTS\\n\")\n    f.write(\"-\" * 40 + \"\\n\")\n    f.write(f\"Total ad-seers (treated): {overall['N_1']:,.0f}\\n\")\n    f.write(f\"Observed secret shoppers: {overall['n_01']:,.0f}\\n\")\n    f.write(f\"Purchase rate (ad-seers): {overall['p_1']:.4f}\\n\")\n    f.write(f\"Contamination ratio (c): {overall['c']:.4f}\\n\")\n    f.write(f\"Secret shopper observed rate: {overall['p_0_observed']:.4f} (NOT a valid control)\\n\\n\")\n    \n    f.write(\"ATE ESTIMATES\\n\")\n    f.write(f\"Observed difference (NOT a valid estimate): {overall['observed_diff_biased']:.4f}\\n\")\n    f.write(\"  âš ï¸ This compares ad-seers to secret shoppers, who are a selected subset\\n\\n\")\n    f.write(\"TRUE CAUSAL BOUNDS:\\n\")\n    f.write(f\"Without monotonicity: [{overall['ate_lower']:.4f}, {overall['ate_upper']:.4f}]\\n\")\n    f.write(f\"With monotonicity: [{overall['ate_lower_mono']:.4f}, {overall['ate_upper_mono']:.4f}]\\n\")\n    f.write(f\"Point estimate: {overall['ate_point_mono']:.4f}\\n\\n\")\n    \n    if overall['p_1'] > 0:\n        f.write(\"PERCENTAGE LIFTS (relative to ad-seers baseline)\\n\")\n        f.write(f\"Bounded point estimate: {100 * overall['ate_point_mono'] / overall['p_1']:.2f}%\\n\")\n        f.write(f\"Bounded range: [{100 * overall['ate_lower_mono'] / overall['p_1']:.2f}%, \"\n                f\"{100 * overall['ate_upper_mono'] / overall['p_1']:.2f}%]\\n\\n\")\n    \n    f.write(\"HETEROGENEOUS EFFECTS BY SEGMENT\\n\")\n    f.write(\"-\" * 40 + \"\\n\")\n    \n    for seg_type in segment_types:\n        f.write(f\"\\n{seg_type}:\\n\")\n        seg_data = df_results[df_results['segment_type'] == seg_type].sort_values('ate_point_mono', ascending=False)\n        for _, row in seg_data.iterrows():\n            f.write(f\"  {row['segment_name']:20s}: {row['ate_point_mono']:.4f} \"\n                   f\"[{row['ate_lower_mono']:.4f}, {row['ate_upper_mono']:.4f}]\\n\")\n    \n    f.write(\"\\nSENSITIVITY ANALYSIS\\n\")\n    f.write(\"-\" * 40 + \"\\n\")\n    f.write(\"Impact of Ï€ bounds on overall cohort ATE:\\n\\n\")\n    for _, row in df_sensitivity.iterrows():\n        f.write(f\"{row['scenario']:15s} {row['pi_range']:15s}: \"\n               f\"{row['ate_point_mono']:.5f} [{row['ate_lower_mono']:.5f}, {row['ate_upper_mono']:.5f}]\\n\")\n    \n    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n    f.write(\"KEY TAKEAWAY: The bounded approach acknowledges fundamental uncertainty\\n\")\n    f.write(\"in observational data while providing actionable causal insights.\\n\")\n    f.write(\"=\" * 80 + \"\\n\")\n    f.write(\"END OF REPORT\\n\")\n\nprint(f\"âœ… Final report saved to {report_file}\")\n\n# Also save detailed results to CSV\ndetailed_results_file = 'bounded_ate_detailed_results.csv'\ndf_results.to_csv(detailed_results_file, index=False)\nprint(f\"âœ… Detailed results saved to {detailed_results_file}\")\n\nsensitivity_results_file = 'sensitivity_analysis_results.csv'\ndf_sensitivity.to_csv(sensitivity_results_file, index=False)\nprint(f\"âœ… Sensitivity analysis saved to {sensitivity_results_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis provides bounded estimates of the causal effect of advertising on purchase probability. Key findings:\n",
    "\n",
    "1. **Overall Effect**: The bounded ATE provides a range of plausible causal effects\n",
    "2. **Heterogeneity**: Effects vary across user segments defined by purchase behavior\n",
    "3. **Sensitivity**: Results are robust to reasonable variations in Ï€ bounds\n",
    "4. **Monotonicity**: Applying the constraint Y(1) â‰¥ Y(0) tightens bounds meaningfully\n",
    "\n",
    "The bounded approach acknowledges fundamental uncertainty while providing actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Snowflake-Native EDA: \"Aggregating the Truth\"\n",
    "\n",
    "## Strategy\n",
    "1. **Unit of Analysis:** Vendor Ledger (not User Journey)\n",
    "2. **Computation:** Push-down to Snowflake (not ELT to pandas)\n",
    "3. **Data Structure:** Scaffolded panel with explicit zeros\n",
    "\n",
    "## Two-Part Data Pull\n",
    "- **Part 1:** Vendor-Week Panel (~3.7M rows) - Q1,Q2,Q3,Q4,Q5,Q8,Q9,Q10\n",
    "- **Part 2:** Mechanism Sample (~50K rows) - Q6,Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration & Connection\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import snowflake.connector\n",
    "\n",
    "warnings.filterwarnings('ignore', message='pandas only supports SQLAlchemy.*')\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ANALYSIS_START = '2025-03-24'\n",
    "ANALYSIS_END = '2025-09-15'\n",
    "N_WEEKS = 26\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('./data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Connection (stays open throughout notebook)\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY',\n",
    "    schema='INCREMENTALITY_RESEARCH'\n",
    ")\n",
    "print(\"[OK] Snowflake connected\")\n",
    "print(f\"Analysis window: {ANALYSIS_START} to {ANALYSIS_END} ({N_WEEKS} weeks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Q1 - Bridge Integrity Check (Orphan Rate)\n",
    "# CRITICAL PATH: If orphan GMV > 50%, project stops.\n",
    "\n",
    "orphan_query = f\"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT p.PURCHASE_ID) as total_purchases,\n",
    "    COUNT(DISTINCT CASE WHEN c.PRODUCT_ID IS NULL THEN p.PURCHASE_ID END) as orphan_purchases,\n",
    "    SUM(p.UNIT_PRICE * p.QUANTITY) as total_gmv,\n",
    "    SUM(CASE WHEN c.PRODUCT_ID IS NULL THEN p.UNIT_PRICE * p.QUANTITY ELSE 0 END) as orphan_gmv\n",
    "FROM PURCHASES p\n",
    "LEFT JOIN CATALOG c ON LOWER(TRIM(p.PRODUCT_ID)) = LOWER(TRIM(c.PRODUCT_ID))\n",
    "WHERE p.PURCHASED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing orphan rate query...\")\n",
    "orphan_stats = pd.read_sql(orphan_query, conn)\n",
    "orphan_rate = orphan_stats['ORPHAN_GMV'].iloc[0] / orphan_stats['TOTAL_GMV'].iloc[0] if orphan_stats['TOTAL_GMV'].iloc[0] > 0 else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q1: BRIDGE INTEGRITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Purchases: {orphan_stats['TOTAL_PURCHASES'].iloc[0]:,}\")\n",
    "print(f\"Orphan Purchases: {orphan_stats['ORPHAN_PURCHASES'].iloc[0]:,}\")\n",
    "print(f\"Total GMV: ${orphan_stats['TOTAL_GMV'].iloc[0]:,.0f}\")\n",
    "print(f\"Orphan GMV: ${orphan_stats['ORPHAN_GMV'].iloc[0]:,.0f}\")\n",
    "print(f\"Orphan Rate: {orphan_rate:.1%}\")\n",
    "print(f\"Status: {'CRITICAL - STOP' if orphan_rate > 0.5 else 'OK' if orphan_rate < 0.1 else 'WARNING'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Q2 - Panel Balance Check\n",
    "\n",
    "balance_query = f\"\"\"\n",
    "SELECT\n",
    "    (SELECT COUNT(DISTINCT LOWER(TO_VARCHAR(VENDOR_ID, 'HEX')))\n",
    "     FROM AUCTIONS_RESULTS\n",
    "     WHERE CREATED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "       AND IS_WINNER = TRUE) as n_bidders,\n",
    "    (SELECT COUNT(DISTINCT v.value::STRING)\n",
    "     FROM CATALOG, LATERAL FLATTEN(input => VENDORS) v) as n_catalog_vendors\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing panel balance query...\")\n",
    "balance = pd.read_sql(balance_query, conn)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q2: PANEL BALANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Bidding Vendors (winners): {balance['N_BIDDERS'].iloc[0]:,}\")\n",
    "print(f\"Catalog Vendors: {balance['N_CATALOG_VENDORS'].iloc[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build Vendor-Week Panel (Server-Side Aggregation)\n",
    "# THE CORE QUERY - builds entire panel inside Snowflake\n",
    "\n",
    "panel_query = f\"\"\"\n",
    "WITH\n",
    "-- Step 1: Vendor Universe from Catalog\n",
    "VENDOR_UNIVERSE AS (\n",
    "    SELECT DISTINCT v.value::STRING AS VENDOR_ID\n",
    "    FROM CATALOG, LATERAL FLATTEN(input => VENDORS) v\n",
    "),\n",
    "\n",
    "-- Step 2: Week Scaffold\n",
    "WEEK_SCAFFOLD AS (\n",
    "    SELECT DATEADD('week', seq4(), DATE '{ANALYSIS_START}') AS week\n",
    "    FROM TABLE(GENERATOR(ROWCOUNT => {N_WEEKS}))\n",
    "),\n",
    "\n",
    "-- Step 3: Vendor x Week Scaffold (densification)\n",
    "VENDOR_WEEK_SCAFFOLD AS (\n",
    "    SELECT v.VENDOR_ID, w.week\n",
    "    FROM VENDOR_UNIVERSE v\n",
    "    CROSS JOIN WEEK_SCAFFOLD w\n",
    "),\n",
    "\n",
    "-- Step 4: Treatment (Spend from winning bids)\n",
    "SPEND_AGG AS (\n",
    "    SELECT\n",
    "        LOWER(TO_VARCHAR(VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "        DATE_TRUNC('week', CREATED_AT) AS week,\n",
    "        SUM(FINAL_BID) AS total_spend,\n",
    "        COUNT(*) AS wins\n",
    "    FROM AUCTIONS_RESULTS\n",
    "    WHERE IS_WINNER = TRUE\n",
    "      AND CREATED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "\n",
    "-- Step 5: Total GMV (via Catalog bridge - THE KEY JOIN)\n",
    "TOTAL_GMV_AGG AS (\n",
    "    SELECT\n",
    "        v.value::STRING AS VENDOR_ID,\n",
    "        DATE_TRUNC('week', p.PURCHASED_AT) AS week,\n",
    "        SUM(p.UNIT_PRICE * p.QUANTITY) AS total_gmv,\n",
    "        COUNT(DISTINCT p.PURCHASE_ID) AS n_purchases\n",
    "    FROM PURCHASES p\n",
    "    JOIN CATALOG c ON LOWER(TRIM(p.PRODUCT_ID)) = LOWER(TRIM(c.PRODUCT_ID))\n",
    "    CROSS JOIN LATERAL FLATTEN(input => c.VENDORS) v\n",
    "    WHERE p.PURCHASED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "\n",
    "-- Step 6: Promoted GMV (click-attributed, 7-day window)\n",
    "PROMOTED_GMV_AGG AS (\n",
    "    SELECT\n",
    "        LOWER(REPLACE(cl.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "        DATE_TRUNC('week', p.PURCHASED_AT) AS week,\n",
    "        SUM(p.UNIT_PRICE * p.QUANTITY) AS promoted_gmv,\n",
    "        COUNT(DISTINCT p.PURCHASE_ID) AS n_attributed\n",
    "    FROM CLICKS cl\n",
    "    JOIN PURCHASES p\n",
    "        ON cl.USER_ID = p.USER_ID\n",
    "        AND LOWER(TRIM(cl.PRODUCT_ID)) = LOWER(TRIM(p.PRODUCT_ID))\n",
    "        AND p.PURCHASED_AT BETWEEN cl.OCCURRED_AT AND DATEADD('day', 7, cl.OCCURRED_AT)\n",
    "    WHERE cl.OCCURRED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "\n",
    "-- Step 7: Impressions aggregation\n",
    "IMPRESSIONS_AGG AS (\n",
    "    SELECT\n",
    "        LOWER(REPLACE(VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "        DATE_TRUNC('week', OCCURRED_AT) AS week,\n",
    "        COUNT(*) AS impressions\n",
    "    FROM IMPRESSIONS\n",
    "    WHERE OCCURRED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "\n",
    "-- Step 8: Clicks aggregation\n",
    "CLICKS_AGG AS (\n",
    "    SELECT\n",
    "        LOWER(REPLACE(VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "        DATE_TRUNC('week', OCCURRED_AT) AS week,\n",
    "        COUNT(*) AS clicks\n",
    "    FROM CLICKS\n",
    "    WHERE OCCURRED_AT BETWEEN '{ANALYSIS_START}' AND '{ANALYSIS_END}'\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "\n",
    "-- Final Join with COALESCE for explicit zeros\n",
    "SELECT\n",
    "    vws.VENDOR_ID,\n",
    "    vws.week,\n",
    "    COALESCE(s.total_spend, 0) AS total_spend,\n",
    "    COALESCE(s.wins, 0) AS wins,\n",
    "    COALESCE(t.total_gmv, 0) AS total_gmv,\n",
    "    COALESCE(t.n_purchases, 0) AS n_purchases,\n",
    "    COALESCE(p.promoted_gmv, 0) AS promoted_gmv,\n",
    "    COALESCE(p.n_attributed, 0) AS n_attributed,\n",
    "    COALESCE(i.impressions, 0) AS impressions,\n",
    "    COALESCE(c.clicks, 0) AS clicks,\n",
    "    COALESCE(t.total_gmv, 0) - COALESCE(p.promoted_gmv, 0) AS organic_gmv,\n",
    "    CASE WHEN COALESCE(s.total_spend, 0) > 0 THEN 1 ELSE 0 END AS has_spend\n",
    "FROM VENDOR_WEEK_SCAFFOLD vws\n",
    "LEFT JOIN SPEND_AGG s ON vws.VENDOR_ID = s.VENDOR_ID AND vws.week = s.week\n",
    "LEFT JOIN TOTAL_GMV_AGG t ON vws.VENDOR_ID = t.VENDOR_ID AND vws.week = t.week\n",
    "LEFT JOIN PROMOTED_GMV_AGG p ON vws.VENDOR_ID = p.VENDOR_ID AND vws.week = p.week\n",
    "LEFT JOIN IMPRESSIONS_AGG i ON vws.VENDOR_ID = i.VENDOR_ID AND vws.week = i.week\n",
    "LEFT JOIN CLICKS_AGG c ON vws.VENDOR_ID = c.VENDOR_ID AND vws.week = c.week\n",
    "ORDER BY vws.VENDOR_ID, vws.week\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing panel query (this may take several minutes)...\")\n",
    "print(f\"Building {N_WEEKS}-week panel for all vendors...\")\n",
    "with tqdm(desc=\"Building panel\") as pbar:\n",
    "    panel = pd.read_sql(panel_query, conn)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"\\nPanel: {len(panel):,} rows, {panel['VENDOR_ID'].nunique():,} vendors, {panel['WEEK'].nunique()} weeks\")\n",
    "panel.to_parquet(DATA_DIR / 'vendor_week_panel.parquet', index=False)\n",
    "print(f\"Saved to {DATA_DIR / 'vendor_week_panel.parquet'}\")\n",
    "panel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Q3 - Treatment Absorbing (Flicker Rate)\n",
    "\n",
    "# Sort and compute transitions\n",
    "panel_sorted = panel.sort_values(['VENDOR_ID', 'WEEK'])\n",
    "panel_sorted['prev_has_spend'] = panel_sorted.groupby('VENDOR_ID')['HAS_SPEND'].shift(1)\n",
    "\n",
    "transitions = panel_sorted.dropna(subset=['prev_has_spend'])\n",
    "on_to_off = ((transitions['prev_has_spend'] == 1) & (transitions['HAS_SPEND'] == 0)).sum()\n",
    "on_to_on = ((transitions['prev_has_spend'] == 1) & (transitions['HAS_SPEND'] == 1)).sum()\n",
    "flicker_rate = on_to_off / (on_to_off + on_to_on) if (on_to_off + on_to_on) > 0 else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q3: TREATMENT ABSORBING (FLICKER RATE)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ON->OFF transitions: {on_to_off:,}\")\n",
    "print(f\"ON->ON transitions: {on_to_on:,}\")\n",
    "print(f\"Flicker Rate: {flicker_rate:.2%}\")\n",
    "print(f\"Status: {'OK (<20%)' if flicker_rate < 0.2 else 'WARNING'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Q4 - Adoption Velocity\n",
    "\n",
    "# First week with spend > 0 per vendor\n",
    "first_spend = panel[panel['HAS_SPEND'] == 1].groupby('VENDOR_ID')['WEEK'].min().reset_index()\n",
    "first_spend.columns = ['VENDOR_ID', 'cohort_week']\n",
    "\n",
    "adoption_by_week = first_spend.groupby('cohort_week').size().reset_index(name='n_new')\n",
    "adoption_by_week['cumulative'] = adoption_by_week['n_new'].cumsum()\n",
    "adoption_by_week['pct_new'] = adoption_by_week['n_new'] / adoption_by_week['n_new'].sum() * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q4: ADOPTION VELOCITY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total cohorts: {len(adoption_by_week)}\")\n",
    "if len(adoption_by_week) > 0:\n",
    "    print(f\"Week 1 adoption: {adoption_by_week['pct_new'].iloc[0]:.1f}%\")\n",
    "    print(f\"Largest cohort: {adoption_by_week['pct_new'].max():.1f}%\")\n",
    "print(\"\\nAdoption by Week:\")\n",
    "print(adoption_by_week.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Q5 - Ashenfelter's Dip\n",
    "\n",
    "# Merge cohort week to panel\n",
    "panel_with_cohort = panel.merge(first_spend, on='VENDOR_ID', how='left')\n",
    "panel_with_cohort['relative_week'] = (\n",
    "    (pd.to_datetime(panel_with_cohort['WEEK']) -\n",
    "     pd.to_datetime(panel_with_cohort['cohort_week'])).dt.days // 7\n",
    ")\n",
    "\n",
    "# Average total_gmv by relative week for treated vendors\n",
    "treated = panel_with_cohort[panel_with_cohort['cohort_week'].notna()]\n",
    "event_study = treated.groupby('relative_week')['TOTAL_GMV'].agg(['mean', 'std', 'count']).reset_index()\n",
    "event_study.columns = ['relative_week', 'mean_gmv', 'std_gmv', 'n_obs']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q5: ASHENFELTER'S DIP (Pre-treatment GMV trajectory)\")\n",
    "print(\"=\"*60)\n",
    "display_range = event_study[(event_study['relative_week'] >= -5) & (event_study['relative_week'] <= 5)]\n",
    "print(display_range.to_string(index=False))\n",
    "\n",
    "# Test for pre-trend\n",
    "pre_treatment = event_study[event_study['relative_week'] < 0]['mean_gmv']\n",
    "if len(pre_treatment) >= 2:\n",
    "    trend = pre_treatment.iloc[-1] - pre_treatment.iloc[0]\n",
    "    print(f\"\\nPre-treatment trend (e=-5 to e=-1): ${trend:.2f}\")\n",
    "    print(f\"Interpretation: {'Declining (Ashenfelter Dip?)' if trend < 0 else 'Stable/Increasing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Q8 - Zero-Inflation (True Population)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q8: ZERO-INFLATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Variable':<20} {'Zero %':>10} {'Non-Zero Mean':>15} {'Non-Zero Median':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col in ['TOTAL_GMV', 'PROMOTED_GMV', 'IMPRESSIONS', 'CLICKS', 'TOTAL_SPEND']:\n",
    "    zero_pct = (panel[col] == 0).sum() / len(panel) * 100\n",
    "    nonzero = panel[panel[col] > 0][col]\n",
    "    mean_val = nonzero.mean() if len(nonzero) > 0 else 0\n",
    "    median_val = nonzero.median() if len(nonzero) > 0 else 0\n",
    "    print(f\"{col:<20} {zero_pct:>9.1f}% {mean_val:>15,.2f} {median_val:>15,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Q9 - Whale Concentration\n",
    "\n",
    "vendor_totals = panel.groupby('VENDOR_ID')['TOTAL_GMV'].sum().sort_values(ascending=False)\n",
    "total_gmv = vendor_totals.sum()\n",
    "n_vendors = len(vendor_totals)\n",
    "\n",
    "# Handle edge cases\n",
    "top_1_n = max(1, int(n_vendors * 0.01))\n",
    "top_10_n = max(1, int(n_vendors * 0.10))\n",
    "\n",
    "top_1_pct = vendor_totals.head(top_1_n).sum() / total_gmv * 100 if total_gmv > 0 else 0\n",
    "top_10_pct = vendor_totals.head(top_10_n).sum() / total_gmv * 100 if total_gmv > 0 else 0\n",
    "\n",
    "# Gini coefficient\n",
    "x = np.sort(vendor_totals.values)\n",
    "n = len(x)\n",
    "if np.sum(x) > 0 and n > 0:\n",
    "    gini = (2 * np.sum((np.arange(1, n+1) * x)) - (n + 1) * np.sum(x)) / (n * np.sum(x))\n",
    "else:\n",
    "    gini = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q9: WHALE CONCENTRATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Vendors: {n_vendors:,}\")\n",
    "print(f\"Total GMV: ${total_gmv:,.0f}\")\n",
    "print(f\"Top 1% ({top_1_n} vendors) share of GMV: {top_1_pct:.1f}%\")\n",
    "print(f\"Top 10% ({top_10_n} vendors) share of GMV: {top_10_pct:.1f}%\")\n",
    "print(f\"Gini coefficient: {gini:.3f}\")\n",
    "print(f\"\\nTop 10 Vendors by GMV:\")\n",
    "print(vendor_totals.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Q10 - Cannibalization\n",
    "\n",
    "# Filter to observations with some activity\n",
    "active = panel[(panel['PROMOTED_GMV'] > 0) | (panel['ORGANIC_GMV'] > 0)]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q10: CANNIBALIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Active observations (promoted or organic GMV > 0): {len(active):,}\")\n",
    "\n",
    "if len(active) > 1:\n",
    "    corr = active['PROMOTED_GMV'].corr(active['ORGANIC_GMV'])\n",
    "    print(f\"Correlation(promoted_gmv, organic_gmv): {corr:.4f}\")\n",
    "    print(f\"Interpretation: {'Substitution' if corr < -0.1 else 'Complement' if corr > 0.1 else 'Independent'}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\nMean Promoted GMV: ${active['PROMOTED_GMV'].mean():,.2f}\")\n",
    "    print(f\"Mean Organic GMV: ${active['ORGANIC_GMV'].mean():,.2f}\")\n",
    "    print(f\"Mean Total GMV: ${active['TOTAL_GMV'].mean():,.2f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 2 - Mechanism Sample (Q6 & Q7)\n",
    "\n",
    "mechanism_query = \"\"\"\n",
    "SELECT\n",
    "    RANKING, FINAL_BID, QUALITY, PACING, CONVERSION_RATE, IS_WINNER\n",
    "FROM AUCTIONS_RESULTS\n",
    "SAMPLE(0.01)\n",
    "WHERE CREATED_AT BETWEEN '2025-09-01' AND '2025-09-07'\n",
    "  AND FINAL_BID IS NOT NULL\n",
    "  AND QUALITY IS NOT NULL\n",
    "  AND PACING IS NOT NULL\n",
    "LIMIT 50000\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching mechanism sample...\")\n",
    "auction_sample = pd.read_sql(mechanism_query, conn)\n",
    "auction_sample.to_parquet(DATA_DIR / 'auction_sample.parquet', index=False)\n",
    "print(f\"Mechanism sample: {len(auction_sample):,} rows\")\n",
    "auction_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Q6 - CPC Verification\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q6: CPC VERIFICATION (FINAL_BID Distribution)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "winners = auction_sample[auction_sample['IS_WINNER'] == True]\n",
    "print(f\"Total bids in sample: {len(auction_sample):,}\")\n",
    "print(f\"Winners: {len(winners):,}\")\n",
    "print(f\"\\nFINAL_BID statistics (all bids):\")\n",
    "print(auction_sample['FINAL_BID'].describe())\n",
    "print(f\"\\nFINAL_BID statistics (winners only):\")\n",
    "print(winners['FINAL_BID'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Q7 - Rank Determinism\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Prepare data\n",
    "X = auction_sample[['FINAL_BID', 'QUALITY', 'PACING']].dropna()\n",
    "y = auction_sample.loc[X.index, 'RANKING']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Q7: RANK DETERMINISM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(X) > 10:\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    r2 = r2_score(y, model.predict(X))\n",
    "    \n",
    "    print(f\"Sample size: {len(X):,}\")\n",
    "    print(f\"R-squared (RANKING ~ FINAL_BID + QUALITY + PACING): {r2:.4f}\")\n",
    "    print(f\"\\nCoefficients:\")\n",
    "    print(f\"  FINAL_BID: {model.coef_[0]:.6f}\")\n",
    "    print(f\"  QUALITY: {model.coef_[1]:.6f}\")\n",
    "    print(f\"  PACING: {model.coef_[2]:.6f}\")\n",
    "    print(f\"  Intercept: {model.intercept_:.6f}\")\n",
    "    print(f\"\\nInterpretation: {'Deterministic' if r2 > 0.9 else 'Some noise' if r2 > 0.5 else 'Noisy'}\")\n",
    "else:\n",
    "    print(\"Insufficient data for regression analysis.\")\n",
    "    r2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Summary Table\n",
    "\n",
    "results = {\n",
    "    'Q1_orphan_rate': f\"{orphan_rate:.1%}\",\n",
    "    'Q2_bidders': f\"{balance['N_BIDDERS'].iloc[0]:,}\",\n",
    "    'Q2_catalog_vendors': f\"{balance['N_CATALOG_VENDORS'].iloc[0]:,}\",\n",
    "    'Q3_flicker_rate': f\"{flicker_rate:.2%}\",\n",
    "    'Q4_n_cohorts': len(adoption_by_week),\n",
    "    'Q4_week1_pct': f\"{adoption_by_week['pct_new'].iloc[0]:.1f}%\" if len(adoption_by_week) > 0 else 'N/A',\n",
    "    'Q8_total_gmv_zero_pct': f\"{(panel['TOTAL_GMV'] == 0).mean()*100:.1f}%\",\n",
    "    'Q9_top1_pct': f\"{top_1_pct:.1f}%\",\n",
    "    'Q9_gini': f\"{gini:.3f}\",\n",
    "    'Q10_cannibalization_corr': f\"{corr:.4f}\" if 'corr' in dir() and len(active) > 1 else 'N/A',\n",
    "    'Q6_n_winners': f\"{len(winners):,}\",\n",
    "    'Q7_rank_r2': f\"{r2:.4f}\"\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<30} {'Value':<20}\")\n",
    "print(\"-\"*50)\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:<30} {v:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Close Connection\n",
    "\n",
    "conn.close()\n",
    "print(\"[OK] Snowflake connection closed\")\n",
    "print(f\"\\nData saved to:\")\n",
    "print(f\"  - {DATA_DIR / 'vendor_week_panel.parquet'}\")\n",
    "print(f\"  - {DATA_DIR / 'auction_sample.parquet'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

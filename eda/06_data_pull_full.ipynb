{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pull - Full 365 Day Window for Fixed Effects Analysis\n",
    "\n",
    "This notebook pulls a continuous 365-day window of data for macro-session based fixed effects analysis.\n",
    "\n",
    "Key features:\n",
    "- Single continuous 365-day data window\n",
    "- CTE-based sampling strategy for efficiency\n",
    "- All filtering happens in Snowflake (no large IN clauses)\n",
    "- Proper ID standardization across all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Suppress pandas SQLAlchemy warning\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    message='pandas only supports SQLAlchemy connectable.*'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load environment variables from parent directory\nload_dotenv('../.env')  # Load from parent directory since .env is at project root\n\n# Time windows - 365 day window\nANALYSIS_END_DATE = date(2025, 9, 2)\nTOTAL_PULL_DAYS = 365\nANALYSIS_START_DATE = ANALYSIS_END_DATE - timedelta(days=TOTAL_PULL_DAYS)\n\n# Sampling parameters\nSAMPLING_FRACTION = 0.0005  # 0.05% of users for 365-day window\n\n# Output paths\nDATA_DIR = Path('data')\nDATA_DIR.mkdir(exist_ok=True)\n\nprint(\"Configuration:\")\nprint(f\"  Analysis period: {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")\nprint(f\"  Total days: {TOTAL_PULL_DAYS}\")\nprint(f\"  Sampling fraction: {SAMPLING_FRACTION:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Snowflake connection established.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"[SUCCESS] Snowflake connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] Could not connect to Snowflake: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Data Fetching Functions with CTE Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sampling_cte(start_date: str, end_date: str, sampling_fraction: float) -> str:\n",
    "    \"\"\"\n",
    "    Build CTE for deterministic user sampling using hash-based bucketing.\n",
    "    This ensures reproducible sampling across runs.\n",
    "    \"\"\"\n",
    "    total_buckets = 10000\n",
    "    selection_threshold = int(total_buckets * sampling_fraction)\n",
    "    \n",
    "    return textwrap.dedent(f\"\"\"\n",
    "        WITH SAMPLED_USER_IDS AS (\n",
    "            WITH ALL_USERS AS (\n",
    "                -- Get all unique users from auctions in the time window\n",
    "                SELECT DISTINCT OPAQUE_USER_ID AS USER_ID\n",
    "                FROM AUCTIONS_USERS\n",
    "                WHERE CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "                  AND OPAQUE_USER_ID IS NOT NULL\n",
    "            ),\n",
    "            BUCKETED_USERS AS (\n",
    "                SELECT\n",
    "                    USER_ID,\n",
    "                    MOD(ABS(HASH(USER_ID)), {total_buckets}) AS bucket\n",
    "                FROM ALL_USERS\n",
    "            )\n",
    "            SELECT USER_ID\n",
    "            FROM BUCKETED_USERS\n",
    "            WHERE bucket < {selection_threshold}\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_users(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_USERS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_USERS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            au.OPAQUE_USER_ID AS USER_ID,\n",
    "            au.CREATED_AT,\n",
    "            DATE(au.CREATED_AT) AS auction_date,\n",
    "            HOUR(au.CREATED_AT) AS auction_hour,\n",
    "            DAYOFWEEK(au.CREATED_AT) AS auction_dow,\n",
    "            WEEKOFYEAR(au.CREATED_AT) AS auction_week\n",
    "        FROM AUCTIONS_USERS au\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY au.OPAQUE_USER_ID, au.CREATED_AT\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} auction records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_results(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_RESULTS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_RESULTS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "            LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n",
    "            LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            ar.RANKING AS bid_rank,\n",
    "            ar.IS_WINNER,\n",
    "            ar.CREATED_AT AS bid_time\n",
    "        FROM AUCTIONS_RESULTS ar\n",
    "        JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} bid records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impressions(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract IMPRESSIONS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting IMPRESSIONS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            i.INTERACTION_ID AS impression_id,\n",
    "            LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            i.USER_ID,\n",
    "            LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            i.OCCURRED_AT AS impression_time\n",
    "        FROM IMPRESSIONS i\n",
    "        JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "        WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} impression records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clicks(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract CLICKS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting CLICKS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            c.INTERACTION_ID AS click_id,\n",
    "            LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.USER_ID,\n",
    "            LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            c.OCCURRED_AT AS click_time\n",
    "        FROM CLICKS c\n",
    "        JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "        WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} click records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purchases(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract PURCHASES table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting PURCHASES...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            p.PURCHASE_ID,\n",
    "            p.PURCHASED_AT AS purchase_time,\n",
    "            LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            p.QUANTITY,\n",
    "            p.UNIT_PRICE,\n",
    "            p.USER_ID,\n",
    "            p.PURCHASE_LINE,\n",
    "            (p.QUANTITY * p.UNIT_PRICE) AS revenue\n",
    "        FROM PURCHASES p\n",
    "        JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "        WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} purchase records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_catalog_and_get_product_ids(conn, start_date: str, end_date: str, sampling_fraction: float) -> tuple[pd.DataFrame, set]:\n",
    "    \"\"\"\n",
    "    Combines product ID collection and catalog extraction into a single, efficient query.\n",
    "    Uses CTE to collect all product IDs from sampled users' events.\n",
    "    \"\"\"\n",
    "    print(\"\\nExtracting CATALOG and collecting all PRODUCT_IDs using CTE...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\",\n",
    "        ALL_PRODUCT_IDS AS (\n",
    "            -- Collect all product IDs from all relevant tables for our sampled users\n",
    "            SELECT DISTINCT LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM AUCTIONS_RESULTS ar\n",
    "            JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "            JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "            WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND ar.PRODUCT_ID IS NOT NULL\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            SELECT DISTINCT LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM IMPRESSIONS i\n",
    "            JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "            WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND i.PRODUCT_ID IS NOT NULL\n",
    "\n",
    "            UNION\n",
    "\n",
    "            SELECT DISTINCT LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM CLICKS c\n",
    "            JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "            WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND c.PRODUCT_ID IS NOT NULL\n",
    "\n",
    "            UNION\n",
    "\n",
    "            SELECT DISTINCT LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM PURCHASES p\n",
    "            JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "            WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND p.PRODUCT_ID IS NOT NULL\n",
    "        )\n",
    "        -- Now, fetch the catalog data for exactly those products\n",
    "        SELECT\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.NAME AS product_name,\n",
    "            c.PRICE AS catalog_price,\n",
    "            c.ACTIVE AS is_active,\n",
    "            c.IS_DELETED,\n",
    "            c.DESCRIPTION,\n",
    "            c.VENDORS,\n",
    "            c.CATEGORIES,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'brand#%%'), ''), '#', 2) AS BRAND,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'department#%%'), ''), '#', 2) AS DEPARTMENT_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category#%%'), ''), '#', 2) AS CATEGORY_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'color#%%'), ''), '#', 2) AS PRIMARY_COLOR,\n",
    "            REPLACE(\n",
    "                ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'style_tag#%%'), ', '),\n",
    "                'style_tag#', ''\n",
    "            ) AS STYLE_TAGS\n",
    "        FROM CATALOG c\n",
    "        JOIN ALL_PRODUCT_IDS ap ON LOWER(TRIM(c.PRODUCT_ID)) = ap.PRODUCT_ID\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # FIX: Standardize all column names to lowercase to prevent KeyErrors\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "\n",
    "    # Get the set of product IDs from the resulting dataframe\n",
    "    product_ids = set(df['product_id'].unique())\n",
    "    print(f\"  Extracted {len(df):,} catalog records for {len(product_ids):,} unique products\")\n",
    "    \n",
    "    return df, product_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Data Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING DATA EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "--- Extracting event data for analysis period ---\n",
      "\n",
      "Extracting AUCTIONS_USERS...\n",
      "  Extracted 657,597 auction records\n",
      "\n",
      "Extracting AUCTIONS_RESULTS...\n",
      "  Extracted 26,362,985 bid records\n",
      "\n",
      "Extracting IMPRESSIONS...\n",
      "  Extracted 2,459,435 impression records\n",
      "\n",
      "Extracting CLICKS...\n",
      "  Extracted 79,031 click records\n",
      "\n",
      "Extracting PURCHASES...\n",
      "  Extracted 11,215 purchase records\n",
      "\n",
      "Extracting CATALOG and collecting all PRODUCT_IDs using CTE...\n",
      "  Extracted 6,842,400 catalog records for 6,842,400 unique products\n",
      "\n",
      "[SUCCESS] Snowflake connection closed\n"
     ]
    }
   ],
   "source": [
    "if conn:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING DATA EXTRACTION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create timestamp for this extraction run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Convert dates to strings for SQL\n",
    "    start_date_str = ANALYSIS_START_DATE.strftime('%Y-%m-%d')\n",
    "    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Extract main event tables\n",
    "    print(\"\\n--- Extracting event data for analysis period ---\")\n",
    "    auctions_users = extract_auctions_users(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    auctions_results = extract_auctions_results(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    impressions = extract_impressions(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    clicks = extract_clicks(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    purchases = extract_purchases(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # Extract catalog with product IDs\n",
    "    catalog, product_ids = extract_catalog_and_get_product_ids(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"\\n[SUCCESS] Snowflake connection closed\")\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] No Snowflake connection available. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA VALIDATION SUMMARY\n",
      "==================================================\n",
      "\n",
      "Record Counts:\n",
      "  Auctions:    657,597\n",
      "  Bids:        26,362,985\n",
      "  Impressions: 2,459,435\n",
      "  Clicks:      79,031\n",
      "  Purchases:   11,215\n",
      "  Products (Catalog): 6,842,400\n",
      "\n",
      "Unique Entities:\n",
      "  Users:       8,350\n",
      "  Vendors:     143,268\n",
      "  Campaigns:   787,356\n",
      "  Products (from Events): 6,842,400\n",
      "\n",
      "Date Ranges:\n",
      "  Auctions:    2025-03-14 00:00:15.367000 to 2025-09-01 23:59:31.293000\n",
      "  Bids:        2025-03-14 00:00:15.407000 to 2025-09-01 23:59:31.302000\n",
      "  Impressions: 2025-03-14 00:00:18 to 2025-09-01 23:59:59\n",
      "  Clicks:      2025-03-14 00:02:16 to 2025-09-01 23:57:08\n",
      "  Purchases:   2025-03-14 00:04:05 to 2025-09-01 23:53:49\n",
      "\n",
      "Conversion Funnel:\n",
      "  Winning Bids:     20,191,987\n",
      "  Impressions:      2,459,435 (12.2% of winning bids)\n",
      "  Clicks:           79,031 (3.2% CTR)\n",
      "  Purchase Events:  11,215\n",
      "\n",
      "Missing Data Check:\n",
      "  Auctions: No missing values\n",
      "  Bids: No missing values\n",
      "  Impressions: No missing values\n",
      "  Clicks: No missing values\n",
      "  Purchases: No missing values\n",
      "  Catalog: No missing values\n"
     ]
    }
   ],
   "source": [
    "def validate_data(auctions_users, auctions_results, impressions, clicks, purchases, catalog, product_ids):\n",
    "    \"\"\"Validate data integrity and print summary statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic counts\n",
    "    print(\"\\nRecord Counts:\")\n",
    "    print(f\"  Auctions:    {len(auctions_users):,}\")\n",
    "    print(f\"  Bids:        {len(auctions_results):,}\")\n",
    "    print(f\"  Impressions: {len(impressions):,}\")\n",
    "    print(f\"  Clicks:      {len(clicks):,}\")\n",
    "    print(f\"  Purchases:   {len(purchases):,}\")\n",
    "    print(f\"  Products (Catalog): {len(catalog):,}\")\n",
    "    \n",
    "    # Unique counts\n",
    "    print(\"\\nUnique Entities:\")\n",
    "    print(f\"  Users:       {auctions_users['user_id'].nunique():,}\")\n",
    "    print(f\"  Vendors:     {auctions_results['vendor_id'].nunique():,}\")\n",
    "    print(f\"  Campaigns:   {auctions_results['campaign_id'].nunique():,}\")\n",
    "    print(f\"  Products (from Events): {len(product_ids):,}\")\n",
    "    \n",
    "    # Date ranges (using standardized lowercase column names)\n",
    "    print(\"\\nDate Ranges:\")\n",
    "    try:\n",
    "        print(f\"  Auctions:    {auctions_users['created_at'].min()} to {auctions_users['created_at'].max()}\")\n",
    "        print(f\"  Bids:        {auctions_results['bid_time'].min()} to {auctions_results['bid_time'].max()}\")\n",
    "        print(f\"  Impressions: {impressions['impression_time'].min()} to {impressions['impression_time'].max()}\")\n",
    "        print(f\"  Clicks:      {clicks['click_time'].min()} to {clicks['click_time'].max()}\")\n",
    "        print(f\"  Purchases:   {purchases['purchase_time'].min()} to {purchases['purchase_time'].max()}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"  ERROR generating date ranges: A timestamp column was not found -> {e}\")\n",
    "\n",
    "    # Conversion funnel\n",
    "    print(\"\\nConversion Funnel:\")\n",
    "    winning_bids = len(auctions_results[auctions_results['is_winner'] == True])\n",
    "    print(f\"  Winning Bids:     {winning_bids:,}\")\n",
    "    if winning_bids > 0 and len(impressions) > 0:\n",
    "        print(f\"  Impressions:      {len(impressions):,} ({len(impressions)/winning_bids*100:.1f}% of winning bids)\")\n",
    "    if len(impressions) > 0 and len(clicks) > 0:\n",
    "        print(f\"  Clicks:           {len(clicks):,} ({len(clicks)/len(impressions)*100:.1f}% CTR)\")\n",
    "    print(f\"  Purchase Events:  {len(purchases):,}\")\n",
    "    \n",
    "    # Missing data check\n",
    "    print(\"\\nMissing Data Check:\")\n",
    "    for df_name, df in [(\"Auctions\", auctions_users), (\"Bids\", auctions_results), \n",
    "                        (\"Impressions\", impressions), (\"Clicks\", clicks),\n",
    "                        (\"Purchases\", purchases), (\"Catalog\", catalog)]:\n",
    "        if not df.empty:\n",
    "            max_missing_pct = (df.isnull().sum().max() / len(df)) * 100\n",
    "            if max_missing_pct > 0:\n",
    "                print(f\"  {df_name}: {max_missing_pct:.2f}% max missing\")\n",
    "            else:\n",
    "                print(f\"  {df_name}: No missing values\")\n",
    "        else:\n",
    "            print(f\"  {df_name}: Empty DataFrame\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN THE VALIDATION FUNCTION ON YOUR REAL DATA\n",
    "# ==============================================================================\n",
    "if 'auctions_users' in locals() and not auctions_users.empty:\n",
    "    validate_data(auctions_users, auctions_results, impressions, clicks, purchases, catalog, product_ids)\n",
    "else:\n",
    "    print(\"DataFrames not loaded. Please re-run the main data extraction pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create directory: /Volumes/rawat/data/marketplace-data\n",
      "[SUCCESS] Directory exists and is ready.\n",
      "[SUCCESS] Wrote a test file to: /Volumes/rawat/data/marketplace-data/write_test.txt\n",
      "\n",
      "The path is valid and writable. You can now run the main data-saving cell.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "# 1. Define the full path to your new directory on the external drive\n",
    "output_dir = Path(\"/Volumes/rawat/data/marketplace-data\")\n",
    "\n",
    "# 2. Create the directory. \n",
    "#    - `parents=True` creates any missing parent folders (like 'data').\n",
    "#    - `exist_ok=True` means it won't crash if the folder already exists.\n",
    "try:\n",
    "    print(f\"Attempting to create directory: {output_dir}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[SUCCESS] Directory exists and is ready.\")\n",
    "\n",
    "    # 3. Create a small test file to confirm write access.\n",
    "    test_file_path = output_dir / \"write_test.txt\"\n",
    "    timestamp_message = f\"Successfully wrote this file at: {datetime.datetime.now()}\"\n",
    "    \n",
    "    test_file_path.write_text(timestamp_message)\n",
    "    \n",
    "    print(f\"[SUCCESS] Wrote a test file to: {test_file_path}\")\n",
    "    print(\"\\nThe path is valid and writable. You can now run the main data-saving cell.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] An error occurred. Please check the path and drive permissions.\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving data checkpoint ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet files: 100%|██████████| 6/6 [04:53<00:00, 48.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All data files saved successfully.\n",
      "Saved metadata to metadata_365d.json\n",
      "\n",
      "================================================================================\n",
      "DATA EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "All data saved to data/\n",
      "Ready for macro-session processing and fixed effects analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'auctions_users' in locals():\n",
    "    print(\"\\n--- Saving data checkpoint ---\")\n",
    "    output_dir = Path(\"./data\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    datasets = [\n",
    "        (\"auctions_users\", auctions_users),\n",
    "        (\"auctions_results\", auctions_results),\n",
    "        (\"impressions\", impressions),\n",
    "        (\"clicks\", clicks),\n",
    "        (\"purchases\", purchases),\n",
    "        (\"catalog\", catalog)\n",
    "    ]\n",
    "    \n",
    "    # Using tqdm without an inner print statement for a cleaner progress bar\n",
    "    for name, df in tqdm(datasets, desc=\"Saving Parquet files\"):\n",
    "        path = output_dir / f\"{name}_365d.parquet\"\n",
    "        df.to_parquet(path, index=False)\n",
    "    \n",
    "    print(\"\\nAll data files saved successfully.\")\n",
    "\n",
    "    # Create metadata file\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'analysis_start_date': start_date_str,\n",
    "        'analysis_end_date': end_date_str,\n",
    "        'total_days': TOTAL_PULL_DAYS,\n",
    "        'sampling_fraction': SAMPLING_FRACTION,\n",
    "        'total_products': len(product_ids),\n",
    "        'row_counts': {name: len(df) for name, df in datasets}\n",
    "    }\n",
    "    \n",
    "    metadata_path = output_dir / f\"metadata_365d.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Saved metadata to {metadata_path.name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"All data saved to {output_dir}/\")\n",
    "    print(f\"Ready for macro-session processing and fixed effects analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying data checkpoint in 'data' (using Polars) ---\n",
      "\n",
      "✅ Directory 'data' found.\n",
      "✅ Metadata file 'metadata_365d.json' loaded successfully.\n",
      "\n",
      "--- Verifying Parquet files against metadata ---\n",
      "\n",
      "Checking: auctions_users_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (657,597 rows)\n",
      "\n",
      "Checking: auctions_results_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (26,362,985 rows)\n",
      "\n",
      "Checking: impressions_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (2,459,435 rows)\n",
      "\n",
      "Checking: clicks_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (79,031 rows)\n",
      "\n",
      "Checking: purchases_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (11,215 rows)\n",
      "\n",
      "Checking: catalog_365d.parquet\n",
      "  - File is readable: ✅\n",
      "  - Row count matches metadata: ✅ (6,842,400 rows)\n",
      "\n",
      "==================================================\n",
      "✅ VERIFICATION COMPLETE: All checks passed!\n",
      "   All expected files exist and their row counts match the metadata.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def verify_data_checkpoint_polars():\n",
    "    \"\"\"\n",
    "    Checks the integrity of the saved data using the Polars library.\n",
    "    It verifies file existence and compares row counts in Parquet files\n",
    "    against the metadata JSON.\n",
    "    \"\"\"\n",
    "    output_dir = Path(\"./data\")\n",
    "    print(f\"--- Verifying data checkpoint in '{output_dir}' (using Polars) ---\\n\")\n",
    "\n",
    "    # 1. Check if the output directory exists\n",
    "    if not output_dir.is_dir():\n",
    "        print(f\"❌ ERROR: Directory '{output_dir}' not found. Did the script run correctly?\")\n",
    "        sys.exit(1)\n",
    "    print(f\"✅ Directory '{output_dir}' found.\")\n",
    "\n",
    "    # 2. Check for and load the metadata file\n",
    "    metadata_path = output_dir / \"metadata_365d.json\"\n",
    "    if not metadata_path.is_file():\n",
    "        print(f\"❌ ERROR: Metadata file '{metadata_path.name}' not found.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ Metadata file '{metadata_path.name}' loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Could not read or parse metadata file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\n--- Verifying Parquet files against metadata ---\")\n",
    "    all_checks_passed = True\n",
    "    \n",
    "    # Get the expected row counts from the metadata file\n",
    "    expected_row_counts = metadata.get('row_counts')\n",
    "    if not expected_row_counts:\n",
    "        print(\"❌ ERROR: 'row_counts' key not found in metadata file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Iterate through datasets listed in metadata and check each one\n",
    "    for name, expected_count in expected_row_counts.items():\n",
    "        file_path = output_dir / f\"{name}_365d.parquet\"\n",
    "        print(f\"\\nChecking: {file_path.name}\")\n",
    "\n",
    "        # Check file existence\n",
    "        if not file_path.is_file():\n",
    "            print(f\"  ❌ ERROR: File not found.\")\n",
    "            all_checks_passed = False\n",
    "            continue\n",
    "\n",
    "        # Try to read the file with Polars and check its height (row count)\n",
    "        try:\n",
    "            # Use read_parquet from Polars\n",
    "            df = pl.read_parquet(file_path)\n",
    "            \n",
    "            # In Polars, .height gives the number of rows\n",
    "            actual_count = df.height\n",
    "            print(f\"  - File is readable: ✅\")\n",
    "            \n",
    "            # Compare row counts\n",
    "            if actual_count == expected_count:\n",
    "                print(f\"  - Row count matches metadata: ✅ ({actual_count:,} rows)\")\n",
    "            else:\n",
    "                print(f\"  - Row count MISMATCH: ❌ (Actual: {actual_count:,}, Expected: {expected_count:,})\")\n",
    "                all_checks_passed = False\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ ERROR: Failed to read Parquet file with Polars: {e}\")\n",
    "            all_checks_passed = False\n",
    "\n",
    "    # 4. Final summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_checks_passed:\n",
    "        print(\"✅ VERIFICATION COMPLETE: All checks passed!\")\n",
    "        print(\"   All expected files exist and their row counts match the metadata.\")\n",
    "    else:\n",
    "        print(\"❌ VERIFICATION FAILED: Issues were found. Please review the errors above.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you have polars and a parquet engine (like pyarrow) installed\n",
    "    # pip install polars pyarrow\n",
    "    verify_data_checkpoint_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (1.33.0)\n",
      "Requirement already satisfied: pyarrow in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install polars pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "if 'auctions_users' in locals():\n",
    "    del auctions_users, auctions_results, impressions, clicks, purchases, catalog\n",
    "    gc.collect()\n",
    "    print(\"✓ Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
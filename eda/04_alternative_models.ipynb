{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_alternative_models.ipynb\n",
    "## Survival Analysis and Discrete Choice Modeling\n",
    "\n",
    "This notebook explores advanced causal models beyond binary purchase outcomes:\n",
    "1. **Survival Analysis (Cox Model)**: Models time-to-conversion to understand how ads affect purchase timing\n",
    "2. **Discrete Choice Modeling**: Models product selection to understand how ads influence which products are chosen\n",
    "\n",
    "### Key Research Questions:\n",
    "- Do ad clicks accelerate the purchase decision (survival)?\n",
    "- Do clicked products have higher probability of being chosen from consideration sets (choice)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:03:41] ================================================================================\n",
      "[2025-09-23 06:03:41] SURVIVAL ANALYSIS AND DISCRETE CHOICE MODELING\n",
      "[2025-09-23 06:03:41] ================================================================================\n",
      "[2025-09-23 06:03:41] Analysis timestamp: 20250923_060341\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Statistical imports\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize logging\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_log = []\n",
    "\n",
    "def log(message: str):\n",
    "    \"\"\"Add message to output log with timestamp\"\"\"\n",
    "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_entry = f\"[{ts}] {message}\"\n",
    "    output_log.append(log_entry)\n",
    "    print(log_entry)\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"SURVIVAL ANALYSIS AND DISCRETE CHOICE MODELING\")\n",
    "log(\"=\"*80)\n",
    "log(f\"Analysis timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:03:46] \n",
      "============================================================\n",
      "[2025-09-23 06:03:46] LOADING CHECKPOINT DATA\n",
      "[2025-09-23 06:03:46] ============================================================\n",
      "[2025-09-23 06:03:46] Loading data from extraction: 20250923_043038\n",
      "[2025-09-23 06:03:46]   Loaded auctions_users: 88,690 rows\n",
      "[2025-09-23 06:03:47]   Loaded auctions_results: 3,410,770 rows\n",
      "[2025-09-23 06:03:47]   Loaded impressions: 347,741 rows\n",
      "[2025-09-23 06:03:47]   Loaded clicks: 11,215 rows\n",
      "[2025-09-23 06:03:47]   Loaded purchases: 1,859 rows\n",
      "[2025-09-23 06:03:50]   Loaded catalog: 4,961,480 rows\n",
      "[2025-09-23 06:03:50]   Loaded hist_purchases: 10,332 rows\n",
      "[2025-09-23 06:03:51]   Loaded hist_impressions: 1,685,675 rows\n",
      "[2025-09-23 06:03:51]   Loaded hist_clicks: 59,691 rows\n",
      "[2025-09-23 06:03:51] \n",
      "✓ Raw data loaded successfully\n",
      "[2025-09-23 06:03:51] ✓ Processed metrics loaded: 269,276 rows\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint data\n",
    "raw_data_dir = Path(\"./data/raw\")\n",
    "processed_data_path = Path(\"./data/user_journey_causal_dataset.parquet\")\n",
    "\n",
    "def load_checkpoint_data() -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
    "    \"\"\"Load the most recent checkpoint data from raw directory.\"\"\"\n",
    "    \n",
    "    if not raw_data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {raw_data_dir}\")\n",
    "    \n",
    "    # Find most recent metadata file\n",
    "    metadata_files = list(raw_data_dir.glob(\"metadata_*.json\"))\n",
    "    if not metadata_files:\n",
    "        raise FileNotFoundError(\"No metadata files found. Please run 01_data_pull.ipynb first.\")\n",
    "    \n",
    "    latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(latest_metadata, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    extraction_timestamp = metadata['timestamp']\n",
    "    log(f\"Loading data from extraction: {extraction_timestamp}\")\n",
    "    \n",
    "    # Load all data files\n",
    "    data = {}\n",
    "    required_files = [\n",
    "        'auctions_users', 'auctions_results', 'impressions', \n",
    "        'clicks', 'purchases', 'catalog'\n",
    "    ]\n",
    "    optional_files = ['hist_purchases', 'hist_impressions', 'hist_clicks']\n",
    "    \n",
    "    for file_type in required_files + optional_files:\n",
    "        pattern = f\"{file_type}_{extraction_timestamp}.parquet\"\n",
    "        file_path = raw_data_dir / pattern\n",
    "        \n",
    "        if file_path.exists():\n",
    "            data[file_type] = pd.read_parquet(file_path)\n",
    "            log(f\"  Loaded {file_type}: {len(data[file_type]):,} rows\")\n",
    "        elif file_type in required_files:\n",
    "            raise FileNotFoundError(f\"Required file not found: {file_path}\")\n",
    "        else:\n",
    "            log(f\"  Optional file not found: {file_type}\")\n",
    "    \n",
    "    return data, metadata\n",
    "\n",
    "# Load the data\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"LOADING CHECKPOINT DATA\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    data, metadata = load_checkpoint_data()\n",
    "    \n",
    "    # Extract individual dataframes\n",
    "    auctions_users = data['auctions_users']\n",
    "    auctions_results = data['auctions_results']\n",
    "    impressions = data['impressions']\n",
    "    clicks = data['clicks']\n",
    "    purchases = data['purchases']\n",
    "    catalog = data['catalog']\n",
    "    \n",
    "    # Historical data (optional)\n",
    "    hist_purchases = data.get('hist_purchases', pd.DataFrame())\n",
    "    hist_impressions = data.get('hist_impressions', pd.DataFrame())\n",
    "    hist_clicks = data.get('hist_clicks', pd.DataFrame())\n",
    "    \n",
    "    log(\"\\n✓ Raw data loaded successfully\")\n",
    "    \n",
    "    # Load processed metrics\n",
    "    if processed_data_path.exists():\n",
    "        metrics = pd.read_parquet(processed_data_path)\n",
    "        log(f\"✓ Processed metrics loaded: {len(metrics):,} rows\")\n",
    "    else:\n",
    "        log(\"ERROR: Processed metrics not found. Please run 02_analysis.ipynb first.\")\n",
    "        raise FileNotFoundError(f\"Processed data not found: {processed_data_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"ERROR: Failed to load data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Build Event Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:03:51] \n",
      "============================================================\n",
      "[2025-09-23 06:03:51] BUILDING EVENT STREAMS\n",
      "[2025-09-23 06:03:51] ============================================================\n",
      "[2025-09-23 06:03:51] Creating unified event stream...\n",
      "[2025-09-23 06:03:51] Total events: 449,505\n",
      "[2025-09-23 06:03:51] Unique journeys: 10,360\n",
      "[2025-09-23 06:03:51] \n",
      "Enhancing event stream with metadata...\n",
      "[2025-09-23 06:03:58] Enhanced events created: 449,505 records\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"BUILDING EVENT STREAMS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Build all_events dataframe\n",
    "log(\"Creating unified event stream...\")\n",
    "events = []\n",
    "\n",
    "# Add auctions\n",
    "auctions = auctions_users.copy()\n",
    "auctions['event_type'] = 'auction'\n",
    "auctions['event_time'] = pd.to_datetime(auctions['CREATED_AT'])\n",
    "auctions['PRODUCT_ID'] = None\n",
    "auctions['VENDOR_ID'] = None\n",
    "events.append(auctions[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add impressions\n",
    "impressions_evt = impressions.copy()\n",
    "impressions_evt['event_type'] = 'impression'\n",
    "impressions_evt['event_time'] = pd.to_datetime(impressions_evt['OCCURRED_AT'])\n",
    "events.append(impressions_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add clicks\n",
    "clicks_evt = clicks.copy()\n",
    "clicks_evt['event_type'] = 'click'\n",
    "clicks_evt['event_time'] = pd.to_datetime(clicks_evt['OCCURRED_AT'])\n",
    "events.append(clicks_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add purchases\n",
    "purchases_evt = purchases.copy()\n",
    "purchases_evt['event_type'] = 'purchase'\n",
    "purchases_evt['event_time'] = pd.to_datetime(purchases_evt['PURCHASED_AT'])\n",
    "purchases_evt['AUCTION_ID'] = None\n",
    "purchases_evt['VENDOR_ID'] = None\n",
    "events.append(purchases_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Combine and sort\n",
    "all_events = pd.concat(events, ignore_index=True)\n",
    "all_events = all_events.sort_values(['USER_ID', 'event_time'])\n",
    "\n",
    "# Create journey IDs using session gaps\n",
    "SESSION_GAP_HOURS = metadata.get('session_gap_hours', 2)\n",
    "all_events['prev_time'] = all_events.groupby('USER_ID')['event_time'].shift()\n",
    "all_events['time_diff'] = (all_events['event_time'] - all_events['prev_time']).dt.total_seconds() / 3600\n",
    "all_events['session_break'] = (all_events['time_diff'] >= SESSION_GAP_HOURS) | all_events['time_diff'].isna()\n",
    "all_events['journey_id'] = all_events.groupby('USER_ID')['session_break'].cumsum()\n",
    "all_events['journey_id'] = all_events['USER_ID'] + '_' + all_events['journey_id'].astype(str)\n",
    "\n",
    "log(f\"Total events: {len(all_events):,}\")\n",
    "log(f\"Unique journeys: {all_events['journey_id'].nunique():,}\")\n",
    "\n",
    "# Build enhanced event stream with metadata\n",
    "log(\"\\nEnhancing event stream with metadata...\")\n",
    "\n",
    "# Add ranking information\n",
    "auction_ranks = auctions_results[['AUCTION_ID', 'PRODUCT_ID', 'RANKING']].copy()\n",
    "auction_ranks = auction_ranks.dropna()\n",
    "auction_ranks = auction_ranks.groupby(['AUCTION_ID', 'PRODUCT_ID'])['RANKING'].min().reset_index()\n",
    "\n",
    "all_events_enhanced = all_events.merge(\n",
    "    auction_ranks, \n",
    "    on=['AUCTION_ID', 'PRODUCT_ID'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add catalog information\n",
    "all_events_enhanced = all_events_enhanced.merge(\n",
    "    catalog[['PRODUCT_ID', 'BRAND', 'DEPARTMENT_ID', 'PRICE']].drop_duplicates(),\n",
    "    on='PRODUCT_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "all_events_enhanced = all_events_enhanced.sort_values(['USER_ID', 'event_time'])\n",
    "log(f\"Enhanced events created: {len(all_events_enhanced):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Helper Functions for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data preparation\n",
    "def _ensure_datetime(df, col):\n",
    "    \"\"\"Ensure column is datetime type\"\"\"\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def _safe_numeric(df, cols):\n",
    "    \"\"\"Safely convert columns to numeric\"\"\"\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Survival Analysis - Time to Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lifelines\n",
      "  Using cached lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting scikit-survival\n",
      "  Downloading scikit_survival-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from lifelines) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from lifelines) (1.16.2)\n",
      "Requirement already satisfied: pandas>=2.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from lifelines) (2.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from lifelines) (3.10.6)\n",
      "Collecting autograd>=1.5 (from lifelines)\n",
      "  Using cached autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "  Using cached autograd_gamma-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from lifelines) (1.2.0)\n",
      "Collecting ecos (from scikit-survival)\n",
      "  Using cached ecos-2.0.14-cp313-cp313-macosx_14_0_arm64.whl\n",
      "Requirement already satisfied: joblib in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from scikit-survival) (1.5.2)\n",
      "Collecting numexpr (from scikit-survival)\n",
      "  Downloading numexpr-2.12.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting osqp<1.0.0,>=0.6.3 (from scikit-survival)\n",
      "  Downloading osqp-0.6.7.post3-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from scikit-survival) (1.7.2)\n",
      "Collecting qdldl (from osqp<1.0.0,>=0.6.3->scikit-survival)\n",
      "  Downloading qdldl-0.1.7.post5-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.17 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.17.0rc1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines) (2.0.0rc2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from pandas>=2.1->lifelines) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from pandas>=2.1->lifelines) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pranjal/Code/marketplace-incrementality/venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
      "Using cached lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "Downloading scikit_survival-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (811 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.8/811.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading osqp-0.6.7.post3-cp313-cp313-macosx_11_0_arm64.whl (237 kB)\n",
      "Using cached autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading numexpr-2.12.1-cp313-cp313-macosx_11_0_arm64.whl (144 kB)\n",
      "Downloading qdldl-0.1.7.post5-cp313-cp313-macosx_11_0_arm64.whl (103 kB)\n",
      "Installing collected packages: numexpr, autograd, qdldl, ecos, autograd-gamma, osqp, scikit-survival, lifelines\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [lifelines]/8\u001b[0m [lifelines]\n",
      "\u001b[1A\u001b[2KSuccessfully installed autograd-1.8.0 autograd-gamma-0.5.0 ecos-2.0.14 lifelines-0.30.0 numexpr-2.12.1 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-survival-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lifelines scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:04:18] \n",
      "================================================================================\n",
      "[2025-09-23 06:04:18] PART 1: SURVIVAL ANALYSIS - TIME TO CONVERSION\n",
      "[2025-09-23 06:04:18] ================================================================================\n",
      "[2025-09-23 06:04:18] Building survival dataset...\n",
      "[2025-09-23 06:04:18] Survival dataset created: 8,523 journeys\n",
      "[2025-09-23 06:04:18]   Journeys with conversion: 816 (9.57%)\n",
      "[2025-09-23 06:04:18]   Mean duration: 0.64 hours\n",
      "[2025-09-23 06:04:18]   Median duration: 0.12 hours\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 1: SURVIVAL ANALYSIS - TIME TO CONVERSION\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "def build_survival_dataset(all_events: pd.DataFrame, metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build journey-level dataset for survival analysis\"\"\"\n",
    "    log(\"Building survival dataset...\")\n",
    "    \n",
    "    _ensure_datetime(all_events, 'event_time')\n",
    "    \n",
    "    # Get journey times\n",
    "    journey_times = (all_events.groupby('journey_id')['event_time']\n",
    "                    .agg(journey_start_time='min', journey_end_time='max')\n",
    "                    .reset_index())\n",
    "    \n",
    "    # Get first purchase time per journey\n",
    "    purchases = all_events[all_events['event_type'] == 'purchase'].copy()\n",
    "    first_purchase = (purchases.groupby('journey_id')['event_time']\n",
    "                      .min().rename('purchase_time').reset_index())\n",
    "    \n",
    "    # Merge times\n",
    "    survival_df = journey_times.merge(first_purchase, on='journey_id', how='left')\n",
    "    \n",
    "    # Calculate durations\n",
    "    survival_df['time_to_purchase_hr'] = (\n",
    "        (survival_df['purchase_time'] - survival_df['journey_start_time'])\n",
    "        .dt.total_seconds() / 3600.0\n",
    "    )\n",
    "    survival_df['total_journey_hr'] = (\n",
    "        (survival_df['journey_end_time'] - survival_df['journey_start_time'])\n",
    "        .dt.total_seconds() / 3600.0\n",
    "    )\n",
    "    \n",
    "    # Duration is time to purchase if purchased, else total journey time\n",
    "    survival_df['duration'] = survival_df['time_to_purchase_hr'].fillna(survival_df['total_journey_hr'])\n",
    "    survival_df['event_observed'] = survival_df['purchase_time'].notna().astype(int)\n",
    "    \n",
    "    # Add journey-level covariates from metrics\n",
    "    journey_covariates = (metrics.groupby('journey_id').agg(\n",
    "        USER_ID=('USER_ID', 'first'),\n",
    "        total_clicks=('clicks_on_product', 'sum'),\n",
    "        total_impressions=('impressions_on_product', 'sum'),\n",
    "        distinct_products_viewed=('PRODUCT_ID', 'nunique'),\n",
    "        avg_price_viewed=('PRICE', 'mean')\n",
    "    ).reset_index())\n",
    "    \n",
    "    # Add user-level historical features if available\n",
    "    user_features = None\n",
    "    if {'USER_ID', 'hist_purchase_count', 'hist_user_ctr'}.issubset(metrics.columns):\n",
    "        user_features = metrics[['USER_ID', 'hist_purchase_count', 'hist_user_ctr']].drop_duplicates('USER_ID')\n",
    "    \n",
    "    # Merge all features\n",
    "    output = survival_df[['journey_id', 'duration', 'event_observed']].merge(\n",
    "        journey_covariates, on='journey_id', how='left'\n",
    "    )\n",
    "    \n",
    "    if user_features is not None:\n",
    "        output = output.merge(user_features, on='USER_ID', how='left')\n",
    "    \n",
    "    # Clean data\n",
    "    output = output.replace([np.inf, -np.inf], np.nan)\n",
    "    output = output.dropna(subset=['duration'])\n",
    "    output = output[output['duration'] > 0].copy()\n",
    "    \n",
    "    log(f\"Survival dataset created: {len(output):,} journeys\")\n",
    "    log(f\"  Journeys with conversion: {output['event_observed'].sum():,} ({output['event_observed'].mean():.2%})\")\n",
    "    log(f\"  Mean duration: {output['duration'].mean():.2f} hours\")\n",
    "    log(f\"  Median duration: {output['duration'].median():.2f} hours\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Build survival dataset\n",
    "final_survival_df = build_survival_dataset(all_events, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:04:21] \n",
      "============================================================\n",
      "[2025-09-23 06:04:21] SURVIVAL ANALYSIS EDA\n",
      "[2025-09-23 06:04:21] ============================================================\n",
      "[2025-09-23 06:04:21] \n",
      "1. OVERALL SURVIVAL CURVE (KAPLAN-MEIER)\n",
      "[2025-09-23 06:04:21] ----------------------------------------\n",
      "[2025-09-23 06:04:21]   Survival at 1h: 0.8421 (conversion: 0.1579)\n",
      "[2025-09-23 06:04:21]   Survival at 2h: 0.7468 (conversion: 0.2532)\n",
      "[2025-09-23 06:04:21]   Survival at 5h: 0.5848 (conversion: 0.4152)\n",
      "[2025-09-23 06:04:21]   Survival at 10h: 0.4804 (conversion: 0.5196)\n",
      "[2025-09-23 06:04:21] \n",
      "2. SURVIVAL BY CLICK STATUS\n",
      "[2025-09-23 06:04:21] ----------------------------------------\n",
      "[2025-09-23 06:04:21]   Journeys with clicks: 3,014 (35.4%)\n",
      "[2025-09-23 06:04:21]   Journeys without clicks: 5,509 (64.6%)\n",
      "[2025-09-23 06:04:21] \n",
      "  Median survival times:\n",
      "[2025-09-23 06:04:21]     With clicks: 8.02 hours\n",
      "[2025-09-23 06:04:21]     Without clicks: 6.47 hours\n",
      "[2025-09-23 06:04:21] \n",
      "3. LOG-RANK TEST\n",
      "[2025-09-23 06:04:21] ----------------------------------------\n",
      "[2025-09-23 06:04:21]   Test statistic: 7.8870\n",
      "[2025-09-23 06:04:21]   p-value: 0.004979\n",
      "[2025-09-23 06:04:21]   ✓ Significant difference between survival curves at 5% level\n",
      "[2025-09-23 06:04:21] \n",
      "4. BASIC SURVIVAL STATISTICS\n",
      "[2025-09-23 06:04:21] ----------------------------------------\n",
      "[2025-09-23 06:04:21]   Conversion by click quartiles:\n",
      "[2025-09-23 06:04:21]     (-0.001, 1.0]: 7.92% conversion, 0.1h median (n=6729.0)\n",
      "[2025-09-23 06:04:21]     (1.0, 458.0]: 15.77% conversion, 0.6h median (n=1794.0)\n"
     ]
    }
   ],
   "source": [
    "# Survival Analysis EDA\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SURVIVAL ANALYSIS EDA\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "def survival_eda(final_survival_df: pd.DataFrame):\n",
    "    \"\"\"Perform exploratory data analysis for survival data\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from lifelines import KaplanMeierFitter\n",
    "        from lifelines.statistics import logrank_test\n",
    "        \n",
    "        log(\"\\n1. OVERALL SURVIVAL CURVE (KAPLAN-MEIER)\")\n",
    "        log(\"-\" * 40)\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=final_survival_df['duration'],\n",
    "                event_observed=final_survival_df['event_observed'])\n",
    "        \n",
    "        # Get survival probabilities at key time points\n",
    "        time_points = [1, 2, 5, 10, 24]  # hours\n",
    "        for t in time_points:\n",
    "            if t <= final_survival_df['duration'].max():\n",
    "                survival_prob = kmf.survival_function_at_times(t).iloc[0]\n",
    "                log(f\"  Survival at {t}h: {survival_prob:.4f} (conversion: {1-survival_prob:.4f})\")\n",
    "        \n",
    "        log(\"\\n2. SURVIVAL BY CLICK STATUS\")\n",
    "        log(\"-\" * 40)\n",
    "        \n",
    "        # Split by click status\n",
    "        has_clicks = final_survival_df['total_clicks'].fillna(0) > 0\n",
    "        df_clicks = final_survival_df[has_clicks]\n",
    "        df_no_clicks = final_survival_df[~has_clicks]\n",
    "        \n",
    "        log(f\"  Journeys with clicks: {len(df_clicks):,} ({len(df_clicks)/len(final_survival_df):.1%})\")\n",
    "        log(f\"  Journeys without clicks: {len(df_no_clicks):,} ({len(df_no_clicks)/len(final_survival_df):.1%})\")\n",
    "        \n",
    "        # Fit separate KM curves\n",
    "        kmf_clicks = KaplanMeierFitter()\n",
    "        kmf_clicks.fit(df_clicks['duration'], event_observed=df_clicks['event_observed'])\n",
    "        \n",
    "        kmf_no_clicks = KaplanMeierFitter()\n",
    "        kmf_no_clicks.fit(df_no_clicks['duration'], event_observed=df_no_clicks['event_observed'])\n",
    "        \n",
    "        log(\"\\n  Median survival times:\")\n",
    "        median_clicks = kmf_clicks.median_survival_time_\n",
    "        median_no_clicks = kmf_no_clicks.median_survival_time_\n",
    "        log(f\"    With clicks: {median_clicks:.2f} hours\")\n",
    "        log(f\"    Without clicks: {median_no_clicks:.2f} hours\")\n",
    "        \n",
    "        # Log-rank test\n",
    "        log(\"\\n3. LOG-RANK TEST\")\n",
    "        log(\"-\" * 40)\n",
    "        \n",
    "        if len(df_clicks) > 0 and len(df_no_clicks) > 0:\n",
    "            results = logrank_test(\n",
    "                df_clicks['duration'], df_no_clicks['duration'],\n",
    "                event_observed_A=df_clicks['event_observed'],\n",
    "                event_observed_B=df_no_clicks['event_observed']\n",
    "            )\n",
    "            \n",
    "            log(f\"  Test statistic: {results.test_statistic:.4f}\")\n",
    "            log(f\"  p-value: {results.p_value:.6f}\")\n",
    "            \n",
    "            if results.p_value < 0.05:\n",
    "                log(\"  ✓ Significant difference between survival curves at 5% level\")\n",
    "            else:\n",
    "                log(\"  ✗ No significant difference between survival curves at 5% level\")\n",
    "        \n",
    "    except ImportError:\n",
    "        log(\"WARNING: lifelines package not installed. Skipping Kaplan-Meier analysis.\")\n",
    "        log(\"Install with: pip install lifelines\")\n",
    "    \n",
    "    # Basic statistics without lifelines\n",
    "    log(\"\\n4. BASIC SURVIVAL STATISTICS\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    # Conversion rates by click intensity\n",
    "    click_bins = pd.qcut(final_survival_df['total_clicks'].fillna(0), \n",
    "                         q=[0, 0.25, 0.5, 0.75, 1.0], \n",
    "                         duplicates='drop')\n",
    "    \n",
    "    conv_by_clicks = final_survival_df.groupby(click_bins, observed=True).agg({\n",
    "        'event_observed': ['mean', 'count'],\n",
    "        'duration': 'median'\n",
    "    })\n",
    "    \n",
    "    log(\"  Conversion by click quartiles:\")\n",
    "    for idx, row in conv_by_clicks.iterrows():\n",
    "        conv_rate = row[('event_observed', 'mean')]\n",
    "        count = row[('event_observed', 'count')]\n",
    "        median_dur = row[('duration', 'median')]\n",
    "        log(f\"    {idx}: {conv_rate:.2%} conversion, {median_dur:.1f}h median (n={count})\")\n",
    "\n",
    "survival_eda(final_survival_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 06:04:25] \n",
      "============================================================\n",
      "[2025-09-23 06:04:25] COX PROPORTIONAL HAZARDS MODEL\n",
      "[2025-09-23 06:04:25] ============================================================\n",
      "[2025-09-23 06:04:25] \n",
      "Fitting Cox model...\n",
      "[2025-09-23 06:04:25] Covariates: total_clicks, total_impressions, avg_price_viewed, distinct_products_viewed, hist_purchase_count, hist_user_ctr\n",
      "[2025-09-23 06:04:25] scikit-survival unavailable or failed: No module named 'sklearn._loss._loss'\n",
      "[2025-09-23 06:04:25] Falling back to lifelines...\n",
      "[2025-09-23 06:04:25] Using lifelines backend...\n",
      "[2025-09-23 06:04:26] \n",
      "========================================\n",
      "[2025-09-23 06:04:26] COX MODEL RESULTS (lifelines)\n",
      "[2025-09-23 06:04:26] ========================================\n",
      "[2025-09-23 06:04:26] \n",
      "                                  coef        HR  se(coef)         z         p\n",
      "covariate                                                                     \n",
      "total_clicks              1.645641e-02  1.016593  0.005167  3.184931  0.001448\n",
      "total_impressions        -5.679060e-05  0.999943  0.000615 -0.092387  0.926390\n",
      "avg_price_viewed          4.441893e-07  1.000000  0.000001  0.330399  0.741098\n",
      "distinct_products_viewed -2.456229e-03  0.997547  0.001206 -2.036168  0.041734\n",
      "hist_purchase_count       3.058390e-03  1.003063  0.001005  3.043174  0.002341\n",
      "hist_user_ctr            -3.640798e-01  0.694836  1.294502 -0.281251  0.778518\n",
      "[2025-09-23 06:04:26] \n",
      "========================================\n",
      "[2025-09-23 06:04:26] MODEL FIT STATISTICS\n",
      "[2025-09-23 06:04:26] ========================================\n",
      "[2025-09-23 06:04:26]   Concordance index: 0.5605\n",
      "[2025-09-23 06:04:26]   Log-likelihood: -5160.2195\n",
      "[2025-09-23 06:04:26] ERROR: Both survival analysis backends failed: Since the model is semi-parametric (and not fully-parametric), the AIC does not exist. You probably want the `.AIC_partial_` property instead.\n"
     ]
    }
   ],
   "source": [
    "# Fit Cox Proportional Hazards Model\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"COX PROPORTIONAL HAZARDS MODEL\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "def fit_cox(final_survival_df: pd.DataFrame, covariates: list):\n",
    "    \"\"\"Fit Cox proportional hazards model with multiple backend options\"\"\"\n",
    "    \n",
    "    log(\"\\nFitting Cox model...\")\n",
    "    log(f\"Covariates: {', '.join(covariates)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = final_survival_df[covariates].copy()\n",
    "    y_time = final_survival_df['duration'].to_numpy()\n",
    "    y_event = final_survival_df['event_observed'].astype(bool).to_numpy()\n",
    "    \n",
    "    # Clean data\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    # Try scikit-survival first\n",
    "    try:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "        from sksurv.util import Surv\n",
    "        \n",
    "        log(\"Using scikit-survival backend...\")\n",
    "        \n",
    "        X_scaled = X.astype(np.float32)\n",
    "        surv_y = Surv.from_arrays(event=y_event, time=y_time)\n",
    "        \n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler(with_mean=True)),\n",
    "            ('cox', CoxPHSurvivalAnalysis(alpha=0.0))\n",
    "        ])\n",
    "        pipe.fit(X_scaled, surv_y)\n",
    "        \n",
    "        # Extract results\n",
    "        coefs = pd.Series(pipe.named_steps['cox'].coef_, index=X.columns, name='coef')\n",
    "        hazard_ratios = np.exp(coefs).rename('HR')\n",
    "        output = pd.concat([coefs, hazard_ratios], axis=1)\n",
    "        \n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"COX MODEL RESULTS (scikit-survival)\")\n",
    "        log(\"=\"*40)\n",
    "        log(\"\\n\" + output.to_string())\n",
    "        \n",
    "        # C-index (concordance)\n",
    "        c_index = pipe.score(X_scaled, surv_y)\n",
    "        log(f\"\\nConcordance index: {c_index:.4f}\")\n",
    "        \n",
    "        return {'backend': 'scikit-survival', 'model': pipe, 'summary': output, 'c_index': c_index}\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"scikit-survival unavailable or failed: {e}\")\n",
    "        log(\"Falling back to lifelines...\")\n",
    "    \n",
    "    # Fallback to lifelines\n",
    "    try:\n",
    "        from lifelines import CoxPHFitter\n",
    "        \n",
    "        log(\"Using lifelines backend...\")\n",
    "        \n",
    "        df_fit = final_survival_df[['duration', 'event_observed'] + covariates].copy()\n",
    "        df_fit = df_fit.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        cph = CoxPHFitter()\n",
    "        cph.fit(df_fit, duration_col='duration', event_col='event_observed', robust=True)\n",
    "        \n",
    "        # Extract results\n",
    "        summary = cph.summary[['coef', 'exp(coef)', 'se(coef)', 'z', 'p']]\n",
    "        summary = summary.rename(columns={'exp(coef)': 'HR'})\n",
    "        \n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"COX MODEL RESULTS (lifelines)\")\n",
    "        log(\"=\"*40)\n",
    "        log(\"\\n\" + summary.to_string())\n",
    "        \n",
    "        # Model statistics\n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"MODEL FIT STATISTICS\")\n",
    "        log(\"=\"*40)\n",
    "        log(f\"  Concordance index: {cph.concordance_index_:.4f}\")\n",
    "        log(f\"  Log-likelihood: {cph.log_likelihood_:.4f}\")\n",
    "        log(f\"  AIC: {cph.AIC_:.4f}\")\n",
    "        \n",
    "        # Check proportional hazards assumption\n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"PROPORTIONAL HAZARDS TEST\")\n",
    "        log(\"=\"*40)\n",
    "        \n",
    "        try:\n",
    "            ph_test = cph.check_assumptions(df_fit, p_value_threshold=0.05, show_plots=False)\n",
    "            log(\"  Proportional hazards assumption check completed\")\n",
    "            log(\"  (Violations would be printed above if any)\")\n",
    "        except:\n",
    "            log(\"  Could not perform proportional hazards test\")\n",
    "        \n",
    "        return {'backend': 'lifelines', 'model': cph, 'summary': summary}\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR: Both survival analysis backends failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define covariates\n",
    "covariates = ['total_clicks', 'total_impressions', 'avg_price_viewed', 'distinct_products_viewed']\n",
    "\n",
    "# Add historical features if available\n",
    "if 'hist_purchase_count' in final_survival_df.columns:\n",
    "    covariates.append('hist_purchase_count')\n",
    "if 'hist_user_ctr' in final_survival_df.columns:\n",
    "    covariates.append('hist_user_ctr')\n",
    "\n",
    "# Filter to valid covariates\n",
    "valid_covariates = [c for c in covariates if c in final_survival_df.columns]\n",
    "\n",
    "# Fit the model\n",
    "cox_results = fit_cox(final_survival_df, valid_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret Cox results\n",
    "if cox_results:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"KEY INTERPRETATIONS - SURVIVAL ANALYSIS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    summary = cox_results['summary']\n",
    "    \n",
    "    if 'total_clicks' in summary.index:\n",
    "        hr_clicks = summary.loc['total_clicks', 'HR']\n",
    "        log(f\"\\ntotal_clicks Hazard Ratio: {hr_clicks:.4f}\")\n",
    "        \n",
    "        if hr_clicks > 1:\n",
    "            pct_increase = (hr_clicks - 1) * 100\n",
    "            log(f\"  → Each additional click increases the hazard of purchase by {pct_increase:.1f}%\")\n",
    "            log(f\"  → This means clicks ACCELERATE conversion (shorter time to purchase)\")\n",
    "        elif hr_clicks < 1:\n",
    "            pct_decrease = (1 - hr_clicks) * 100\n",
    "            log(f\"  → Each additional click decreases the hazard of purchase by {pct_decrease:.1f}%\")\n",
    "            log(f\"  → This means clicks DELAY conversion (longer time to purchase)\")\n",
    "        else:\n",
    "            log(f\"  → Clicks have no effect on conversion timing\")\n",
    "    \n",
    "    if 'total_impressions' in summary.index:\n",
    "        hr_impr = summary.loc['total_impressions', 'HR']\n",
    "        log(f\"\\ntotal_impressions Hazard Ratio: {hr_impr:.4f}\")\n",
    "        \n",
    "        if hr_impr > 1:\n",
    "            log(f\"  → More impressions associated with faster conversion\")\n",
    "        else:\n",
    "            log(f\"  → More impressions associated with slower conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Discrete Choice Modeling - Product Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:55:30] \n",
      "================================================================================\n",
      "[2025-09-23 05:55:30] PART 2: DISCRETE CHOICE MODELING - PRODUCT SELECTION\n",
      "[2025-09-23 05:55:30] ================================================================================\n",
      "[2025-09-23 05:55:30] Building choice dataset...\n",
      "[2025-09-23 05:55:30]   Found 1,859 purchase instances\n",
      "[2025-09-23 05:55:30]   Applied time filter: only products impressed before purchase\n",
      "[2025-09-23 05:55:30]   Sampling up to 10 non-chosen alternatives per choice set\n",
      "[2025-09-23 05:55:30] Choice dataset created:\n",
      "[2025-09-23 05:55:30]   46 choice instances\n",
      "[2025-09-23 05:55:30]   464 total alternatives\n",
      "[2025-09-23 05:55:30]   Average choice set size: 10.1\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2: DISCRETE CHOICE MODELING - PRODUCT SELECTION\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "def build_choice_dataset(all_events_enhanced: pd.DataFrame,\n",
    "                        metrics: pd.DataFrame,\n",
    "                        sample_k: int = None,\n",
    "                        time_filter: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Build choice sets for conditional logit modeling\"\"\"\n",
    "    \n",
    "    log(\"Building choice dataset...\")\n",
    "    _ensure_datetime(all_events_enhanced, 'event_time')\n",
    "    \n",
    "    # Get purchases with instance IDs\n",
    "    purchases_for_choice = (\n",
    "        all_events_enhanced.query(\"event_type == 'purchase'\")\n",
    "        .sort_values(['journey_id', 'event_time'])\n",
    "        .assign(purchase_order=lambda d: d.groupby('journey_id').cumcount() + 1)\n",
    "        .rename(columns={'PRODUCT_ID': 'purchased_product_id',\n",
    "                        'event_time': 'purchase_time'})\n",
    "    )\n",
    "    purchases_for_choice['choice_instance_id'] = (\n",
    "        purchases_for_choice['journey_id'].astype(str) + '__' + \n",
    "        purchases_for_choice['purchase_order'].astype(str)\n",
    "    )\n",
    "    \n",
    "    log(f\"  Found {len(purchases_for_choice):,} purchase instances\")\n",
    "    \n",
    "    # Get impressions as candidates\n",
    "    impressions_for_choice = (\n",
    "        all_events_enhanced.query(\"event_type == 'impression'\")\n",
    "        .rename(columns={'event_time': 'impression_time'})\n",
    "    )\n",
    "    \n",
    "    # Create candidate sets\n",
    "    candidates = (\n",
    "        purchases_for_choice[['journey_id', 'choice_instance_id', 'purchase_time', 'purchased_product_id']]\n",
    "        .merge(impressions_for_choice[['journey_id', 'PRODUCT_ID', 'impression_time']],\n",
    "               on='journey_id', how='left')\n",
    "    )\n",
    "    \n",
    "    # Apply time filter: only products impressed before purchase\n",
    "    if time_filter:\n",
    "        candidates = candidates[candidates['impression_time'] <= candidates['purchase_time']]\n",
    "        log(\"  Applied time filter: only products impressed before purchase\")\n",
    "    \n",
    "    # Create choice sets\n",
    "    choice_sets = candidates[['choice_instance_id', 'journey_id', 'PRODUCT_ID']].drop_duplicates()\n",
    "    \n",
    "    # Add chosen flag\n",
    "    choice_df = choice_sets.merge(\n",
    "        purchases_for_choice[['choice_instance_id', 'purchased_product_id']],\n",
    "        on='choice_instance_id', how='left'\n",
    "    )\n",
    "    choice_df['chosen'] = (choice_df['PRODUCT_ID'] == choice_df['purchased_product_id']).astype(int)\n",
    "    choice_df = choice_df.drop(columns='purchased_product_id')\n",
    "    \n",
    "    # Add product attributes from metrics\n",
    "    keep_cols = ['journey_id', 'PRODUCT_ID', 'clicks_on_product', 'impressions_on_product', 'PRICE']\n",
    "    keep_cols += [c for c in ['BRAND', 'DEPARTMENT_ID'] if c in metrics.columns]\n",
    "    attributes = metrics[keep_cols].drop_duplicates(['journey_id', 'PRODUCT_ID'])\n",
    "    \n",
    "    final_choice = choice_df.merge(attributes, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "    \n",
    "    # Optional negative sampling\n",
    "    if sample_k is not None:\n",
    "        log(f\"  Sampling up to {sample_k} non-chosen alternatives per choice set\")\n",
    "        \n",
    "        def _sample(group):\n",
    "            positives = group[group['chosen'] == 1]\n",
    "            negatives = group[group['chosen'] == 0]\n",
    "            if len(negatives) > sample_k:\n",
    "                negatives = negatives.sample(sample_k, random_state=42)\n",
    "            return pd.concat([positives, negatives], ignore_index=True)\n",
    "        \n",
    "        final_choice = final_choice.groupby('choice_instance_id', group_keys=False).apply(_sample)\n",
    "    \n",
    "    # Sanity filters: each choice set must have exactly 1 chosen product\n",
    "    chosen_counts = final_choice.groupby('choice_instance_id')['chosen'].sum()\n",
    "    valid_choices = chosen_counts[chosen_counts == 1].index\n",
    "    final_choice = final_choice[final_choice['choice_instance_id'].isin(valid_choices)]\n",
    "    \n",
    "    # Filter: choice sets must have at least 2 alternatives\n",
    "    set_sizes = final_choice.groupby('choice_instance_id')['PRODUCT_ID'].nunique()\n",
    "    valid_sets = set_sizes[set_sizes >= 2].index\n",
    "    final_choice = final_choice[final_choice['choice_instance_id'].isin(valid_sets)].copy()\n",
    "    \n",
    "    # Create binary click variable\n",
    "    final_choice['was_clicked'] = (final_choice['clicks_on_product'].fillna(0) > 0).astype(int)\n",
    "    \n",
    "    log(f\"Choice dataset created:\")\n",
    "    log(f\"  {len(final_choice['choice_instance_id'].unique()):,} choice instances\")\n",
    "    log(f\"  {len(final_choice):,} total alternatives\")\n",
    "    log(f\"  Average choice set size: {final_choice.groupby('choice_instance_id').size().mean():.1f}\")\n",
    "    \n",
    "    return final_choice\n",
    "\n",
    "# Build choice dataset\n",
    "final_choice_df = build_choice_dataset(\n",
    "    all_events_enhanced, \n",
    "    metrics, \n",
    "    sample_k=10,  # Sample up to 10 non-chosen alternatives\n",
    "    time_filter=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:55:30] \n",
      "============================================================\n",
      "[2025-09-23 05:55:30] DISCRETE CHOICE EDA\n",
      "[2025-09-23 05:55:30] ============================================================\n",
      "[2025-09-23 05:55:30] \n",
      "1. ATTRIBUTE COMPARISON: CHOSEN VS NOT CHOSEN\n",
      "[2025-09-23 05:55:30] ----------------------------------------\n",
      "[2025-09-23 05:55:30] \n",
      "chosen                     0          1\n",
      "PRICE_mean         74.244019  37.695652\n",
      "PRICE_median       39.000000  29.000000\n",
      "clicks_mean         0.078947   1.326087\n",
      "clicks_sum         33.000000  61.000000\n",
      "impressions_mean    1.361244   1.369565\n",
      "impressions_sum   569.000000  63.000000\n",
      "was_clicked_rate    0.062201   0.891304\n",
      "count             418.000000  46.000000\n",
      "[2025-09-23 05:55:30] \n",
      "  Click rate lift for chosen products: 14.33x\n",
      "[2025-09-23 05:55:30] \n",
      "2. CHOICE SET SIZE DISTRIBUTION\n",
      "[2025-09-23 05:55:30] ----------------------------------------\n",
      "[2025-09-23 05:55:30]   Choice set size statistics:\n",
      "[2025-09-23 05:55:30]     Mean: 10.1\n",
      "[2025-09-23 05:55:30]     Median: 11\n",
      "[2025-09-23 05:55:30]     Min: 2\n",
      "[2025-09-23 05:55:30]     Max: 11\n",
      "[2025-09-23 05:55:30]     Std: 2.4\n",
      "[2025-09-23 05:55:30] \n",
      "  Most common choice set sizes:\n",
      "[2025-09-23 05:55:30]     2 products: 2 sets (4.3%)\n",
      "[2025-09-23 05:55:30]     4 products: 2 sets (4.3%)\n",
      "[2025-09-23 05:55:30]     6 products: 1 sets (2.2%)\n",
      "[2025-09-23 05:55:30]     8 products: 1 sets (2.2%)\n",
      "[2025-09-23 05:55:30]     10 products: 2 sets (4.3%)\n",
      "[2025-09-23 05:55:30]     11 products: 38 sets (82.6%)\n",
      "[2025-09-23 05:55:30] \n",
      "3. PRICE DISTRIBUTION OF PURCHASED PRODUCTS\n",
      "[2025-09-23 05:55:30] ----------------------------------------\n",
      "[2025-09-23 05:55:30]   Mean price: $37.70\n",
      "[2025-09-23 05:55:30]   Median price: $29.00\n",
      "[2025-09-23 05:55:30]   25th percentile: $16.00\n",
      "[2025-09-23 05:55:30]   75th percentile: $55.00\n",
      "[2025-09-23 05:55:30] \n",
      "4. ATTRIBUTE CORRELATIONS\n",
      "[2025-09-23 05:55:30] ----------------------------------------\n",
      "[2025-09-23 05:55:30] \n",
      "                           PRICE  clicks_on_product  impressions_on_product\n",
      "PRICE                   1.000000          -0.079952               -0.075371\n",
      "clicks_on_product      -0.079952           1.000000                0.081653\n",
      "impressions_on_product -0.075371           0.081653                1.000000\n"
     ]
    }
   ],
   "source": [
    "# Choice Model EDA\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"DISCRETE CHOICE EDA\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "def choice_eda(final_choice_df: pd.DataFrame):\n",
    "    \"\"\"Exploratory analysis for choice data\"\"\"\n",
    "    \n",
    "    log(\"\\n1. ATTRIBUTE COMPARISON: CHOSEN VS NOT CHOSEN\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    comparison = final_choice_df.groupby('chosen').agg(\n",
    "        PRICE_mean=('PRICE', 'mean'),\n",
    "        PRICE_median=('PRICE', 'median'),\n",
    "        clicks_mean=('clicks_on_product', 'mean'),\n",
    "        clicks_sum=('clicks_on_product', 'sum'),\n",
    "        impressions_mean=('impressions_on_product', 'mean'),\n",
    "        impressions_sum=('impressions_on_product', 'sum'),\n",
    "        was_clicked_rate=('was_clicked', 'mean'),\n",
    "        count=('was_clicked', 'count')\n",
    "    ).T\n",
    "    \n",
    "    log(\"\\n\" + comparison.to_string())\n",
    "    \n",
    "    # Calculate lift\n",
    "    if 'was_clicked_rate' in comparison.index:\n",
    "        click_rate_chosen = comparison.loc['was_clicked_rate', 1]\n",
    "        click_rate_not_chosen = comparison.loc['was_clicked_rate', 0]\n",
    "        if click_rate_not_chosen > 0:\n",
    "            lift = click_rate_chosen / click_rate_not_chosen\n",
    "            log(f\"\\n  Click rate lift for chosen products: {lift:.2f}x\")\n",
    "    \n",
    "    log(\"\\n2. CHOICE SET SIZE DISTRIBUTION\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    set_sizes = final_choice_df.groupby('choice_instance_id')['PRODUCT_ID'].nunique()\n",
    "    \n",
    "    log(\"  Choice set size statistics:\")\n",
    "    log(f\"    Mean: {set_sizes.mean():.1f}\")\n",
    "    log(f\"    Median: {set_sizes.median():.0f}\")\n",
    "    log(f\"    Min: {set_sizes.min()}\")\n",
    "    log(f\"    Max: {set_sizes.max()}\")\n",
    "    log(f\"    Std: {set_sizes.std():.1f}\")\n",
    "    \n",
    "    # Distribution of set sizes\n",
    "    size_dist = set_sizes.value_counts().sort_index().head(10)\n",
    "    log(\"\\n  Most common choice set sizes:\")\n",
    "    for size, count in size_dist.items():\n",
    "        log(f\"    {size} products: {count} sets ({count/len(set_sizes):.1%})\")\n",
    "    \n",
    "    log(\"\\n3. PRICE DISTRIBUTION OF PURCHASED PRODUCTS\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    chosen_products = final_choice_df[final_choice_df['chosen'] == 1]['PRICE'].dropna()\n",
    "    if len(chosen_products) > 0:\n",
    "        log(f\"  Mean price: ${chosen_products.mean():.2f}\")\n",
    "        log(f\"  Median price: ${chosen_products.median():.2f}\")\n",
    "        log(f\"  25th percentile: ${chosen_products.quantile(0.25):.2f}\")\n",
    "        log(f\"  75th percentile: ${chosen_products.quantile(0.75):.2f}\")\n",
    "    \n",
    "    log(\"\\n4. ATTRIBUTE CORRELATIONS\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    corr_vars = ['PRICE', 'clicks_on_product', 'impressions_on_product']\n",
    "    corr_matrix = final_choice_df[corr_vars].corr()\n",
    "    log(\"\\n\" + corr_matrix.to_string())\n",
    "    \n",
    "    if abs(corr_matrix.values[np.triu_indices_from(corr_matrix.values, 1)]).max() > 0.7:\n",
    "        log(\"\\n  WARNING: High correlation detected. May cause multicollinearity issues.\")\n",
    "\n",
    "choice_eda(final_choice_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:55:30] \n",
      "============================================================\n",
      "[2025-09-23 05:55:30] CONDITIONAL LOGIT MODEL\n",
      "[2025-09-23 05:55:30] ============================================================\n",
      "[2025-09-23 05:55:30] \n",
      "Fitting conditional logit with features: PRICE_std, was_clicked, impressions_on_product\n",
      "[2025-09-23 05:55:30]   46 choice instances\n",
      "[2025-09-23 05:55:30]   464 total observations\n",
      "[2025-09-23 05:55:30] xlogit unavailable or failed: No module named 'xlogit'\n",
      "[2025-09-23 05:55:30] pylogit unavailable or failed: No module named 'pylogit'\n",
      "[2025-09-23 05:55:30] Using custom MLE implementation...\n",
      "[2025-09-23 05:55:30] \n",
      "========================================\n",
      "[2025-09-23 05:55:30] CONDITIONAL LOGIT RESULTS (custom MLE)\n",
      "[2025-09-23 05:55:30] ========================================\n",
      "[2025-09-23 05:55:30] \n",
      "                            coef        se         z             p\n",
      "PRICE_std              -3.582652  1.526925 -2.346318  1.895993e-02\n",
      "was_clicked             6.012926  1.139593  5.276383  1.317588e-07\n",
      "impressions_on_product -0.169836  0.459406 -0.369687  7.116159e-01\n",
      "[2025-09-23 05:55:30] \n",
      "Log-likelihood: -21.3142\n",
      "[2025-09-23 05:55:30] Convergence: True\n",
      "[2025-09-23 05:55:30] Iterations: 9\n"
     ]
    }
   ],
   "source": [
    "# Fit Conditional Logit Model\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CONDITIONAL LOGIT MODEL\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "def fit_conditional_logit(final_choice_df: pd.DataFrame, X_cols=None):\n",
    "    \"\"\"Fit conditional logit model with multiple backend options\"\"\"\n",
    "    \n",
    "    if X_cols is None:\n",
    "        X_cols = ['PRICE', 'was_clicked', 'impressions_on_product']\n",
    "    \n",
    "    log(f\"\\nFitting conditional logit with features: {', '.join(X_cols)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df = final_choice_df[['choice_instance_id', 'PRODUCT_ID', 'chosen'] + X_cols].copy()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    # Create integer IDs\n",
    "    df['obs_id'], _ = pd.factorize(df['choice_instance_id'])\n",
    "    df['alt_id'], _ = pd.factorize(df['PRODUCT_ID'])\n",
    "    \n",
    "    log(f\"  {len(df['obs_id'].unique()):,} choice instances\")\n",
    "    log(f\"  {len(df):,} total observations\")\n",
    "    \n",
    "    # Try xlogit first\n",
    "    try:\n",
    "        from xlogit import MultinomialLogit\n",
    "        \n",
    "        log(\"Using xlogit backend...\")\n",
    "        \n",
    "        model = MultinomialLogit()\n",
    "        model.fit(\n",
    "            X=df[X_cols].values,\n",
    "            y=df['chosen'].values.astype(int),\n",
    "            alt=df['alt_id'].values.astype(int),\n",
    "            obs=df['obs_id'].values.astype(int),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        summary = pd.DataFrame({\n",
    "            'coef': model.coeff_,\n",
    "            'se': np.sqrt(np.diag(model.cov_))\n",
    "        }, index=X_cols)\n",
    "        summary['z'] = summary['coef'] / summary['se']\n",
    "        summary['p'] = 2 * (1 - stats.norm.cdf(abs(summary['z'])))\n",
    "        \n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"CONDITIONAL LOGIT RESULTS (xlogit)\")\n",
    "        log(\"=\"*40)\n",
    "        log(\"\\n\" + summary.to_string())\n",
    "        \n",
    "        return {'backend': 'xlogit', 'model': model, 'summary': summary}\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"xlogit unavailable or failed: {e}\")\n",
    "    \n",
    "    # Try pylogit\n",
    "    try:\n",
    "        import pylogit as pl\n",
    "        \n",
    "        log(\"Using pylogit backend...\")\n",
    "        \n",
    "        # Create specification\n",
    "        spec = {c: 'all_same' for c in X_cols}\n",
    "        \n",
    "        model = pl.create_choice_model(\n",
    "            data=df,\n",
    "            alt_id_col='alt_id',\n",
    "            obs_id_col='obs_id',\n",
    "            choice_col='chosen',\n",
    "            specification=spec,\n",
    "            model_type='MNL'\n",
    "        )\n",
    "        \n",
    "        model.fit_mle(init_vals=None, print_res=False)\n",
    "        \n",
    "        summary = pd.DataFrame({\n",
    "            'coef': model.params,\n",
    "            'se': np.sqrt(np.diag(model.variance_matrix)),\n",
    "        }, index=X_cols)\n",
    "        summary['z'] = summary['coef'] / summary['se']\n",
    "        summary['p'] = 2 * (1 - stats.norm.cdf(abs(summary['z'])))\n",
    "        \n",
    "        log(\"\\n\" + \"=\"*40)\n",
    "        log(\"CONDITIONAL LOGIT RESULTS (pylogit)\")\n",
    "        log(\"=\"*40)\n",
    "        log(\"\\n\" + summary.to_string())\n",
    "        \n",
    "        return {'backend': 'pylogit', 'model': model, 'summary': summary}\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"pylogit unavailable or failed: {e}\")\n",
    "    \n",
    "    # Fallback to custom MLE\n",
    "    log(\"Using custom MLE implementation...\")\n",
    "    \n",
    "    X = df[X_cols].to_numpy(dtype=float)\n",
    "    y = df['chosen'].to_numpy(dtype=int)\n",
    "    obs = df['obs_id'].to_numpy(dtype=int)\n",
    "    \n",
    "    # Group indices\n",
    "    counts = np.bincount(obs)\n",
    "    group_idx = np.r_[0, np.cumsum(counts)]\n",
    "    \n",
    "    def nll_grad_hess(beta):\n",
    "        beta = beta.reshape(-1, 1)\n",
    "        utilities = X @ beta\n",
    "        log_likelihood = 0.0\n",
    "        gradient = np.zeros_like(beta)\n",
    "        hessian = np.zeros((beta.size, beta.size))\n",
    "        \n",
    "        for i in range(len(group_idx) - 1):\n",
    "            start, end = group_idx[i], group_idx[i + 1]\n",
    "            u_group = utilities[start:end]\n",
    "            y_group = y[start:end].reshape(-1, 1)\n",
    "            X_group = X[start:end, :]\n",
    "            \n",
    "            # Numerical stability\n",
    "            max_u = u_group.max()\n",
    "            exp_u = np.exp(u_group - max_u)\n",
    "            denom = exp_u.sum()\n",
    "            probs = exp_u / denom\n",
    "            \n",
    "            log_likelihood += float(y_group.T @ (u_group - max_u) - np.log(denom))\n",
    "            gradient += X_group.T @ (y_group - probs)\n",
    "            \n",
    "            W = np.diagflat(probs) - probs @ probs.T\n",
    "            hessian -= X_group.T @ W @ X_group\n",
    "        \n",
    "        return -log_likelihood, -gradient.flatten(), -hessian\n",
    "    \n",
    "    def objective(b): return nll_grad_hess(b)[0]\n",
    "    def jacobian(b): return nll_grad_hess(b)[1]\n",
    "    def hess(b): return nll_grad_hess(b)[2]\n",
    "    \n",
    "    # Optimize\n",
    "    beta_init = np.zeros(len(X_cols))\n",
    "    result = minimize(\n",
    "        objective, beta_init, \n",
    "        jac=jacobian, hess=hess, \n",
    "        method='trust-ncg',\n",
    "        options={'gtol': 1e-6, 'maxiter': 200}\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    beta_hat = result.x\n",
    "    cov_matrix = np.linalg.inv(hess(beta_hat))\n",
    "    se = np.sqrt(np.diag(cov_matrix))\n",
    "    z_scores = beta_hat / se\n",
    "    p_values = 2 * (1 - stats.norm.cdf(abs(z_scores)))\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'coef': beta_hat,\n",
    "        'se': se,\n",
    "        'z': z_scores,\n",
    "        'p': p_values\n",
    "    }, index=X_cols)\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"CONDITIONAL LOGIT RESULTS (custom MLE)\")\n",
    "    log(\"=\"*40)\n",
    "    log(\"\\n\" + summary.to_string())\n",
    "    \n",
    "    log(f\"\\nLog-likelihood: {-result.fun:.4f}\")\n",
    "    log(f\"Convergence: {result.success}\")\n",
    "    log(f\"Iterations: {result.nit}\")\n",
    "    \n",
    "    return {'backend': 'custom', 'model': {'opt': result, 'cov': cov_matrix}, 'summary': summary}\n",
    "\n",
    "# Define features for choice model\n",
    "choice_features = ['PRICE', 'was_clicked', 'impressions_on_product']\n",
    "\n",
    "# Standardize price for better convergence\n",
    "final_choice_df['PRICE_std'] = (\n",
    "    (final_choice_df['PRICE'] - final_choice_df['PRICE'].mean()) / \n",
    "    final_choice_df['PRICE'].std()\n",
    ")\n",
    "choice_features_std = ['PRICE_std', 'was_clicked', 'impressions_on_product']\n",
    "\n",
    "# Fit the model\n",
    "clogit_results = fit_conditional_logit(final_choice_df, X_cols=choice_features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:55:30] \n",
      "============================================================\n",
      "[2025-09-23 05:55:30] KEY INTERPRETATIONS - DISCRETE CHOICE MODEL\n",
      "[2025-09-23 05:55:30] ============================================================\n",
      "[2025-09-23 05:55:30] \n",
      "was_clicked coefficient: 6.0129 (p=0.0000)\n",
      "[2025-09-23 05:55:30]   → Clicking a product multiplies its odds of being chosen by 408.68\n",
      "[2025-09-23 05:55:30]   → ✓ Effect is statistically significant at 5% level\n",
      "[2025-09-23 05:55:30]   → This is STRONG EVIDENCE of causal incrementality\n",
      "[2025-09-23 05:55:30]   → Clicks directly influence product choice within consideration sets\n",
      "[2025-09-23 05:55:30] \n",
      "PRICE_std coefficient: -3.5827 (p=0.0190)\n",
      "[2025-09-23 05:55:30]   → Higher prices decrease probability of choice (expected)\n",
      "[2025-09-23 05:55:30] \n",
      "impressions_on_product coefficient: -0.1698 (p=0.7116)\n",
      "[2025-09-23 05:55:30]   → Each additional impression changes choice probability\n",
      "[2025-09-23 05:55:30]   → More impressions decrease choice probability (fatigue effect)\n"
     ]
    }
   ],
   "source": [
    "# Interpret Choice Model Results\n",
    "if clogit_results:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"KEY INTERPRETATIONS - DISCRETE CHOICE MODEL\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    summary = clogit_results['summary']\n",
    "    \n",
    "    if 'was_clicked' in summary.index:\n",
    "        coef_click = summary.loc['was_clicked', 'coef']\n",
    "        p_click = summary.loc['was_clicked', 'p']\n",
    "        \n",
    "        log(f\"\\nwas_clicked coefficient: {coef_click:.4f} (p={p_click:.4f})\")\n",
    "        \n",
    "        # Calculate odds ratio / probability change\n",
    "        exp_coef = np.exp(coef_click)\n",
    "        log(f\"  → Clicking a product multiplies its odds of being chosen by {exp_coef:.2f}\")\n",
    "        \n",
    "        if p_click < 0.05:\n",
    "            log(f\"  → ✓ Effect is statistically significant at 5% level\")\n",
    "            log(f\"  → This is STRONG EVIDENCE of causal incrementality\")\n",
    "            log(f\"  → Clicks directly influence product choice within consideration sets\")\n",
    "        else:\n",
    "            log(f\"  → ✗ Effect is not statistically significant at 5% level\")\n",
    "    \n",
    "    if 'PRICE_std' in summary.index:\n",
    "        coef_price = summary.loc['PRICE_std', 'coef']\n",
    "        p_price = summary.loc['PRICE_std', 'p']\n",
    "        \n",
    "        log(f\"\\nPRICE_std coefficient: {coef_price:.4f} (p={p_price:.4f})\")\n",
    "        \n",
    "        if coef_price < 0:\n",
    "            log(f\"  → Higher prices decrease probability of choice (expected)\")\n",
    "        else:\n",
    "            log(f\"  → Higher prices increase probability of choice (luxury effect?)\")\n",
    "    \n",
    "    if 'impressions_on_product' in summary.index:\n",
    "        coef_impr = summary.loc['impressions_on_product', 'coef']\n",
    "        p_impr = summary.loc['impressions_on_product', 'p']\n",
    "        \n",
    "        log(f\"\\nimpressions_on_product coefficient: {coef_impr:.4f} (p={p_impr:.4f})\")\n",
    "        log(f\"  → Each additional impression changes choice probability\")\n",
    "        \n",
    "        if coef_impr > 0:\n",
    "            log(f\"  → More impressions increase choice probability (exposure effect)\")\n",
    "        else:\n",
    "            log(f\"  → More impressions decrease choice probability (fatigue effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:55:30] \n",
      "================================================================================\n",
      "[2025-09-23 05:55:30] ANALYSIS COMPLETE\n",
      "[2025-09-23 05:55:30] ================================================================================\n",
      "[2025-09-23 05:55:30] \n",
      "FINAL SUMMARY\n",
      "[2025-09-23 05:55:30] ------------------------------------------------------------\n",
      "[2025-09-23 05:55:30] Survival Analysis:\n",
      "[2025-09-23 05:55:30]   Journeys analyzed: 8,523\n",
      "[2025-09-23 05:55:30]   Conversion rate: 9.57%\n",
      "[2025-09-23 05:55:30]   Median survival time: 0.12 hours\n",
      "[2025-09-23 05:55:30] \n",
      "Discrete Choice Model:\n",
      "[2025-09-23 05:55:30]   Choice instances: 46\n",
      "[2025-09-23 05:55:30]   Total alternatives: 464\n",
      "[2025-09-23 05:55:30]   Click rate (chosen): 89.13%\n",
      "[2025-09-23 05:55:30]   Click rate (not chosen): 6.22%\n",
      "[2025-09-23 05:55:30] \n",
      "✓ Results saved to: data/alternative_models_results_20250923_055513.txt\n",
      "[2025-09-23 05:55:30] Total log entries: 139\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK EXECUTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save all results to text file\n",
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"ANALYSIS COMPLETE\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "log(\"\\nFINAL SUMMARY\")\n",
    "log(\"-\" * 60)\n",
    "log(f\"Survival Analysis:\")\n",
    "log(f\"  Journeys analyzed: {len(final_survival_df):,}\")\n",
    "log(f\"  Conversion rate: {final_survival_df['event_observed'].mean():.2%}\")\n",
    "log(f\"  Median survival time: {final_survival_df['duration'].median():.2f} hours\")\n",
    "\n",
    "log(f\"\\nDiscrete Choice Model:\")\n",
    "log(f\"  Choice instances: {len(final_choice_df['choice_instance_id'].unique()):,}\")\n",
    "log(f\"  Total alternatives: {len(final_choice_df):,}\")\n",
    "log(f\"  Click rate (chosen): {final_choice_df[final_choice_df['chosen']==1]['was_clicked'].mean():.2%}\")\n",
    "log(f\"  Click rate (not chosen): {final_choice_df[final_choice_df['chosen']==0]['was_clicked'].mean():.2%}\")\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"./data\") / f\"alternative_models_results_{timestamp}.txt\"\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('\\n'.join(output_log))\n",
    "\n",
    "log(f\"\\n✓ Results saved to: {output_path}\")\n",
    "log(f\"Total log entries: {len(output_log)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

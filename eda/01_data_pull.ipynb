{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_data_pull.ipynb\n",
    "## Snowflake Data Extraction with Checkpointing\n",
    "\n",
    "This notebook handles all data extraction from Snowflake and saves checkpoints for analysis.\n",
    "\n",
    "### Workflow:\n",
    "1. Connect to Snowflake\n",
    "2. Sample users deterministically using hash-based bucketing\n",
    "3. Extract raw event data (auctions, impressions, clicks, purchases)\n",
    "4. Extract catalog data with enhanced parsing\n",
    "5. Save all data as timestamped checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import textwrap\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress the specific pandas UserWarning for non-SQLAlchemy connections\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    message='pandas only supports SQLAlchemy connectable.*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Analysis period: 2025-08-24 to 2025-09-07\n",
      "  Historical period: 2025-05-26 to 2025-08-24\n",
      "  Sampling fraction: 0.10%\n",
      "  Journey window: 168 hours\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION & HYPERPARAMETERS ---\n",
    "\n",
    "# Load environment variables from the .env file for secure credential management\n",
    "load_dotenv()\n",
    "\n",
    "# ANALYSIS_END_DATE: The last date to be included in our analysis window\n",
    "ANALYSIS_END_DATE = date(2025, 9, 7)\n",
    "\n",
    "# DAYS_WINDOW: Number of days of data to pull, counting back from ANALYSIS_END_DATE\n",
    "DAYS_WINDOW = 14  # Two weeks of data\n",
    "\n",
    "# SAMPLING_FRACTION: Percentage of users to include in the analysis\n",
    "# 0.001 = 0.1% of users for development, can increase for production\n",
    "SAMPLING_FRACTION = 0.001\n",
    "\n",
    "# JOURNEY_WINDOW_HOURS: Duration that defines a single user journey/session\n",
    "JOURNEY_WINDOW_HOURS = 168  # 7 days\n",
    "\n",
    "# SESSION_GAP_HOURS: Hours of inactivity that define a new session within a journey\n",
    "SESSION_GAP_HOURS = 2\n",
    "\n",
    "# HISTORICAL_DAYS: Days of historical data to pull for feature engineering\n",
    "HISTORICAL_DAYS = 90  # 3 months of history\n",
    "\n",
    "# Calculate date ranges\n",
    "ANALYSIS_START_DATE = ANALYSIS_END_DATE - timedelta(days=DAYS_WINDOW)\n",
    "HISTORICAL_START_DATE = ANALYSIS_START_DATE - timedelta(days=HISTORICAL_DAYS)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Analysis period: {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")\n",
    "print(f\"  Historical period: {HISTORICAL_START_DATE} to {ANALYSIS_START_DATE}\")\n",
    "print(f\"  Sampling fraction: {SAMPLING_FRACTION:.2%}\")\n",
    "print(f\"  Journey window: {JOURNEY_WINDOW_HOURS} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Snowflake connection established.\n"
     ]
    }
   ],
   "source": [
    "# --- SNOWFLAKE CONNECTION ---\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"[SUCCESS] Snowflake connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] Could not connect to Snowflake: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sampling_cte(start_date: str, end_date: str, sampling_fraction: float) -> str:\n",
    "    \"\"\"\n",
    "    Build CTE for deterministic user sampling using hash-based bucketing.\n",
    "    This ensures reproducible sampling across runs.\n",
    "    \"\"\"\n",
    "    total_buckets = 10000\n",
    "    selection_threshold = int(total_buckets * sampling_fraction)\n",
    "    \n",
    "    return textwrap.dedent(f\"\"\"\n",
    "        WITH SAMPLED_USER_IDS AS (\n",
    "            WITH REPEAT_PURCHASERS AS (\n",
    "                SELECT USER_ID\n",
    "                FROM PURCHASES\n",
    "                WHERE PURCHASED_AT BETWEEN '{start_date}'\n",
    "                  AND '{end_date}'\n",
    "                GROUP BY USER_ID\n",
    "                HAVING COUNT(DISTINCT PURCHASE_ID) >= 2\n",
    "            ),\n",
    "            BUCKETED_USERS AS (\n",
    "                SELECT\n",
    "                    USER_ID,\n",
    "                    MOD(ABS(HASH(USER_ID)), {total_buckets}) AS bucket\n",
    "                FROM REPEAT_PURCHASERS\n",
    "            )\n",
    "            SELECT USER_ID\n",
    "            FROM BUCKETED_USERS\n",
    "            WHERE bucket < {selection_threshold}\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_users(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_USERS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_USERS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            au.OPAQUE_USER_ID AS USER_ID,\n",
    "            au.CREATED_AT\n",
    "        FROM AUCTIONS_USERS au\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"AUCTIONS_USERS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} auction records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_results(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_RESULTS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_RESULTS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "            LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n",
    "            LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            ar.RANKING,\n",
    "            ar.IS_WINNER,\n",
    "            ar.CREATED_AT\n",
    "        FROM AUCTIONS_RESULTS ar\n",
    "        JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"AUCTIONS_RESULTS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} bid records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impressions(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract IMPRESSIONS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting IMPRESSIONS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            i.INTERACTION_ID,\n",
    "            LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            i.USER_ID,\n",
    "            LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            i.OCCURRED_AT\n",
    "        FROM IMPRESSIONS i\n",
    "        JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "        WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"IMPRESSIONS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} impression records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clicks(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract CLICKS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting CLICKS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            c.INTERACTION_ID,\n",
    "            LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.USER_ID,\n",
    "            LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            c.OCCURRED_AT\n",
    "        FROM CLICKS c\n",
    "        JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "        WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"CLICKS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} click records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purchases(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract PURCHASES table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting PURCHASES...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            p.PURCHASE_ID,\n",
    "            p.PURCHASED_AT,\n",
    "            LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            p.QUANTITY,\n",
    "            p.UNIT_PRICE,\n",
    "            p.USER_ID,\n",
    "            p.PURCHASE_LINE\n",
    "        FROM PURCHASES p\n",
    "        JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "        WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"PURCHASES\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} purchase records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_catalog(conn, product_ids: set) -> pd.DataFrame:\n",
    "    \"\"\"Extract CATALOG data for relevant products with enhanced category parsing.\"\"\"\n",
    "    print(\"\\nExtracting CATALOG...\")\n",
    "    print(f\"  Found {len(product_ids):,} unique products to fetch\")\n",
    "    \n",
    "    # Handle the edge case where there are no product IDs to fetch\n",
    "    if not product_ids:\n",
    "        print(\"  No product IDs found, creating an empty catalog DataFrame.\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'PRODUCT_ID', 'NAME', 'PRICE', 'ACTIVE', 'IS_DELETED',\n",
    "            'BRAND', 'DEPARTMENT_ID', 'CATEGORY_ID', 'PRIMARY_COLOR', 'STYLE_TAGS'\n",
    "        ])\n",
    "    \n",
    "    # Convert the set to a list for the parameters\n",
    "    params_list = list(product_ids)\n",
    "    \n",
    "    # Create the correct number of placeholders for parameterized query\n",
    "    placeholders = ', '.join(['%s'] * len(params_list))\n",
    "    \n",
    "    # Build the query using placeholders, NOT the actual values\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TRIM(PRODUCT_ID)) as PRODUCT_ID,\n",
    "            NAME,\n",
    "            PRICE,\n",
    "            ACTIVE,\n",
    "            IS_DELETED,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(CATEGORIES, x -> x LIKE 'brand#%'), ''), '#', 2) AS BRAND,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(CATEGORIES, x -> x LIKE 'department#%'), ''), '#', 2) AS DEPARTMENT_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(CATEGORIES, x -> x LIKE 'category#%'), ''), '#', 2) AS CATEGORY_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(CATEGORIES, x -> x LIKE 'color#%'), ''), '#', 2) AS PRIMARY_COLOR,\n",
    "            REPLACE(\n",
    "                ARRAY_TO_STRING(FILTER(CATEGORIES, x -> x LIKE 'style_tag#%'), ', '),\n",
    "                'style_tag#', ''\n",
    "            ) AS STYLE_TAGS\n",
    "        FROM CATALOG\n",
    "        WHERE LOWER(TRIM(PRODUCT_ID)) IN ({placeholders})\n",
    "    \"\"\"\n",
    "    \n",
    "    with tqdm(desc=\"CATALOG\") as pbar:\n",
    "        # Pass the query and the list of parameters separately to pandas\n",
    "        # Pandas will pass them to the database driver, which handles binding them safely\n",
    "        df = pd.read_sql(query, conn, params=params_list)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} catalog records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_catalog_and_get_product_ids(conn, hist_start_date: str, end_date: str, sampling_fraction: float) -> tuple[pd.DataFrame, set]:\n",
    "    \"\"\"\n",
    "    Combines product ID collection and catalog extraction into a single, efficient query.\n",
    "    \n",
    "    This function uses a \"mono-CTE\" approach to:\n",
    "    1. Identify all sampled users.\n",
    "    2. Collect all unique product IDs associated with them from all event tables.\n",
    "    3. Fetch the catalog entries for only those products.\n",
    "    \n",
    "    This is far more efficient than collecting IDs in Python and sending them back to the DB.\n",
    "    \"\"\"\n",
    "    print(\"\\nExtracting CATALOG and collecting all PRODUCT_IDs using a mono-CTE...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(hist_start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    # We add a comma after the first CTE (sampling_cte) and remove the redundant 'WITH'\n",
    "    # from the start of the second CTE definition.\n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\",\n",
    "        ALL_PRODUCT_IDS AS (\n",
    "            -- Collect all product IDs from all relevant tables for our sampled users\n",
    "            SELECT DISTINCT LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM AUCTIONS_RESULTS ar\n",
    "            JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "            JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "            WHERE ar.CREATED_AT BETWEEN '{hist_start_date}' AND '{end_date}'\n",
    "              AND ar.PRODUCT_ID IS NOT NULL\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            SELECT DISTINCT LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM IMPRESSIONS i\n",
    "            JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "            WHERE i.OCCURRED_AT BETWEEN '{hist_start_date}' AND '{end_date}'\n",
    "              AND i.PRODUCT_ID IS NOT NULL\n",
    "\n",
    "            UNION\n",
    "\n",
    "            SELECT DISTINCT LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM CLICKS c\n",
    "            JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "            WHERE c.OCCURRED_AT BETWEEN '{hist_start_date}' AND '{end_date}'\n",
    "              AND c.PRODUCT_ID IS NOT NULL\n",
    "\n",
    "            UNION\n",
    "\n",
    "            SELECT DISTINCT LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM PURCHASES p\n",
    "            JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "            WHERE p.PURCHASED_AT BETWEEN '{hist_start_date}' AND '{end_date}'\n",
    "              AND p.PRODUCT_ID IS NOT NULL\n",
    "        )\n",
    "        -- Now, fetch the catalog data for exactly those products\n",
    "        SELECT\n",
    "            c.PRODUCT_ID,\n",
    "            c.NAME,\n",
    "            c.PRICE,\n",
    "            c.ACTIVE,\n",
    "            c.IS_DELETED,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'brand#%%'), ''), '#', 2) AS BRAND,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'department#%%'), ''), '#', 2) AS DEPARTMENT_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category#%%'), ''), '#', 2) AS CATEGORY_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'color#%%'), ''), '#', 2) AS PRIMARY_COLOR,\n",
    "            REPLACE(\n",
    "                ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'style_tag#%%'), ', '),\n",
    "                'style_tag#', ''\n",
    "            ) AS STYLE_TAGS\n",
    "        FROM CATALOG c\n",
    "        JOIN ALL_PRODUCT_IDS ap ON c.PRODUCT_ID = ap.PRODUCT_ID\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"CATALOG (Mono-CTE)\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "\n",
    "    # Get the set of product IDs from the resulting dataframe\n",
    "    product_ids = set(df['PRODUCT_ID'].unique())\n",
    "    print(f\"  Extracted {len(df):,} catalog records for {len(product_ids):,} unique products.\")\n",
    "    \n",
    "    return df, product_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Data Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING REVISED DATA EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "--- Extracting main analysis period data ---\n",
      "\n",
      "--- Extracting historical data for feature engineering ---\n",
      "\n",
      "Extracting CATALOG and collecting all PRODUCT_IDs using a mono-CTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CATALOG (Mono-CTE): 4961480it [17:43, 4664.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 4,961,480 catalog records for 4,961,480 unique products.\n",
      "\n",
      "[SUCCESS] Snowflake connection closed\n",
      "\n",
      "--- Saving data checkpoint ---\n",
      "  Saved auctions_users: 88,690 rows to auctions_users_20250923_043038.parquet\n",
      "  Saved auctions_results: 3,410,770 rows to auctions_results_20250923_043038.parquet\n",
      "  Saved impressions: 347,741 rows to impressions_20250923_043038.parquet\n",
      "  Saved clicks: 11,215 rows to clicks_20250923_043038.parquet\n",
      "  Saved purchases: 1,859 rows to purchases_20250923_043038.parquet\n",
      "  Saved catalog: 4,961,480 rows to catalog_20250923_043038.parquet\n",
      "  Saved hist_purchases: 10,332 rows to hist_purchases_20250923_043038.parquet\n",
      "  Saved hist_impressions: 1,685,675 rows to hist_impressions_20250923_043038.parquet\n",
      "  Saved hist_clicks: 59,691 rows to hist_clicks_20250923_043038.parquet\n",
      "\n",
      "  Saved metadata to metadata_20250923_043038.json\n",
      "\n",
      "================================================================================\n",
      "DATA EXTRACTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% --- MAIN DATA EXTRACTION PIPELINE (REVISED) ---\n",
    "\n",
    "if conn:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING REVISED DATA EXTRACTION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create timestamp for this extraction run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Convert dates to strings for SQL\n",
    "    start_date_str = ANALYSIS_START_DATE.strftime('%Y-%m-%d')\n",
    "    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n",
    "    hist_start_str = HISTORICAL_START_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # --- Extract main event tables (as before) ---\n",
    "    print(\"\\n--- Extracting main analysis period data ---\")\n",
    "    #auctions_users = extract_auctions_users(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    #auctions_results = extract_auctions_results(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    #impressions = extract_impressions(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    #clicks = extract_clicks(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    #purchases = extract_purchases(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    print(\"\\n--- Extracting historical data for feature engineering ---\")\n",
    "    #hist_purchases = extract_purchases(conn, hist_start_str, start_date_str, SAMPLING_FRACTION)\n",
    "    #hist_impressions = extract_impressions(conn, hist_start_str, start_date_str, SAMPLING_FRACTION)\n",
    "    #hist_clicks = extract_clicks(conn, hist_start_str, start_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # --- NEW: EFFICIENT CATALOG & PRODUCT ID EXTRACTION ---\n",
    "    # This single call replaces the slow Python-side ID collection and the failing catalog pull.\n",
    "    catalog, product_ids = extract_catalog_and_get_product_ids(conn, hist_start_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # Close connection\n",
    "    #conn.close()\n",
    "    print(\"\\n[SUCCESS] Snowflake connection closed\")\n",
    "    \n",
    "    # --- Save all data as checkpoint (no changes needed below this line) ---\n",
    "    print(\"\\n--- Saving data checkpoint ---\")\n",
    "    output_dir = Path(\"./data/raw\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    datasets = [\n",
    "        (\"auctions_users\", auctions_users),\n",
    "        (\"auctions_results\", auctions_results),\n",
    "        (\"impressions\", impressions),\n",
    "        (\"clicks\", clicks),\n",
    "        (\"purchases\", purchases),\n",
    "        (\"catalog\", catalog),\n",
    "        (\"hist_purchases\", hist_purchases),\n",
    "        (\"hist_impressions\", hist_impressions),\n",
    "        (\"hist_clicks\", hist_clicks)\n",
    "    ]\n",
    "    \n",
    "    for name, df in datasets:\n",
    "        path = output_dir / f\"{name}_{timestamp}.parquet\"\n",
    "        df.to_parquet(path, index=False)\n",
    "        print(f\"  Saved {name}: {len(df):,} rows to {path.name}\")\n",
    "    \n",
    "    # Create a metadata file\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'analysis_start_date': start_date_str,\n",
    "        'analysis_end_date': end_date_str,\n",
    "        'historical_start_date': hist_start_str,\n",
    "        'sampling_fraction': SAMPLING_FRACTION,\n",
    "        'journey_window_hours': JOURNEY_WINDOW_HOURS,\n",
    "        'total_products': len(product_ids),\n",
    "        'row_counts': {name: len(df) for name, df in datasets}\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    metadata_path = output_dir / f\"metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n  Saved metadata to {metadata_path.name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] No Snowflake connection available. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Main Analysis Period:\n",
      "  Unique users: 1,185\n",
      "  Total auctions: 88,690\n",
      "  Total bids: 3,410,770\n",
      "  Total impressions: 347,741\n",
      "  Total clicks: 11,215\n",
      "  Total purchases: 1,859\n",
      "  Catalog products: 4,961,480\n",
      "\n",
      "Historical Period (for features):\n",
      "  Historical purchases: 10,332\n",
      "  Historical impressions: 1,685,675\n",
      "  Historical clicks: 59,691\n",
      "\n",
      "Data Quality Checks:\n",
      "  Auction ID overlap (users ∩ results): 88,019 / 88,690\n",
      "  Users with purchases: 619 / 1,185\n",
      "  Product catalog coverage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Data extraction summary\n",
    "if 'auctions_users' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nMain Analysis Period:\")\n",
    "    print(f\"  Unique users: {auctions_users['USER_ID'].nunique():,}\")\n",
    "    print(f\"  Total auctions: {len(auctions_users):,}\")\n",
    "    print(f\"  Total bids: {len(auctions_results):,}\")\n",
    "    print(f\"  Total impressions: {len(impressions):,}\")\n",
    "    print(f\"  Total clicks: {len(clicks):,}\")\n",
    "    print(f\"  Total purchases: {len(purchases):,}\")\n",
    "    print(f\"  Catalog products: {len(catalog):,}\")\n",
    "    \n",
    "    print(\"\\nHistorical Period (for features):\")\n",
    "    print(f\"  Historical purchases: {len(hist_purchases):,}\")\n",
    "    print(f\"  Historical impressions: {len(hist_impressions):,}\")\n",
    "    print(f\"  Historical clicks: {len(hist_clicks):,}\")\n",
    "    \n",
    "    # Basic data quality checks\n",
    "    print(\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check auction ID consistency\n",
    "    auction_ids_users = set(auctions_users['AUCTION_ID'].unique())\n",
    "    auction_ids_results = set(auctions_results['AUCTION_ID'].unique())\n",
    "    overlap = len(auction_ids_users & auction_ids_results)\n",
    "    print(f\"  Auction ID overlap (users ∩ results): {overlap:,} / {len(auction_ids_users):,}\")\n",
    "    \n",
    "    # Check user ID consistency\n",
    "    users_in_auctions = set(auctions_users['USER_ID'].unique())\n",
    "    users_in_purchases = set(purchases['USER_ID'].unique())\n",
    "    print(f\"  Users with purchases: {len(users_in_purchases):,} / {len(users_in_auctions):,}\")\n",
    "    \n",
    "    # Check product coverage in catalog\n",
    "    products_in_events = product_ids\n",
    "    products_in_catalog = set(catalog['PRODUCT_ID'].unique())\n",
    "    coverage = len(products_in_catalog) / len(products_in_events) if products_in_events else 0\n",
    "    print(f\"  Product catalog coverage: {coverage:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_ID</th>\n",
       "      <th>PURCHASED_AT</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>UNIT_PRICE</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>PURCHASE_LINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68bcbedad4b9431a669592df</td>\n",
       "      <td>2025-09-06 23:08:25</td>\n",
       "      <td>68bcb71021b8010caf01c85a</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>ext1:a1a0b122-a736-4356-b2bb-50173b4c173f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68bafb0f7d56cdcb0fb8f926</td>\n",
       "      <td>2025-09-05 15:00:35</td>\n",
       "      <td>67e4d2e2d90d21f8fe59231a</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>ext1:3511adb0-f429-4ec2-a4a9-a1a405ceccc9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68ba60ac21a0acdc46219cdd</td>\n",
       "      <td>2025-09-05 04:01:52</td>\n",
       "      <td>66f187f1d90d21033b1222ea</td>\n",
       "      <td>1</td>\n",
       "      <td>2500</td>\n",
       "      <td>ext1:ed4f9c63-747f-4e4f-b8ab-f8541ef7f07c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68b2812f3bc447eb226e5456</td>\n",
       "      <td>2025-08-30 04:42:27</td>\n",
       "      <td>68649c9305deb3739a26ad67</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>ext1:5bbf83a2-724a-49e9-99a6-99b663e23ff1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68b5e6d7a84d7eeb906a8da8</td>\n",
       "      <td>2025-09-01 18:33:02</td>\n",
       "      <td>687ad2ef88849a368daf78ac</td>\n",
       "      <td>1</td>\n",
       "      <td>2200</td>\n",
       "      <td>ext1:cfae9c0f-b056-4b3a-8590-e565fde82ff7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PURCHASE_ID        PURCHASED_AT                PRODUCT_ID  \\\n",
       "0  68bcbedad4b9431a669592df 2025-09-06 23:08:25  68bcb71021b8010caf01c85a   \n",
       "1  68bafb0f7d56cdcb0fb8f926 2025-09-05 15:00:35  67e4d2e2d90d21f8fe59231a   \n",
       "2  68ba60ac21a0acdc46219cdd 2025-09-05 04:01:52  66f187f1d90d21033b1222ea   \n",
       "3  68b2812f3bc447eb226e5456 2025-08-30 04:42:27  68649c9305deb3739a26ad67   \n",
       "4  68b5e6d7a84d7eeb906a8da8 2025-09-01 18:33:02  687ad2ef88849a368daf78ac   \n",
       "\n",
       "   QUANTITY  UNIT_PRICE                                    USER_ID  \\\n",
       "0         1        2000  ext1:a1a0b122-a736-4356-b2bb-50173b4c173f   \n",
       "1         1        5000  ext1:3511adb0-f429-4ec2-a4a9-a1a405ceccc9   \n",
       "2         1        2500  ext1:ed4f9c63-747f-4e4f-b8ab-f8541ef7f07c   \n",
       "3         1        2000  ext1:5bbf83a2-724a-49e9-99a6-99b663e23ff1   \n",
       "4         1        2200  ext1:cfae9c0f-b056-4b3a-8590-e565fde82ff7   \n",
       "\n",
       "   PURCHASE_LINE  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              4  \n",
       "4              1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

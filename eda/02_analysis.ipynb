{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_analysis.ipynb\n",
    "## Journey Processing and Causal Analysis\n",
    "\n",
    "This notebook loads checkpointed data and performs comprehensive causal analysis.\n",
    "\n",
    "### Workflow:\n",
    "1. **Load Checkpoint** - Load data from 01_data_pull.ipynb\n",
    "2. **Journey Processing** - Create user journeys and sessions\n",
    "3. **Feature Engineering** - Create advanced features\n",
    "4. **EDA** - Comprehensive exploratory analysis\n",
    "5. **Causal Analysis** - Regression models with robust standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Statistical modeling imports\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize logging\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_log = []\n",
    "\n",
    "def log(message: str):\n",
    "    \"\"\"Add message to output log\"\"\"\n",
    "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_entry = f\"[{ts}] {message}\"\n",
    "    output_log.append(log_entry)\n",
    "    print(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Checkpoint Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:02:51] Loading data from extraction: 20250923_043038\n",
      "[2025-09-23 05:02:51]   Loaded auctions_users: 88,690 rows\n",
      "[2025-09-23 05:02:52]   Loaded auctions_results: 3,410,770 rows\n",
      "[2025-09-23 05:02:52]   Loaded impressions: 347,741 rows\n",
      "[2025-09-23 05:02:52]   Loaded clicks: 11,215 rows\n",
      "[2025-09-23 05:02:52]   Loaded purchases: 1,859 rows\n",
      "[2025-09-23 05:02:55]   Loaded catalog: 4,961,480 rows\n",
      "[2025-09-23 05:02:55]   Loaded hist_purchases: 10,332 rows\n",
      "[2025-09-23 05:02:55]   Loaded hist_impressions: 1,685,675 rows\n",
      "[2025-09-23 05:02:56]   Loaded hist_clicks: 59,691 rows\n",
      "[2025-09-23 05:02:56] \n",
      "✓ Data loaded successfully\n",
      "[2025-09-23 05:02:56]   Analysis period: 2025-08-24 to 2025-09-07\n",
      "[2025-09-23 05:02:56]   Sampling fraction: 0.10%\n",
      "[2025-09-23 05:02:56]   Journey window: 168 hours\n"
     ]
    }
   ],
   "source": [
    "# Check for existing data files\n",
    "raw_data_dir = Path(\"./data/raw\")\n",
    "processed_data_path = Path(\"./data/user_journey_causal_dataset.parquet\")\n",
    "\n",
    "def load_checkpoint_data() -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
    "    \"\"\"Load the most recent checkpoint data from raw directory.\"\"\"\n",
    "    \n",
    "    if not raw_data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {raw_data_dir}\")\n",
    "    \n",
    "    # Find most recent metadata file\n",
    "    metadata_files = list(raw_data_dir.glob(\"metadata_*.json\"))\n",
    "    if not metadata_files:\n",
    "        raise FileNotFoundError(\"No metadata files found. Please run 01_data_pull.ipynb first.\")\n",
    "    \n",
    "    latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(latest_metadata, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    extraction_timestamp = metadata['timestamp']\n",
    "    log(f\"Loading data from extraction: {extraction_timestamp}\")\n",
    "    \n",
    "    # Load all data files\n",
    "    data = {}\n",
    "    required_files = [\n",
    "        'auctions_users', 'auctions_results', 'impressions', \n",
    "        'clicks', 'purchases', 'catalog'\n",
    "    ]\n",
    "    optional_files = ['hist_purchases', 'hist_impressions', 'hist_clicks']\n",
    "    \n",
    "    for file_type in required_files + optional_files:\n",
    "        pattern = f\"{file_type}_{extraction_timestamp}.parquet\"\n",
    "        file_path = raw_data_dir / pattern\n",
    "        \n",
    "        if file_path.exists():\n",
    "            data[file_type] = pd.read_parquet(file_path)\n",
    "            log(f\"  Loaded {file_type}: {len(data[file_type]):,} rows\")\n",
    "        elif file_type in required_files:\n",
    "            raise FileNotFoundError(f\"Required file not found: {file_path}\")\n",
    "        else:\n",
    "            log(f\"  Optional file not found: {file_type}\")\n",
    "    \n",
    "    return data, metadata\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data, metadata = load_checkpoint_data()\n",
    "    \n",
    "    # Extract individual dataframes\n",
    "    auctions_users = data['auctions_users']\n",
    "    auctions_results = data['auctions_results']\n",
    "    impressions = data['impressions']\n",
    "    clicks = data['clicks']\n",
    "    purchases = data['purchases']\n",
    "    catalog = data['catalog']\n",
    "    \n",
    "    # Historical data (optional)\n",
    "    hist_purchases = data.get('hist_purchases', pd.DataFrame())\n",
    "    hist_impressions = data.get('hist_impressions', pd.DataFrame())\n",
    "    hist_clicks = data.get('hist_clicks', pd.DataFrame())\n",
    "    \n",
    "    log(\"\\n✓ Data loaded successfully\")\n",
    "    log(f\"  Analysis period: {metadata['analysis_start_date']} to {metadata['analysis_end_date']}\")\n",
    "    log(f\"  Sampling fraction: {metadata['sampling_fraction']:.2%}\")\n",
    "    log(f\"  Journey window: {metadata['journey_window_hours']} hours\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"ERROR: Failed to load checkpoint data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Journey Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:02:57] \n",
      "================================================================================\n",
      "[2025-09-23 05:02:57] SECTION 2: JOURNEY PROCESSING\n",
      "[2025-09-23 05:02:57] ================================================================================\n",
      "[2025-09-23 05:02:57] \n",
      "Sessionizing events...\n",
      "[2025-09-23 05:02:57] Total events: 449,505\n"
     ]
    }
   ],
   "source": [
    "# Sessionize events\n",
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"SECTION 2: JOURNEY PROCESSING\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "log(\"\\nSessionizing events...\")\n",
    "events = []\n",
    "\n",
    "# Add auctions\n",
    "auctions = auctions_users.copy()\n",
    "auctions['event_type'] = 'auction'\n",
    "auctions['event_time'] = pd.to_datetime(auctions['CREATED_AT'])\n",
    "auctions['PRODUCT_ID'] = None\n",
    "auctions['VENDOR_ID'] = None\n",
    "events.append(auctions[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add impressions\n",
    "impressions_evt = impressions.copy()\n",
    "impressions_evt['event_type'] = 'impression'\n",
    "impressions_evt['event_time'] = pd.to_datetime(impressions_evt['OCCURRED_AT'])\n",
    "events.append(impressions_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add clicks\n",
    "clicks_evt = clicks.copy()\n",
    "clicks_evt['event_type'] = 'click'\n",
    "clicks_evt['event_time'] = pd.to_datetime(clicks_evt['OCCURRED_AT'])\n",
    "events.append(clicks_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Add purchases\n",
    "purchases_evt = purchases.copy()\n",
    "purchases_evt['event_type'] = 'purchase'\n",
    "purchases_evt['event_time'] = pd.to_datetime(purchases_evt['PURCHASED_AT'])\n",
    "purchases_evt['AUCTION_ID'] = None\n",
    "purchases_evt['VENDOR_ID'] = None\n",
    "events.append(purchases_evt[['USER_ID', 'AUCTION_ID', 'event_type', 'event_time', 'PRODUCT_ID', 'VENDOR_ID']])\n",
    "\n",
    "# Combine and sort\n",
    "all_events = pd.concat(events, ignore_index=True)\n",
    "all_events = all_events.sort_values(['USER_ID', 'event_time'])\n",
    "log(f\"Total events: {len(all_events):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:01] \n",
      "Enhancing event data with metadata...\n",
      "[2025-09-23 05:03:03]   Created ranking lookup with 3,410,466 auction-product pairs\n",
      "[2025-09-23 05:03:08] Enhanced events created: 449,505 total events with metadata\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced event stream with metadata\n",
    "log(\"\\nEnhancing event data with metadata...\")\n",
    "\n",
    "# Add ranking information from auctions_results\n",
    "auction_ranks = auctions_results[['AUCTION_ID', 'PRODUCT_ID', 'RANKING']].copy()\n",
    "auction_ranks = auction_ranks.dropna()\n",
    "auction_ranks = auction_ranks.groupby(['AUCTION_ID', 'PRODUCT_ID'])['RANKING'].min().reset_index()\n",
    "log(f\"  Created ranking lookup with {len(auction_ranks):,} auction-product pairs\")\n",
    "\n",
    "# Merge rankings into events\n",
    "all_events_enhanced = all_events.merge(\n",
    "    auction_ranks, \n",
    "    on=['AUCTION_ID', 'PRODUCT_ID'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add catalog information\n",
    "all_events_enhanced = all_events_enhanced.merge(\n",
    "    catalog[['PRODUCT_ID', 'BRAND', 'DEPARTMENT_ID', 'PRICE']].drop_duplicates(),\n",
    "    on='PRODUCT_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "all_events_enhanced = all_events_enhanced.sort_values(['USER_ID', 'event_time'])\n",
    "log(f\"Enhanced events created: {len(all_events_enhanced):,} total events with metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:09] \n",
      "Identifying session breaks (>2 hour gaps)...\n",
      "[2025-09-23 05:03:10] Created 10,360 unique journeys\n"
     ]
    }
   ],
   "source": [
    "# Identify session breaks (2+ hour gaps)\n",
    "SESSION_GAP_HOURS = metadata.get('session_gap_hours', 2)\n",
    "\n",
    "log(f\"\\nIdentifying session breaks (>{SESSION_GAP_HOURS} hour gaps)...\")\n",
    "all_events['prev_time'] = all_events.groupby('USER_ID')['event_time'].shift()\n",
    "all_events['time_diff'] = (all_events['event_time'] - all_events['prev_time']).dt.total_seconds() / 3600\n",
    "all_events['session_break'] = (all_events['time_diff'] >= SESSION_GAP_HOURS) | all_events['time_diff'].isna()\n",
    "\n",
    "# Assign journey IDs\n",
    "all_events['journey_id'] = all_events.groupby('USER_ID')['session_break'].cumsum()\n",
    "all_events['journey_id'] = all_events['USER_ID'] + '_' + all_events['journey_id'].astype(str)\n",
    "\n",
    "unique_journeys = all_events['journey_id'].nunique()\n",
    "log(f\"Created {unique_journeys:,} unique journeys\")\n",
    "\n",
    "# Also apply to enhanced events\n",
    "all_events_enhanced['prev_time'] = all_events_enhanced.groupby('USER_ID')['event_time'].shift()\n",
    "all_events_enhanced['time_diff'] = (all_events_enhanced['event_time'] - all_events_enhanced['prev_time']).dt.total_seconds() / 3600\n",
    "all_events_enhanced['session_break'] = (all_events_enhanced['time_diff'] >= SESSION_GAP_HOURS) | all_events_enhanced['time_diff'].isna()\n",
    "all_events_enhanced['journey_id'] = all_events_enhanced.groupby('USER_ID')['session_break'].cumsum()\n",
    "all_events_enhanced['journey_id'] = all_events_enhanced['USER_ID'] + '_' + all_events_enhanced['journey_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:13] \n",
      "================================================================================\n",
      "[2025-09-23 05:03:13] SECTION 3: FEATURE ENGINEERING\n",
      "[2025-09-23 05:03:13] ================================================================================\n",
      "[2025-09-23 05:03:13] \n",
      "Aggregating journey metrics...\n",
      "[2025-09-23 05:03:13] Unique (journey, product) pairs: 269,276\n",
      "[2025-09-23 05:03:14] Base metrics shape: (269276, 7)\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"SECTION 3: FEATURE ENGINEERING\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Aggregate metrics at (journey_id, product_id) level\n",
    "log(\"\\nAggregating journey metrics...\")\n",
    "\n",
    "# Get products that received impressions\n",
    "impression_events = all_events[all_events['event_type'] == 'impression']\n",
    "journey_products = impression_events[['journey_id', 'PRODUCT_ID']].drop_duplicates()\n",
    "log(f\"Unique (journey, product) pairs: {len(journey_products):,}\")\n",
    "\n",
    "# Initialize metrics DataFrame\n",
    "metrics = journey_products.copy()\n",
    "\n",
    "# Add user_id\n",
    "journey_users = all_events[['journey_id', 'USER_ID']].drop_duplicates()\n",
    "metrics = metrics.merge(journey_users, on='journey_id', how='left')\n",
    "\n",
    "# Calculate impression counts\n",
    "imp_counts = impression_events.groupby(['journey_id', 'PRODUCT_ID']).size().reset_index(name='impressions_on_product')\n",
    "metrics = metrics.merge(imp_counts, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "\n",
    "# Calculate click counts\n",
    "click_events = all_events[all_events['event_type'] == 'click']\n",
    "click_counts = click_events.groupby(['journey_id', 'PRODUCT_ID']).size().reset_index(name='clicks_on_product')\n",
    "metrics = metrics.merge(click_counts, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "metrics['clicks_on_product'] = metrics['clicks_on_product'].fillna(0).astype(int)\n",
    "\n",
    "# Calculate purchase counts\n",
    "purchase_events = all_events[all_events['event_type'] == 'purchase']\n",
    "purchase_counts = purchase_events.groupby(['journey_id', 'PRODUCT_ID']).size().reset_index(name='purchases_on_product')\n",
    "metrics = metrics.merge(purchase_counts, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "metrics['purchases_on_product'] = metrics['purchases_on_product'].fillna(0).astype(int)\n",
    "metrics['did_purchase_product'] = (metrics['purchases_on_product'] > 0).astype(int)\n",
    "\n",
    "log(f\"Base metrics shape: {metrics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:25] \n",
      "============================================================\n",
      "[2025-09-23 05:03:25] FEATURE: HALO EFFECTS\n",
      "[2025-09-23 05:03:25] ============================================================\n",
      "[2025-09-23 05:03:30]   Products where same brand purchased: 2,474\n",
      "[2025-09-23 05:03:30]   Products where same department purchased: 11,153\n",
      "[2025-09-23 05:03:30]   Brand purchase rate: 0.9188%\n",
      "[2025-09-23 05:03:30]   Department purchase rate: 4.1418%\n"
     ]
    }
   ],
   "source": [
    "# HALO EFFECTS\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FEATURE: HALO EFFECTS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Get purchased brands/departments per journey\n",
    "purchase_events_enhanced = all_events_enhanced[all_events_enhanced['event_type'] == 'purchase'].copy()\n",
    "\n",
    "if len(purchase_events_enhanced) > 0:\n",
    "    # Brands purchased in each journey\n",
    "    purchased_brands = purchase_events_enhanced.dropna(subset=['BRAND']).groupby('journey_id')['BRAND'].apply(lambda x: x.unique().tolist()).reset_index()\n",
    "    purchased_brands.columns = ['journey_id', 'purchased_brands']\n",
    "    \n",
    "    # Departments purchased in each journey\n",
    "    purchased_depts = purchase_events_enhanced.dropna(subset=['DEPARTMENT_ID']).groupby('journey_id')['DEPARTMENT_ID'].apply(lambda x: x.unique().tolist()).reset_index()\n",
    "    purchased_depts.columns = ['journey_id', 'purchased_departments']\n",
    "    \n",
    "    # Merge into metrics\n",
    "    metrics = metrics.merge(purchased_brands, on='journey_id', how='left')\n",
    "    metrics = metrics.merge(purchased_depts, on='journey_id', how='left')\n",
    "    \n",
    "    # Fill NaNs with empty lists\n",
    "    metrics['purchased_brands'] = metrics['purchased_brands'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    metrics['purchased_departments'] = metrics['purchased_departments'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    \n",
    "    # Add brand/department info to metrics\n",
    "    metrics = metrics.merge(\n",
    "        catalog[['PRODUCT_ID', 'BRAND', 'DEPARTMENT_ID']].drop_duplicates(),\n",
    "        on='PRODUCT_ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Create brand halo outcome\n",
    "    metrics['did_purchase_brand_in_journey'] = metrics.apply(\n",
    "        lambda row: 1 if pd.notna(row['BRAND']) and row['BRAND'] in row['purchased_brands'] else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create department halo outcome\n",
    "    metrics['did_purchase_department_in_journey'] = metrics.apply(\n",
    "        lambda row: 1 if pd.notna(row['DEPARTMENT_ID']) and row['DEPARTMENT_ID'] in row['purchased_departments'] else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    log(f\"  Products where same brand purchased: {metrics['did_purchase_brand_in_journey'].sum():,}\")\n",
    "    log(f\"  Products where same department purchased: {metrics['did_purchase_department_in_journey'].sum():,}\")\n",
    "    log(f\"  Brand purchase rate: {metrics['did_purchase_brand_in_journey'].mean():.4%}\")\n",
    "    log(f\"  Department purchase rate: {metrics['did_purchase_department_in_journey'].mean():.4%}\")\n",
    "else:\n",
    "    metrics['did_purchase_brand_in_journey'] = 0\n",
    "    metrics['did_purchase_department_in_journey'] = 0\n",
    "    log(\"  No purchases found - halo outcomes set to 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:30] \n",
      "============================================================\n",
      "[2025-09-23 05:03:30] FEATURE: COMPETITIVE CONTEXT\n",
      "[2025-09-23 05:03:30] ============================================================\n",
      "[2025-09-23 05:03:30] Calculating auction competitiveness...\n",
      "[2025-09-23 05:03:35]   Avg Winning Rank - Mean: 24.99\n",
      "[2025-09-23 05:03:35]   Product Win Rate - Mean: 0.8947\n"
     ]
    }
   ],
   "source": [
    "# COMPETITIVE CONTEXT\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FEATURE: COMPETITIVE CONTEXT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Auction competitiveness metrics\n",
    "log(\"Calculating auction competitiveness...\")\n",
    "auction_metrics = auctions_results.groupby('AUCTION_ID').agg({\n",
    "    'VENDOR_ID': 'nunique',\n",
    "    'PRODUCT_ID': 'nunique',\n",
    "    'RANKING': ['min', 'max', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "auction_metrics.columns = ['AUCTION_ID', 'num_bidders', 'num_products', 'min_rank', 'max_rank', 'avg_rank']\n",
    "auction_metrics['auction_competitiveness'] = auction_metrics['num_bidders'] * np.log1p(auction_metrics['max_rank'])\n",
    "\n",
    "# Map average winning rank to impressed products\n",
    "impression_events_enhanced = all_events_enhanced[all_events_enhanced['event_type'] == 'impression']\n",
    "avg_winning_rank = impression_events_enhanced.merge(\n",
    "    auction_metrics[['AUCTION_ID', 'avg_rank']], \n",
    "    on='AUCTION_ID', \n",
    "    how='left'\n",
    ")\n",
    "avg_winning_rank = avg_winning_rank.groupby(['journey_id', 'PRODUCT_ID'])['avg_rank'].mean().reset_index()\n",
    "avg_winning_rank.columns = ['journey_id', 'PRODUCT_ID', 'avg_winning_rank']\n",
    "\n",
    "metrics = metrics.merge(avg_winning_rank, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "\n",
    "# Product win rates\n",
    "product_wins = auctions_results[auctions_results['IS_WINNER'] == True].groupby('PRODUCT_ID').size().reset_index(name='wins')\n",
    "product_bids = auctions_results.groupby('PRODUCT_ID').size().reset_index(name='bids')\n",
    "product_win_rate = product_wins.merge(product_bids, on='PRODUCT_ID')\n",
    "product_win_rate['product_win_rate'] = product_win_rate['wins'] / product_win_rate['bids']\n",
    "\n",
    "metrics = metrics.merge(product_win_rate[['PRODUCT_ID', 'product_win_rate']], on='PRODUCT_ID', how='left')\n",
    "\n",
    "log(f\"  Avg Winning Rank - Mean: {metrics['avg_winning_rank'].mean():.2f}\")\n",
    "log(f\"  Product Win Rate - Mean: {metrics['product_win_rate'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:35] \n",
      "============================================================\n",
      "[2025-09-23 05:03:35] FEATURE: TEMPORAL DYNAMICS\n",
      "[2025-09-23 05:03:35] ============================================================\n",
      "[2025-09-23 05:03:35] Calculating click order features...\n",
      "[2025-09-23 05:03:35]   Products that were first click: 3,029\n",
      "[2025-09-23 05:03:35]   Products that were last click: 3,047\n"
     ]
    }
   ],
   "source": [
    "# TEMPORAL FEATURES\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FEATURE: TEMPORAL DYNAMICS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Click Order Features\n",
    "log(\"Calculating click order features...\")\n",
    "click_events_enhanced = all_events_enhanced[all_events_enhanced['event_type'] == 'click'].copy()\n",
    "\n",
    "if len(click_events_enhanced) > 0:\n",
    "    # Sort clicks by time within each journey\n",
    "    click_events_enhanced = click_events_enhanced.sort_values(['journey_id', 'event_time'])\n",
    "    \n",
    "    # Add click order within journey\n",
    "    click_events_enhanced['click_order_in_journey'] = click_events_enhanced.groupby('journey_id').cumcount() + 1\n",
    "    \n",
    "    # Get first and last click positions for each product\n",
    "    first_clicks = click_events_enhanced.groupby(['journey_id', 'PRODUCT_ID'])['click_order_in_journey'].min().reset_index()\n",
    "    first_clicks.columns = ['journey_id', 'PRODUCT_ID', 'first_click_position']\n",
    "    \n",
    "    last_clicks = click_events_enhanced.groupby(['journey_id', 'PRODUCT_ID'])['click_order_in_journey'].max().reset_index()\n",
    "    last_clicks.columns = ['journey_id', 'PRODUCT_ID', 'last_click_position']\n",
    "    \n",
    "    # Merge into metrics\n",
    "    metrics = metrics.merge(first_clicks, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "    metrics = metrics.merge(last_clicks, on=['journey_id', 'PRODUCT_ID'], how='left')\n",
    "    \n",
    "    # Create binary flags\n",
    "    metrics['is_first_click_in_journey'] = (metrics['first_click_position'] == 1).astype(int).fillna(0)\n",
    "    \n",
    "    # Find the last clicked product in each journey\n",
    "    max_clicks_per_journey = metrics.groupby('journey_id')['last_click_position'].max().reset_index()\n",
    "    max_clicks_per_journey.columns = ['journey_id', 'max_click_position']\n",
    "    metrics = metrics.merge(max_clicks_per_journey, on='journey_id', how='left')\n",
    "    metrics['is_last_click_product'] = (metrics['last_click_position'] == metrics['max_click_position']).astype(int).fillna(0)\n",
    "    \n",
    "    log(f\"  Products that were first click: {metrics['is_first_click_in_journey'].sum():,}\")\n",
    "    log(f\"  Products that were last click: {metrics['is_last_click_product'].sum():,}\")\n",
    "else:\n",
    "    metrics['is_first_click_in_journey'] = 0\n",
    "    metrics['is_last_click_product'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:35] \n",
      "Calculating journey-level metrics...\n",
      "[2025-09-23 05:03:37] Final dataset shape: (269276, 24)\n",
      "[2025-09-23 05:03:37] Purchase rate: 0.0171%\n"
     ]
    }
   ],
   "source": [
    "# Journey-level metrics\n",
    "log(\"\\nCalculating journey-level metrics...\")\n",
    "\n",
    "total_clicks = click_events.groupby('journey_id').size().reset_index(name='total_clicks')\n",
    "metrics = metrics.merge(total_clicks, on='journey_id', how='left')\n",
    "metrics['total_clicks'] = metrics['total_clicks'].fillna(0).astype(int)\n",
    "\n",
    "# Journey duration\n",
    "journey_times = all_events.groupby('journey_id')['event_time'].agg(['min', 'max']).reset_index()\n",
    "journey_times['journey_duration_hours'] = (journey_times['max'] - journey_times['min']).dt.total_seconds() / 3600\n",
    "metrics = metrics.merge(journey_times[['journey_id', 'journey_duration_hours']], on='journey_id', how='left')\n",
    "\n",
    "# Distinct products viewed\n",
    "distinct_products = impression_events.groupby('journey_id')['PRODUCT_ID'].nunique().reset_index(name='distinct_products')\n",
    "metrics = metrics.merge(distinct_products, on='journey_id', how='left')\n",
    "\n",
    "# Add catalog features\n",
    "if 'PRICE' not in metrics.columns:\n",
    "    catalog_features = catalog[['PRODUCT_ID', 'PRICE']].copy()\n",
    "    metrics = metrics.merge(catalog_features, on='PRODUCT_ID', how='left')\n",
    "\n",
    "log(f\"Final dataset shape: {metrics.shape}\")\n",
    "log(f\"Purchase rate: {metrics['did_purchase_product'].mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:37] \n",
      "Engineering historical features...\n",
      "[2025-09-23 05:03:37]   Added user purchase history features\n",
      "[2025-09-23 05:03:37]   Added user CTR history features\n",
      "[2025-09-23 05:03:37]   Added vendor CTR history features\n",
      "[2025-09-23 05:03:37] \n",
      "Final dataset shape with all features: (269276, 32)\n",
      "[2025-09-23 05:03:37] Purchase rate: 0.0171%\n"
     ]
    }
   ],
   "source": [
    "# Historical Features (if available)\n",
    "log(\"\\nEngineering historical features...\")\n",
    "\n",
    "if len(hist_purchases) > 0:\n",
    "    # User-level historical features\n",
    "    user_purchase_history = hist_purchases.groupby('USER_ID').agg({\n",
    "        'PURCHASE_ID': 'count',\n",
    "        'UNIT_PRICE': ['mean', 'sum']\n",
    "    }).reset_index()\n",
    "    user_purchase_history.columns = ['USER_ID', 'hist_purchase_count', 'hist_avg_price', 'hist_total_spend']\n",
    "    metrics = metrics.merge(user_purchase_history, on='USER_ID', how='left')\n",
    "    log(f\"  Added user purchase history features\")\n",
    "\n",
    "if len(hist_impressions) > 0 and len(hist_clicks) > 0:\n",
    "    # User CTR history\n",
    "    user_imp_counts = hist_impressions.groupby('USER_ID').size().reset_index(name='hist_impressions')\n",
    "    user_click_counts = hist_clicks.groupby('USER_ID').size().reset_index(name='hist_clicks')\n",
    "    \n",
    "    user_ctr = user_imp_counts.merge(user_click_counts, on='USER_ID', how='left')\n",
    "    user_ctr['hist_clicks'] = user_ctr['hist_clicks'].fillna(0)\n",
    "    user_ctr['hist_user_ctr'] = user_ctr['hist_clicks'] / user_ctr['hist_impressions']\n",
    "    user_ctr['hist_user_ctr'] = user_ctr['hist_user_ctr'].fillna(0)\n",
    "    \n",
    "    metrics = metrics.merge(user_ctr[['USER_ID', 'hist_user_ctr', 'hist_impressions', 'hist_clicks']], \n",
    "                            on='USER_ID', how='left')\n",
    "    log(f\"  Added user CTR history features\")\n",
    "    \n",
    "    # Vendor CTR history\n",
    "    if 'VENDOR_ID' in hist_impressions.columns:\n",
    "        vendor_imp_counts = hist_impressions.groupby('VENDOR_ID').size().reset_index(name='vendor_hist_imps')\n",
    "        vendor_click_counts = hist_clicks.groupby('VENDOR_ID').size().reset_index(name='vendor_hist_clicks')\n",
    "        \n",
    "        vendor_ctr = vendor_imp_counts.merge(vendor_click_counts, on='VENDOR_ID', how='left')\n",
    "        vendor_ctr['vendor_hist_clicks'] = vendor_ctr['vendor_hist_clicks'].fillna(0)\n",
    "        vendor_ctr['vendor_hist_ctr'] = vendor_ctr['vendor_hist_clicks'] / vendor_ctr['vendor_hist_imps']\n",
    "        vendor_ctr['vendor_hist_ctr'] = vendor_ctr['vendor_hist_ctr'].fillna(0)\n",
    "        \n",
    "        # Get vendor_id for each product from impressions\n",
    "        product_vendor = impressions[['PRODUCT_ID', 'VENDOR_ID']].drop_duplicates()\n",
    "        metrics = metrics.merge(product_vendor, on='PRODUCT_ID', how='left', suffixes=('', '_new'))\n",
    "        if 'VENDOR_ID_new' in metrics.columns:\n",
    "            metrics['VENDOR_ID'] = metrics['VENDOR_ID'].fillna(metrics['VENDOR_ID_new'])\n",
    "            metrics = metrics.drop(columns=['VENDOR_ID_new'])\n",
    "        metrics = metrics.merge(vendor_ctr[['VENDOR_ID', 'vendor_hist_ctr']], on='VENDOR_ID', how='left')\n",
    "        log(f\"  Added vendor CTR history features\")\n",
    "\n",
    "# Fill missing historical features with 0\n",
    "hist_cols = [c for c in metrics.columns if 'hist_' in c or 'vendor_hist' in c]\n",
    "for col in hist_cols:\n",
    "    if col in metrics.columns:\n",
    "        metrics[col] = metrics[col].fillna(0)\n",
    "\n",
    "log(f\"\\nFinal dataset shape with all features: {metrics.shape}\")\n",
    "log(f\"Purchase rate: {metrics['did_purchase_product'].mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "✓ Processed dataset saved to: data/user_journey_causal_dataset.parquet\n",
      "[2025-09-23 05:03:38]   Shape: (269276, 32)\n",
      "[2025-09-23 05:03:38]   Memory usage: 202.48 MB\n"
     ]
    }
   ],
   "source": [
    "# Save processed dataset\n",
    "dataset_path = Path(\"./data/user_journey_causal_dataset.parquet\")\n",
    "dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "metrics.to_parquet(dataset_path, index=False)\n",
    "log(f\"\\n✓ Processed dataset saved to: {dataset_path}\")\n",
    "log(f\"  Shape: {metrics.shape}\")\n",
    "log(f\"  Memory usage: {metrics.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "================================================================================\n",
      "[2025-09-23 05:03:38] SECTION 4: EXPLORATORY DATA ANALYSIS\n",
      "[2025-09-23 05:03:38] ================================================================================\n",
      "[2025-09-23 05:03:38] \n",
      "1. JOURNEY OVERVIEW\n",
      "[2025-09-23 05:03:38] ------------------------------------------------------------\n",
      "[2025-09-23 05:03:38] Total journeys: 7,820\n",
      "[2025-09-23 05:03:38] Total users: 1,124\n",
      "[2025-09-23 05:03:38] Total products: 215,589\n",
      "[2025-09-23 05:03:38] Total observations: 269,276\n",
      "[2025-09-23 05:03:38] \n",
      "Average events per journey:\n",
      "[2025-09-23 05:03:38]   auction: 8.56\n",
      "[2025-09-23 05:03:38]   click: 1.08\n",
      "[2025-09-23 05:03:38]   impression: 33.57\n",
      "[2025-09-23 05:03:38]   purchase: 0.18\n",
      "[2025-09-23 05:03:38] \n",
      "Journey duration (hours):\n",
      "[2025-09-23 05:03:38]   Mean: 2.00\n",
      "[2025-09-23 05:03:38]   Median: 0.89\n",
      "[2025-09-23 05:03:38]   75th percentile: 2.36\n",
      "[2025-09-23 05:03:38]   95th percentile: 8.15\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"SECTION 4: EXPLORATORY DATA ANALYSIS\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# 1. JOURNEY OVERVIEW\n",
    "log(\"\\n1. JOURNEY OVERVIEW\")\n",
    "log(\"-\" * 60)\n",
    "log(f\"Total journeys: {metrics['journey_id'].nunique():,}\")\n",
    "log(f\"Total users: {metrics['USER_ID'].nunique():,}\")\n",
    "log(f\"Total products: {metrics['PRODUCT_ID'].nunique():,}\")\n",
    "log(f\"Total observations: {len(metrics):,}\")\n",
    "\n",
    "# Journey composition\n",
    "journey_stats = all_events.groupby('journey_id')['event_type'].value_counts().unstack(fill_value=0)\n",
    "log(\"\\nAverage events per journey:\")\n",
    "for event_type in journey_stats.columns:\n",
    "    log(f\"  {event_type}: {journey_stats[event_type].mean():.2f}\")\n",
    "\n",
    "# Journey duration distribution\n",
    "log(\"\\nJourney duration (hours):\")\n",
    "log(f\"  Mean: {metrics['journey_duration_hours'].mean():.2f}\")\n",
    "log(f\"  Median: {metrics['journey_duration_hours'].median():.2f}\")\n",
    "log(f\"  75th percentile: {metrics['journey_duration_hours'].quantile(0.75):.2f}\")\n",
    "log(f\"  95th percentile: {metrics['journey_duration_hours'].quantile(0.95):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "2. CONVERSION FUNNEL\n",
      "[2025-09-23 05:03:38] ------------------------------------------------------------\n",
      "[2025-09-23 05:03:38] Journey-level funnel:\n",
      "[2025-09-23 05:03:38]   Journeys with impressions: 7,820 (100.00%)\n",
      "[2025-09-23 05:03:38]   Journeys with clicks: 3,047 (38.96%)\n",
      "[2025-09-23 05:03:38]   Journeys with purchases: 43 (0.55%)\n",
      "[2025-09-23 05:03:38] \n",
      "Product-level funnel (products that received impressions):\n",
      "[2025-09-23 05:03:38]   Products impressed: 269,276 (100.00%)\n",
      "[2025-09-23 05:03:38]   Products clicked: 9,768 (3.63%)\n",
      "[2025-09-23 05:03:38]   Products purchased: 46 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# 2. CONVERSION FUNNEL\n",
    "log(\"\\n2. CONVERSION FUNNEL\")\n",
    "log(\"-\" * 60)\n",
    "\n",
    "# Journey-level funnel\n",
    "journey_level = metrics.groupby('journey_id').agg({\n",
    "    'impressions_on_product': 'sum',\n",
    "    'clicks_on_product': 'sum',\n",
    "    'purchases_on_product': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "journeys_with_impressions = (journey_level['impressions_on_product'] > 0).sum()\n",
    "journeys_with_clicks = (journey_level['clicks_on_product'] > 0).sum()\n",
    "journeys_with_purchases = (journey_level['purchases_on_product'] > 0).sum()\n",
    "\n",
    "log(\"Journey-level funnel:\")\n",
    "log(f\"  Journeys with impressions: {journeys_with_impressions:,} (100.00%)\")\n",
    "log(f\"  Journeys with clicks: {journeys_with_clicks:,} ({journeys_with_clicks/journeys_with_impressions:.2%})\")\n",
    "log(f\"  Journeys with purchases: {journeys_with_purchases:,} ({journeys_with_purchases/journeys_with_impressions:.2%})\")\n",
    "\n",
    "# Product-level funnel\n",
    "log(\"\\nProduct-level funnel (products that received impressions):\")\n",
    "products_with_clicks = (metrics['clicks_on_product'] > 0).sum()\n",
    "products_purchased = (metrics['did_purchase_product'] > 0).sum()\n",
    "\n",
    "log(f\"  Products impressed: {len(metrics):,} (100.00%)\")\n",
    "log(f\"  Products clicked: {products_with_clicks:,} ({products_with_clicks/len(metrics):.2%})\")\n",
    "log(f\"  Products purchased: {products_purchased:,} ({products_purchased/len(metrics):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "3. ENGAGEMENT METRICS\n",
      "[2025-09-23 05:03:38] ------------------------------------------------------------\n",
      "[2025-09-23 05:03:38] Overall CTR: 3.19% (11,087 clicks / 347,741 impressions)\n",
      "[2025-09-23 05:03:38] \n",
      "Conversion rates:\n",
      "[2025-09-23 05:03:38]   Products with clicks: 0.4197%\n",
      "[2025-09-23 05:03:38]   Products without clicks: 0.0019%\n",
      "[2025-09-23 05:03:38]   Lift from clicking: 216.9x\n",
      "[2025-09-23 05:03:38] \n",
      "Click distribution on impressed products:\n",
      "[2025-09-23 05:03:38]   0 clicks: 259,508 products (96.37%)\n",
      "[2025-09-23 05:03:38]   1 clicks: 8,637 products (3.21%)\n",
      "[2025-09-23 05:03:38]   2 clicks: 977 products (0.36%)\n",
      "[2025-09-23 05:03:38]   3 clicks: 130 products (0.05%)\n",
      "[2025-09-23 05:03:38]   4 clicks: 17 products (0.01%)\n",
      "[2025-09-23 05:03:38]   5 clicks: 4 products (0.00%)\n",
      "[2025-09-23 05:03:38]   6 clicks: 3 products (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# 3. ENGAGEMENT METRICS\n",
    "log(\"\\n3. ENGAGEMENT METRICS\")\n",
    "log(\"-\" * 60)\n",
    "\n",
    "# Overall CTR\n",
    "total_impressions = metrics['impressions_on_product'].sum()\n",
    "total_clicks = metrics['clicks_on_product'].sum()\n",
    "ctr = total_clicks / total_impressions if total_impressions > 0 else 0\n",
    "log(f\"Overall CTR: {ctr:.2%} ({total_clicks:,} clicks / {total_impressions:,} impressions)\")\n",
    "\n",
    "# Conversion rate by click status\n",
    "clicked_products = metrics[metrics['clicks_on_product'] > 0]\n",
    "not_clicked_products = metrics[metrics['clicks_on_product'] == 0]\n",
    "\n",
    "log(\"\\nConversion rates:\")\n",
    "log(f\"  Products with clicks: {clicked_products['did_purchase_product'].mean():.4%}\")\n",
    "log(f\"  Products without clicks: {not_clicked_products['did_purchase_product'].mean():.4%}\")\n",
    "if not_clicked_products['did_purchase_product'].mean() > 0:\n",
    "    lift = (clicked_products['did_purchase_product'].mean() / not_clicked_products['did_purchase_product'].mean() - 1)\n",
    "    log(f\"  Lift from clicking: {lift:.1f}x\")\n",
    "\n",
    "# Click distribution\n",
    "log(\"\\nClick distribution on impressed products:\")\n",
    "click_dist = metrics['clicks_on_product'].value_counts().sort_index().head(10)\n",
    "for clicks, count in click_dist.items():\n",
    "    log(f\"  {clicks} clicks: {count:,} products ({count/len(metrics):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "4. PRODUCT CHARACTERISTICS\n",
      "[2025-09-23 05:03:38] ------------------------------------------------------------\n",
      "[2025-09-23 05:03:38] Price distribution (for products with price data):\n",
      "[2025-09-23 05:03:38]   Mean: $166.34\n",
      "[2025-09-23 05:03:38]   Median: $40.00\n",
      "[2025-09-23 05:03:38]   25th percentile: $24.00\n",
      "[2025-09-23 05:03:38]   75th percentile: $75.00\n",
      "[2025-09-23 05:03:38] \n",
      "Conversion by price quartile:\n",
      "[2025-09-23 05:03:38]   Q1 $3-$24: 0.0270%\n",
      "[2025-09-23 05:03:38]   Q2 $25-$40: 0.0135%\n",
      "[2025-09-23 05:03:38]   Q3 $41-$75: 0.0224%\n",
      "[2025-09-23 05:03:38]   Q4 $76-$8008135: 0.0060%\n"
     ]
    }
   ],
   "source": [
    "# 4. PRODUCT CHARACTERISTICS\n",
    "log(\"\\n4. PRODUCT CHARACTERISTICS\")\n",
    "log(\"-\" * 60)\n",
    "\n",
    "# Price distribution\n",
    "log(\"Price distribution (for products with price data):\")\n",
    "price_data = metrics[metrics['PRICE'].notna()]\n",
    "if len(price_data) > 0:\n",
    "    log(f\"  Mean: ${price_data['PRICE'].mean():.2f}\")\n",
    "    log(f\"  Median: ${price_data['PRICE'].median():.2f}\")\n",
    "    log(f\"  25th percentile: ${price_data['PRICE'].quantile(0.25):.2f}\")\n",
    "    log(f\"  75th percentile: ${price_data['PRICE'].quantile(0.75):.2f}\")\n",
    "\n",
    "    # Conversion by price quartile\n",
    "    try:\n",
    "        price_data['price_quartile'] = pd.qcut(price_data['PRICE'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')\n",
    "        log(\"\\nConversion by price quartile:\")\n",
    "        for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "            q_data = price_data[price_data['price_quartile'] == q]\n",
    "            if len(q_data) > 0:\n",
    "                conv_rate = q_data['did_purchase_product'].mean()\n",
    "                price_range = f\"${q_data['PRICE'].min():.0f}-${q_data['PRICE'].max():.0f}\"\n",
    "                log(f\"  {q} {price_range}: {conv_rate:.4%}\")\n",
    "    except:\n",
    "        log(\"  Could not create price quartiles - insufficient price variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Causal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "================================================================================\n",
      "[2025-09-23 05:03:38] SECTION 5: CAUSAL ANALYSIS\n",
      "[2025-09-23 05:03:38] ================================================================================\n",
      "[2025-09-23 05:03:38] INFO: Filled missing PRICE with median value of $40.00\n",
      "[2025-09-23 05:03:38] Prepared 269,276 observations for analysis\n",
      "[2025-09-23 05:03:38] Features created including interactions and transformations\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"SECTION 5: CAUSAL ANALYSIS\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Prepare features for regression\n",
    "df = metrics.copy()\n",
    "\n",
    "# Handle missing values\n",
    "if 'PRICE' in df.columns:\n",
    "    median_price = df['PRICE'].median()\n",
    "    df['PRICE'].fillna(median_price, inplace=True)\n",
    "    log(f\"INFO: Filled missing PRICE with median value of ${median_price:.2f}\")\n",
    "\n",
    "# For other numerics, 0 is reasonable\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col != 'PRICE':  # Already handled\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "# Create log-transformed features\n",
    "df['log_price'] = np.log1p(df['PRICE'])\n",
    "df['log_journey_duration'] = np.log1p(df['journey_duration_hours'])\n",
    "df['log_impressions'] = np.log1p(df['impressions_on_product'])\n",
    "\n",
    "# Create interaction terms\n",
    "df['clicks_x_price'] = df['clicks_on_product'] * df['log_price']\n",
    "df['clicks_x_duration'] = df['total_clicks'] * df['log_journey_duration']\n",
    "\n",
    "# Create revenue outcome\n",
    "df['log_revenue'] = np.log1p(df['PRICE'] * df['did_purchase_product'])\n",
    "\n",
    "log(f\"Prepared {len(df):,} observations for analysis\")\n",
    "log(f\"Features created including interactions and transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "============================================================\n",
      "[2025-09-23 05:03:38] MODEL SPECIFICATION\n",
      "[2025-09-23 05:03:38] ============================================================\n",
      "[2025-09-23 05:03:38] MODEL FORMULATIONS:\n",
      "[2025-09-23 05:03:38]   Treatment: total_clicks (journey-level clicks)\n",
      "[2025-09-23 05:03:38]   Product treatment: clicks_on_product (product-specific clicks)\n",
      "[2025-09-23 05:03:38]   Base controls: log_price, log_journey_duration, distinct_products, log_impressions\n",
      "[2025-09-23 05:03:38]   Historical controls: hist_purchase_count, hist_user_ctr, vendor_hist_ctr\n",
      "[2025-09-23 05:03:38]   Competitive controls: avg_winning_rank, product_win_rate\n",
      "[2025-09-23 05:03:38] \n",
      "  Logit: did_purchase_product ~ total_clicks + clicks_on_product + log_price + log_journey_duration + distinct_products + log_impressions + hist_purchase_count + hist_user_ctr + vendor_hist_ctr + avg_winning_rank + product_win_rate\n",
      "[2025-09-23 05:03:38]   OLS:   log_revenue ~ total_clicks + clicks_on_product + log_price + log_journey_duration + distinct_products + log_impressions + hist_purchase_count + hist_user_ctr + vendor_hist_ctr + avg_winning_rank + product_win_rate\n"
     ]
    }
   ],
   "source": [
    "# DEFINE MODEL FORMULAS\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"MODEL SPECIFICATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Base controls\n",
    "base_controls = ['log_price', 'log_journey_duration', 'distinct_products', 'log_impressions']\n",
    "\n",
    "# Add historical controls if they exist\n",
    "historical_controls = []\n",
    "for col in ['hist_purchase_count', 'hist_user_ctr', 'vendor_hist_ctr']:\n",
    "    if col in df.columns:\n",
    "        historical_controls.append(col)\n",
    "\n",
    "# Add competitive context if exists\n",
    "competitive_controls = []\n",
    "for col in ['avg_winning_rank', 'product_win_rate']:\n",
    "    if col in df.columns:\n",
    "        competitive_controls.append(col)\n",
    "\n",
    "# Combine all controls\n",
    "all_controls = base_controls + historical_controls + competitive_controls\n",
    "control_str = \" + \".join(all_controls)\n",
    "\n",
    "# Treatment variables\n",
    "treatment_var = \"total_clicks\"\n",
    "product_clicks_var = \"clicks_on_product\"\n",
    "\n",
    "# Model formulas\n",
    "logit_formula = f\"did_purchase_product ~ {treatment_var} + {product_clicks_var} + {control_str}\"\n",
    "ols_formula = f\"log_revenue ~ {treatment_var} + {product_clicks_var} + {control_str}\"\n",
    "\n",
    "log(\"MODEL FORMULATIONS:\")\n",
    "log(f\"  Treatment: {treatment_var} (journey-level clicks)\")\n",
    "log(f\"  Product treatment: {product_clicks_var} (product-specific clicks)\")\n",
    "log(f\"  Base controls: {', '.join(base_controls)}\")\n",
    "if historical_controls:\n",
    "    log(f\"  Historical controls: {', '.join(historical_controls)}\")\n",
    "if competitive_controls:\n",
    "    log(f\"  Competitive controls: {', '.join(competitive_controls)}\")\n",
    "log(f\"\\n  Logit: {logit_formula}\")\n",
    "log(f\"  OLS:   {ols_formula}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:38] \n",
      "============================================================\n",
      "[2025-09-23 05:03:38] MODEL 1: LOGISTIC REGRESSION\n",
      "[2025-09-23 05:03:38] ============================================================\n",
      "[2025-09-23 05:03:39] \n",
      "Logistic Regression Results:\n",
      "[2025-09-23 05:03:39] ----------------------------------------\n",
      "[2025-09-23 05:03:39]                             Logit Regression Results                            \n",
      "================================================================================\n",
      "Dep. Variable:     did_purchase_product   No. Observations:               269276\n",
      "Model:                            Logit   Df Residuals:                   269264\n",
      "Method:                             MLE   Df Model:                           11\n",
      "Date:                  Tue, 23 Sep 2025   Pseudo R-squ.:                  0.2462\n",
      "Time:                          05:03:39   Log-Likelihood:                -335.47\n",
      "converged:                         True   LL-Null:                       -445.04\n",
      "Covariance Type:              nonrobust   LLR p-value:                 7.832e-41\n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -9.3998      1.883     -4.993      0.000     -13.090      -5.710\n",
      "total_clicks             0.0255      0.018      1.423      0.155      -0.010       0.061\n",
      "clicks_on_product        2.2250      0.144     15.484      0.000       1.943       2.507\n",
      "log_price               -0.4946      0.190     -2.607      0.009      -0.866      -0.123\n",
      "log_journey_duration     0.6798      0.283      2.399      0.016       0.124       1.235\n",
      "distinct_products       -0.0070      0.003     -2.391      0.017      -0.013      -0.001\n",
      "log_impressions         -1.5912      0.656     -2.426      0.015      -2.876      -0.306\n",
      "hist_purchase_count      0.0022      0.005      0.408      0.683      -0.008       0.012\n",
      "hist_user_ctr          -19.6732      6.617     -2.973      0.003     -32.643      -6.704\n",
      "vendor_hist_ctr         -2.7304      3.773     -0.724      0.469     -10.125       4.664\n",
      "avg_winning_rank        -0.0087      0.020     -0.436      0.663      -0.048       0.031\n",
      "product_win_rate         4.1699      1.611      2.588      0.010       1.012       7.327\n",
      "========================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.65 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] ODDS RATIOS AND 95% CONFIDENCE INTERVALS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39] \n",
      "                      Coefficient    Odds Ratio  Std. Error  z-statistic         P>|z|   OR CI Lower  OR CI Upper\n",
      "Intercept               -9.399773  8.274286e-05    1.882609    -4.992951  5.946360e-07  2.066575e-06     0.003313\n",
      "total_clicks             0.025483  1.025811e+00    0.017907     1.423102  1.547067e-01  9.904325e-01     1.062453\n",
      "clicks_on_product        2.224953  9.253046e+00    0.143696    15.483714  4.469133e-54  6.981845e+00    12.263071\n",
      "log_price               -0.494564  6.098365e-01    0.189672    -2.607477  9.121224e-03  4.204987e-01     0.884427\n",
      "log_journey_duration     0.679790  1.973463e+00    0.283421     2.398515  1.646170e-02  1.132349e+00     3.439357\n",
      "distinct_products       -0.007020  9.930043e-01    0.002936    -2.391089  1.679846e-02  9.873064e-01     0.998735\n",
      "log_impressions         -1.591189  2.036834e-01    0.655766    -2.426457  1.524705e-02  5.633346e-02     0.736453\n",
      "hist_purchase_count      0.002153  1.002155e+00    0.005273     0.408265  6.830791e-01  9.918517e-01     1.012565\n",
      "hist_user_ctr          -19.673166  2.857937e-09    6.617237    -2.973018  2.948872e-03  6.659626e-15     0.001226\n",
      "vendor_hist_ctr         -2.730447  6.519016e-02    3.772594    -0.723758  4.692141e-01  4.008149e-05   106.027920\n",
      "avg_winning_rank        -0.008732  9.913064e-01    0.020048    -0.435539  6.631715e-01  9.531103e-01     1.031033\n",
      "product_win_rate         4.169917  6.471008e+01    1.610970     2.588451  9.640854e-03  2.752403e+00  1521.359748\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] KEY INTERPRETATIONS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39] \n",
      "'total_clicks' (Journey-level clicks):\n",
      "[2025-09-23 05:03:39]   Odds Ratio = 1.0258, p-value = 0.1547\n",
      "[2025-09-23 05:03:39]   Interpretation: Each additional journey click multiplies the odds of purchase by 1.0258\n",
      "[2025-09-23 05:03:39]   ✗ Not statistically significant at 5% level\n",
      "[2025-09-23 05:03:39] \n",
      "'clicks_on_product' (Product-specific clicks):\n",
      "[2025-09-23 05:03:39]   Odds Ratio = 9.2530, p-value = 0.0000\n",
      "[2025-09-23 05:03:39]   Interpretation: Each click on the specific product multiplies the odds of purchasing it by 9.2530\n",
      "[2025-09-23 05:03:39]   ✓ Statistically significant at 5% level\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] MODEL FIT STATISTICS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39]   Pseudo R-squared: 0.2462\n",
      "[2025-09-23 05:03:39]   Log-Likelihood: -335.4733\n",
      "[2025-09-23 05:03:39]   AIC: 694.9467\n",
      "[2025-09-23 05:03:39]   BIC: 820.9886\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Fit model using formula API\n",
    "    logit_model = smf.logit(formula=logit_formula, data=df)\n",
    "    logit_results = logit_model.fit(disp=0, maxiter=100)\n",
    "    \n",
    "    log(\"\\nLogistic Regression Results:\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    # Print summary\n",
    "    log(str(logit_results.summary()))\n",
    "    \n",
    "    # Calculate Odds Ratios\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"ODDS RATIOS AND 95% CONFIDENCE INTERVALS\")\n",
    "    log(\"=\"*40)\n",
    "    \n",
    "    odds_ratios = pd.DataFrame({\n",
    "        \"Coefficient\": logit_results.params,\n",
    "        \"Odds Ratio\": np.exp(logit_results.params),\n",
    "        \"Std. Error\": logit_results.bse,\n",
    "        \"z-statistic\": logit_results.tvalues,\n",
    "        \"P>|z|\": logit_results.pvalues,\n",
    "        \"OR CI Lower\": np.exp(logit_results.conf_int()[0]),\n",
    "        \"OR CI Upper\": np.exp(logit_results.conf_int()[1]),\n",
    "    })\n",
    "    \n",
    "    log(\"\\n\" + odds_ratios.to_string())\n",
    "    \n",
    "    # Key interpretations\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"KEY INTERPRETATIONS\")\n",
    "    log(\"=\"*40)\n",
    "    \n",
    "    if treatment_var in odds_ratios.index:\n",
    "        or_total = odds_ratios.loc[treatment_var, \"Odds Ratio\"]\n",
    "        p_total = odds_ratios.loc[treatment_var, \"P>|z|\"]\n",
    "        log(f\"\\n'{treatment_var}' (Journey-level clicks):\")\n",
    "        log(f\"  Odds Ratio = {or_total:.4f}, p-value = {p_total:.4f}\")\n",
    "        log(f\"  Interpretation: Each additional journey click multiplies the odds of purchase by {or_total:.4f}\")\n",
    "        if p_total < 0.05:\n",
    "            log(f\"  ✓ Statistically significant at 5% level\")\n",
    "        else:\n",
    "            log(f\"  ✗ Not statistically significant at 5% level\")\n",
    "    \n",
    "    if product_clicks_var in odds_ratios.index:\n",
    "        or_product = odds_ratios.loc[product_clicks_var, \"Odds Ratio\"]\n",
    "        p_product = odds_ratios.loc[product_clicks_var, \"P>|z|\"]\n",
    "        log(f\"\\n'{product_clicks_var}' (Product-specific clicks):\")\n",
    "        log(f\"  Odds Ratio = {or_product:.4f}, p-value = {p_product:.4f}\")\n",
    "        log(f\"  Interpretation: Each click on the specific product multiplies the odds of purchasing it by {or_product:.4f}\")\n",
    "        if p_product < 0.05:\n",
    "            log(f\"  ✓ Statistically significant at 5% level\")\n",
    "        else:\n",
    "            log(f\"  ✗ Not statistically significant at 5% level\")\n",
    "    \n",
    "    # Model fit statistics\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"MODEL FIT STATISTICS\")\n",
    "    log(\"=\"*40)\n",
    "    log(f\"  Pseudo R-squared: {logit_results.prsquared:.4f}\")\n",
    "    log(f\"  Log-Likelihood: {logit_results.llf:.4f}\")\n",
    "    log(f\"  AIC: {logit_results.aic:.4f}\")\n",
    "    log(f\"  BIC: {logit_results.bic:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"ERROR in logistic regression: {e}\")\n",
    "    import traceback\n",
    "    log(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:39] \n",
      "============================================================\n",
      "[2025-09-23 05:03:39] MODEL 2: OLS REGRESSION (WITH ROBUST SE)\n",
      "[2025-09-23 05:03:39] ============================================================\n",
      "[2025-09-23 05:03:39] \n",
      "OLS Regression Results (with Heteroskedasticity-Robust Standard Errors):\n",
      "[2025-09-23 05:03:39] ----------------------------------------\n",
      "[2025-09-23 05:03:39]                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            log_revenue   R-squared:                       0.006\n",
      "Model:                            OLS   Adj. R-squared:                  0.006\n",
      "Method:                 Least Squares   F-statistic:                     3.749\n",
      "Date:                Tue, 23 Sep 2025   Prob (F-statistic):           2.19e-05\n",
      "Time:                        05:03:39   Log-Likelihood:             4.4947e+05\n",
      "No. Observations:              269276   AIC:                        -8.989e+05\n",
      "Df Residuals:                  269264   BIC:                        -8.988e+05\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                0.0005      0.001      0.808      0.419      -0.001       0.002\n",
      "total_clicks          5.551e-07   1.96e-06      0.283      0.777   -3.29e-06     4.4e-06\n",
      "clicks_on_product        0.0152      0.003      5.657      0.000       0.010       0.020\n",
      "log_price               -0.0002   6.89e-05     -2.768      0.006      -0.000   -5.57e-05\n",
      "log_journey_duration     0.0003      0.000      2.069      0.039    1.36e-05       0.001\n",
      "distinct_products     -6.67e-07    2.9e-07     -2.301      0.021   -1.24e-06   -9.88e-08\n",
      "log_impressions         -0.0007      0.000     -1.848      0.065      -0.001     4.1e-05\n",
      "hist_purchase_count   1.441e-06   2.91e-06      0.495      0.621   -4.27e-06    7.15e-06\n",
      "hist_user_ctr           -0.0125      0.004     -3.514      0.000      -0.020      -0.006\n",
      "vendor_hist_ctr         -0.0010      0.001     -0.861      0.389      -0.003       0.001\n",
      "avg_winning_rank     -1.178e-05   1.13e-05     -1.043      0.297   -3.39e-05    1.03e-05\n",
      "product_win_rate         0.0015      0.000      3.494      0.000       0.001       0.002\n",
      "==============================================================================\n",
      "Omnibus:                   993073.882   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     495883906358.599\n",
      "Skew:                          80.089   Prob(JB):                         0.00\n",
      "Kurtosis:                    6649.159   Cond. No.                     1.45e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "[2] The condition number is large, 1.45e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] KEY COEFFICIENTS WITH ROBUST STANDARD ERRORS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39] \n",
      "                       Coefficient     Robust SE  t-statistic         P>|t|  CI Lower      CI Upper\n",
      "Intercept             5.304621e-04  6.562426e-04     0.808332  4.188994e-01 -0.000756  1.816674e-03\n",
      "total_clicks          5.551121e-07  1.963875e-06     0.282662  7.774363e-01 -0.000003  4.404236e-06\n",
      "clicks_on_product     1.516136e-02  2.679893e-03     5.657449  1.536393e-08  0.009909  2.041385e-02\n",
      "log_price            -1.907370e-04  6.890442e-05    -2.768139  5.637741e-03 -0.000326 -5.568683e-05\n",
      "log_journey_duration  2.582880e-04  1.248232e-04     2.069230  3.852448e-02  0.000014  5.029370e-04\n",
      "distinct_products    -6.670149e-07  2.898990e-07    -2.300853  2.139996e-02 -0.000001 -9.882328e-08\n",
      "log_impressions      -6.767107e-04  3.661849e-04    -1.848003  6.460191e-02 -0.001394  4.099843e-05\n",
      "hist_purchase_count   1.441214e-06  2.913305e-06     0.494701  6.208114e-01 -0.000004  7.151187e-06\n",
      "hist_user_ctr        -1.252501e-02  3.564558e-03    -3.513764  4.418057e-04 -0.019511 -5.538609e-03\n",
      "vendor_hist_ctr      -9.561404e-04  1.111077e-03    -0.860553  3.894844e-01 -0.003134  1.221530e-03\n",
      "avg_winning_rank     -1.178037e-05  1.129121e-05    -1.043323  2.967987e-01 -0.000034  1.034998e-05\n",
      "product_win_rate      1.470044e-03  4.206808e-04     3.494441  4.750564e-04  0.000646  2.294563e-03\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] KEY INTERPRETATIONS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39] \n",
      "'total_clicks' (Journey-level clicks):\n",
      "[2025-09-23 05:03:39]   Coefficient = 0.000001, p-value = 0.7774\n",
      "[2025-09-23 05:03:39]   Interpretation: Each additional journey click is associated with 0.000% change in revenue\n",
      "[2025-09-23 05:03:39]   ✗ Not statistically significant at 5% level\n",
      "[2025-09-23 05:03:39] \n",
      "'clicks_on_product' (Product-specific clicks):\n",
      "[2025-09-23 05:03:39]   Coefficient = 0.015161, p-value = 0.0000\n",
      "[2025-09-23 05:03:39]   Interpretation: Each click on the specific product is associated with 1.528% change in revenue\n",
      "[2025-09-23 05:03:39]   ✓ Statistically significant at 5% level (with robust SE)\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] MODEL FIT STATISTICS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39]   R-squared: 0.0056\n",
      "[2025-09-23 05:03:39]   Adjusted R-squared: 0.0055\n",
      "[2025-09-23 05:03:39]   F-statistic: 3.7492 (p=0.000022)\n",
      "[2025-09-23 05:03:39]   AIC: -898908.2903\n",
      "[2025-09-23 05:03:39]   BIC: -898782.2484\n",
      "[2025-09-23 05:03:39] \n",
      "========================================\n",
      "[2025-09-23 05:03:39] DIAGNOSTIC TESTS\n",
      "[2025-09-23 05:03:39] ========================================\n",
      "[2025-09-23 05:03:39]   Breusch-Pagan test for heteroskedasticity:\n",
      "[2025-09-23 05:03:39]     LM statistic = 1414.6947, p-value = 0.0000\n",
      "[2025-09-23 05:03:39]     ✓ Evidence of heteroskedasticity - robust SEs are appropriate\n"
     ]
    }
   ],
   "source": [
    "# OLS REGRESSION WITH ROBUST STANDARD ERRORS\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"MODEL 2: OLS REGRESSION (WITH ROBUST SE)\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Fit model using formula API with robust standard errors\n",
    "    ols_model = smf.ols(formula=ols_formula, data=df)\n",
    "    # Use HC3 robust standard errors (most conservative)\n",
    "    ols_results = ols_model.fit(cov_type='HC3')\n",
    "    \n",
    "    log(\"\\nOLS Regression Results (with Heteroskedasticity-Robust Standard Errors):\")\n",
    "    log(\"-\" * 40)\n",
    "    \n",
    "    # Print summary\n",
    "    log(str(ols_results.summary()))\n",
    "    \n",
    "    # Extract key coefficients\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"KEY COEFFICIENTS WITH ROBUST STANDARD ERRORS\")\n",
    "    log(\"=\"*40)\n",
    "    \n",
    "    coef_summary = pd.DataFrame({\n",
    "        \"Coefficient\": ols_results.params,\n",
    "        \"Robust SE\": ols_results.bse,\n",
    "        \"t-statistic\": ols_results.tvalues,\n",
    "        \"P>|t|\": ols_results.pvalues,\n",
    "        \"CI Lower\": ols_results.conf_int()[0],\n",
    "        \"CI Upper\": ols_results.conf_int()[1],\n",
    "    })\n",
    "    \n",
    "    log(\"\\n\" + coef_summary.to_string())\n",
    "    \n",
    "    # Key interpretations\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"KEY INTERPRETATIONS\")\n",
    "    log(\"=\"*40)\n",
    "    \n",
    "    if treatment_var in coef_summary.index:\n",
    "        coef_total = coef_summary.loc[treatment_var, \"Coefficient\"]\n",
    "        p_total = coef_summary.loc[treatment_var, \"P>|t|\"]\n",
    "        pct_change_total = (np.exp(coef_total) - 1) * 100\n",
    "        log(f\"\\n'{treatment_var}' (Journey-level clicks):\")\n",
    "        log(f\"  Coefficient = {coef_total:.6f}, p-value = {p_total:.4f}\")\n",
    "        log(f\"  Interpretation: Each additional journey click is associated with {pct_change_total:.3f}% change in revenue\")\n",
    "        if p_total < 0.05:\n",
    "            log(f\"  ✓ Statistically significant at 5% level (with robust SE)\")\n",
    "        else:\n",
    "            log(f\"  ✗ Not statistically significant at 5% level\")\n",
    "    \n",
    "    if product_clicks_var in coef_summary.index:\n",
    "        coef_product = coef_summary.loc[product_clicks_var, \"Coefficient\"]\n",
    "        p_product = coef_summary.loc[product_clicks_var, \"P>|t|\"]\n",
    "        pct_change_product = (np.exp(coef_product) - 1) * 100\n",
    "        log(f\"\\n'{product_clicks_var}' (Product-specific clicks):\")\n",
    "        log(f\"  Coefficient = {coef_product:.6f}, p-value = {p_product:.4f}\")\n",
    "        log(f\"  Interpretation: Each click on the specific product is associated with {pct_change_product:.3f}% change in revenue\")\n",
    "        if p_product < 0.05:\n",
    "            log(f\"  ✓ Statistically significant at 5% level (with robust SE)\")\n",
    "        else:\n",
    "            log(f\"  ✗ Not statistically significant at 5% level\")\n",
    "    \n",
    "    # Model fit statistics\n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"MODEL FIT STATISTICS\")\n",
    "    log(\"=\"*40)\n",
    "    log(f\"  R-squared: {ols_results.rsquared:.4f}\")\n",
    "    log(f\"  Adjusted R-squared: {ols_results.rsquared_adj:.4f}\")\n",
    "    log(f\"  F-statistic: {ols_results.fvalue:.4f} (p={ols_results.f_pvalue:.6f})\")\n",
    "    log(f\"  AIC: {ols_results.aic:.4f}\")\n",
    "    log(f\"  BIC: {ols_results.bic:.4f}\")\n",
    "    \n",
    "    # Heteroskedasticity test\n",
    "    residuals = ols_results.resid\n",
    "    exog = ols_model.exog\n",
    "    bp_test = het_breuschpagan(residuals, exog)\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*40)\n",
    "    log(\"DIAGNOSTIC TESTS\")\n",
    "    log(\"=\"*40)\n",
    "    log(f\"  Breusch-Pagan test for heteroskedasticity:\")\n",
    "    log(f\"    LM statistic = {bp_test[0]:.4f}, p-value = {bp_test[1]:.4f}\")\n",
    "    if bp_test[1] < 0.05:\n",
    "        log(f\"    ✓ Evidence of heteroskedasticity - robust SEs are appropriate\")\n",
    "    else:\n",
    "        log(f\"    ✗ No strong evidence of heteroskedasticity\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"ERROR in OLS regression: {e}\")\n",
    "    import traceback\n",
    "    log(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:39] \n",
      "============================================================\n",
      "[2025-09-23 05:03:39] ROBUSTNESS CHECKS\n",
      "[2025-09-23 05:03:39] ============================================================\n",
      "[2025-09-23 05:03:39] \n",
      "1. Simple Mean Comparison:\n",
      "[2025-09-23 05:03:39]   Treated (clicks>0) conversion: 0.0228%\n",
      "[2025-09-23 05:03:39]   Control (clicks=0) conversion: 0.0037%\n",
      "[2025-09-23 05:03:39]   Raw difference: 0.0191%\n",
      "[2025-09-23 05:03:39]   T-test: t=3.4722, p=0.0005\n",
      "[2025-09-23 05:03:39] \n",
      "2. Click Intensity Analysis:\n",
      "[2025-09-23 05:03:39]   Conversion by click bins:\n",
      "[2025-09-23 05:03:39]     [0.0, 1.0): 0.0037% (n=80,712.0)\n",
      "[2025-09-23 05:03:39]     [1.0, 2.0): 0.0172% (n=40,697.0)\n",
      "[2025-09-23 05:03:39]     [2.0, 5.0): 0.0214% (n=65,442.0)\n",
      "[2025-09-23 05:03:39]     [5.0, 10.0): 0.0231% (n=43,296.0)\n",
      "[2025-09-23 05:03:39]     [10.0, inf): 0.0307% (n=39,129.0)\n"
     ]
    }
   ],
   "source": [
    "# ROBUSTNESS CHECKS\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"ROBUSTNESS CHECKS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Simple mean comparison\n",
    "log(\"\\n1. Simple Mean Comparison:\")\n",
    "treated = df[df['total_clicks'] > 0]\n",
    "control = df[df['total_clicks'] == 0]\n",
    "\n",
    "treated_conv = treated['did_purchase_product'].mean()\n",
    "control_conv = control['did_purchase_product'].mean()\n",
    "\n",
    "log(f\"  Treated (clicks>0) conversion: {treated_conv:.4%}\")\n",
    "log(f\"  Control (clicks=0) conversion: {control_conv:.4%}\")\n",
    "log(f\"  Raw difference: {(treated_conv - control_conv):.4%}\")\n",
    "\n",
    "# T-test\n",
    "t_stat, p_value = stats.ttest_ind(treated['did_purchase_product'], control['did_purchase_product'])\n",
    "log(f\"  T-test: t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "\n",
    "# Click intensity analysis\n",
    "log(\"\\n2. Click Intensity Analysis:\")\n",
    "click_bins = [0, 1, 2, 5, 10, float('inf')]\n",
    "df['click_bin'] = pd.cut(df['total_clicks'], bins=click_bins, right=False)\n",
    "\n",
    "bin_conv = df.groupby('click_bin', observed=True)['did_purchase_product'].agg(['mean', 'count'])\n",
    "log(\"  Conversion by click bins:\")\n",
    "for idx, row in bin_conv.iterrows():\n",
    "    log(f\"    {idx}: {row['mean']:.4%} (n={row['count']:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 05:03:39] \n",
      "================================================================================\n",
      "[2025-09-23 05:03:39] ANALYSIS COMPLETE\n",
      "[2025-09-23 05:03:39] ================================================================================\n",
      "[2025-09-23 05:03:39] \n",
      "Results saved to: data/causal_analysis_results_20250923_050249.txt\n",
      "[2025-09-23 05:03:39] Total log entries: 216\n",
      "[2025-09-23 05:03:39] \n",
      "============================================================\n",
      "[2025-09-23 05:03:39] FINAL SUMMARY\n",
      "[2025-09-23 05:03:39] ============================================================\n",
      "[2025-09-23 05:03:39]   Total users analyzed: 1,124\n",
      "[2025-09-23 05:03:39]   Total journeys: 7,820\n",
      "[2025-09-23 05:03:39]   Total products: 215,589\n",
      "[2025-09-23 05:03:39]   Overall purchase rate: 0.0171%\n",
      "[2025-09-23 05:03:39]   CTR: 3.19%\n"
     ]
    }
   ],
   "source": [
    "# Save comprehensive results\n",
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"ANALYSIS COMPLETE\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "output_path = Path(\"./data\") / f\"causal_analysis_results_{timestamp}.txt\"\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('\\n'.join(output_log))\n",
    "\n",
    "log(f\"\\nResults saved to: {output_path}\")\n",
    "log(f\"Total log entries: {len(output_log)}\")\n",
    "\n",
    "# Summary statistics\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FINAL SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "log(f\"  Total users analyzed: {df['USER_ID'].nunique():,}\")\n",
    "log(f\"  Total journeys: {df['journey_id'].nunique():,}\")\n",
    "log(f\"  Total products: {df['PRODUCT_ID'].nunique():,}\")\n",
    "log(f\"  Overall purchase rate: {df['did_purchase_product'].mean():.4%}\")\n",
    "log(f\"  CTR: {(df['clicks_on_product'].sum() / df['impressions_on_product'].sum()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

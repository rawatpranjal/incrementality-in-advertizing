{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: IMPORTS + CONNECTION (run once, handles duo)\n",
    "import os\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "OUTPUT_DIR = Path(\"../0_data/round3_p1\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY',\n",
    "    schema='INCREMENTALITY_RESEARCH'\n",
    ")\n",
    "print(\"[SUCCESS] Connected to Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: DATA PULL (Round 3 P1 - Placement 1 focus, 1 week, 0.1% sample)\n# CONFIG\nMINUTES_WINDOW = 10080  # 1 week (7 days * 24 hours * 60 minutes)\nSAMPLE_FRACTION = 0.001  # 0.1% of users\nPLACEMENT_FILTER = '1'  # Placement 1 (search page) only\nTOTAL_BUCKETS = 10000\nSELECTION_THRESHOLD = int(TOTAL_BUCKETS * SAMPLE_FRACTION)\n\nprint(f\"[Round 3 P1] Pulling {MINUTES_WINDOW}min ({MINUTES_WINDOW/60/24:.0f} days), PLACEMENT={PLACEMENT_FILTER}, {SAMPLE_FRACTION:.1%} users...\")\nprint(f\"Expected yield: ~4M auction_results rows (~500MB-600MB total)\")\n\n# CTE for deterministic user sampling - ONLY users who have P1 auctions\nCTE_SQL = f\"\"\"\nWITH SAMPLED_USERS AS (\n    SELECT OPAQUE_USER_ID FROM (\n        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), {TOTAL_BUCKETS}) AS bucket\n        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n              WHERE CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n              AND PLACEMENT = '{PLACEMENT_FILTER}')\n    ) WHERE bucket < {SELECTION_THRESHOLD}\n)\n\"\"\"\n\n# 1. AUCTIONS_USERS (P1 only)\nprint(\"\\n1/6 AUCTIONS_USERS (P1 only)...\")\nauctions_users = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n       au.OPAQUE_USER_ID AS USER_ID, au.PLACEMENT, au.CREATED_AT\nFROM AUCTIONS_USERS au\nJOIN SAMPLED_USERS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID\nWHERE au.CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\nAND au.PLACEMENT = '{PLACEMENT_FILTER}'\n\"\"\", conn)\nprint(f\"  {len(auctions_users):,} rows, {auctions_users['USER_ID'].nunique():,} users\")\nprint(f\"  Placements: {auctions_users['PLACEMENT'].value_counts().to_dict()}\")\n\n# 2. AUCTIONS_RESULTS (P1 only)\nprint(\"\\n2/6 AUCTIONS_RESULTS (P1 only)...\")\nauctions_results = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n       LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n       LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n       LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n       ar.RANKING, ar.IS_WINNER, ar.FINAL_BID, ar.QUALITY,\n       ar.CONVERSION_RATE, ar.PACING, ar.PRICE, ar.CREATED_AT\nFROM AUCTIONS_RESULTS ar\nJOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\nJOIN SAMPLED_USERS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID\nWHERE ar.CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\nAND au.PLACEMENT = '{PLACEMENT_FILTER}'\n\"\"\", conn)\nprint(f\"  {len(auctions_results):,} rows\")\n\n# 3. IMPRESSIONS (all for sampled users - not filtered by placement since impressions don't have placement)\nprint(\"\\n3/6 IMPRESSIONS...\")\nimpressions = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT i.INTERACTION_ID, LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n       LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID, i.USER_ID,\n       LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n       LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID, i.OCCURRED_AT\nFROM IMPRESSIONS i\nJOIN SAMPLED_USERS s ON i.USER_ID = s.OPAQUE_USER_ID\nWHERE i.OCCURRED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(impressions):,} rows\")\n\n# 4. CLICKS (all for sampled users)\nprint(\"\\n4/6 CLICKS...\")\nclicks = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT c.INTERACTION_ID, LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n       LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID, c.USER_ID,\n       LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n       LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID, c.OCCURRED_AT\nFROM CLICKS c\nJOIN SAMPLED_USERS s ON c.USER_ID = s.OPAQUE_USER_ID\nWHERE c.OCCURRED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(clicks):,} rows\")\n\n# 5. PURCHASES (all for sampled users)\nprint(\"\\n5/6 PURCHASES...\")\npurchases = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT p.PURCHASE_ID,\n       LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n       p.USER_ID,\n       p.PURCHASED_AT,\n       p.QUANTITY,\n       p.UNIT_PRICE,\n       p.PURCHASE_LINE\nFROM PURCHASES p\nJOIN SAMPLED_USERS s ON p.USER_ID = s.OPAQUE_USER_ID\nWHERE p.PURCHASED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(purchases):,} rows, {purchases['USER_ID'].nunique():,} users\")\nprint(f\"  Total revenue: ${(purchases['QUANTITY'] * purchases['UNIT_PRICE']).sum()/100:,.2f}\")\n\n# 6. CATALOG (only products that received impressions)\nprint(\"\\n6/6 CATALOG...\")\nproduct_ids = impressions['PRODUCT_ID'].dropna().unique().tolist()\nprint(f\"  Products to fetch: {len(product_ids):,}\")\nif len(product_ids) > 0:\n    batch_size = 10000\n    catalog_dfs = []\n    for i in tqdm(range(0, len(product_ids), batch_size), desc=\"Catalog\"):\n        batch = product_ids[i:i+batch_size]\n        placeholders = ', '.join(['%s'] * len(batch))\n        batch_df = pd.read_sql(f\"\"\"\n        SELECT LOWER(TRIM(PRODUCT_ID)) AS PRODUCT_ID, NAME, PRICE AS CATALOG_PRICE,\n               ACTIVE, IS_DELETED, CATEGORIES, DESCRIPTION\n        FROM CATALOG WHERE LOWER(TRIM(PRODUCT_ID)) IN ({placeholders})\n        \"\"\", conn, params=batch)\n        catalog_dfs.append(batch_df)\n    catalog = pd.concat(catalog_dfs, ignore_index=True) if catalog_dfs else pd.DataFrame()\nelse:\n    catalog = pd.DataFrame()\nprint(f\"  {len(catalog):,} rows\")\n\n# SAVE\nprint(\"\\nSaving parquet files...\")\nauctions_results.to_parquet(OUTPUT_DIR / \"auctions_results_r3_p1.parquet\", index=False)\nauctions_users.to_parquet(OUTPUT_DIR / \"auctions_users_r3_p1.parquet\", index=False)\nimpressions.to_parquet(OUTPUT_DIR / \"impressions_r3_p1.parquet\", index=False)\nclicks.to_parquet(OUTPUT_DIR / \"clicks_r3_p1.parquet\", index=False)\npurchases.to_parquet(OUTPUT_DIR / \"purchases_r3_p1.parquet\", index=False)\ncatalog.to_parquet(OUTPUT_DIR / \"catalog_r3_p1.parquet\", index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"DONE - Round 3 P1 (Placement 1 Focus)\")\nprint(\"=\"*50)\nprint(f\"Output directory: {OUTPUT_DIR.resolve()}\")\nprint(f\"auctions_users:   {len(auctions_users):,} ({auctions_users['USER_ID'].nunique():,} users)\")\nprint(f\"auctions_results: {len(auctions_results):,}\")\nprint(f\"impressions:      {len(impressions):,}\")\nprint(f\"clicks:           {len(clicks):,}\")\nprint(f\"purchases:        {len(purchases):,}\")\nprint(f\"catalog:          {len(catalog):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: VERIFICATION CHECKS\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# V1: Verify all auctions have PLACEMENT = '1'\n",
    "print(\"\\n--- V1: Placement verification ---\")\n",
    "placement_counts = auctions_users['PLACEMENT'].value_counts()\n",
    "print(f\"Placement distribution: {placement_counts.to_dict()}\")\n",
    "if len(placement_counts) == 1 and placement_counts.index[0] == '1':\n",
    "    print(\"[PASS] All auctions are Placement 1\")\n",
    "else:\n",
    "    print(\"[FAIL] Non-P1 auctions found!\")\n",
    "\n",
    "# V2: Verify date range spans ~7 days\n",
    "print(\"\\n--- V2: Date range verification ---\")\n",
    "min_date = auctions_users['CREATED_AT'].min()\n",
    "max_date = auctions_users['CREATED_AT'].max()\n",
    "date_range_days = (max_date - min_date).total_seconds() / 86400\n",
    "print(f\"Min date: {min_date}\")\n",
    "print(f\"Max date: {max_date}\")\n",
    "print(f\"Date range: {date_range_days:.2f} days\")\n",
    "if date_range_days >= 6.5:\n",
    "    print(\"[PASS] Date range spans approximately 7 days\")\n",
    "else:\n",
    "    print(f\"[WARN] Date range is {date_range_days:.2f} days (expected ~7)\")\n",
    "\n",
    "# V3: Row counts match expectations\n",
    "print(\"\\n--- V3: Row count summary ---\")\n",
    "print(f\"auctions_users:   {len(auctions_users):,}\")\n",
    "print(f\"auctions_results: {len(auctions_results):,}\")\n",
    "print(f\"impressions:      {len(impressions):,}\")\n",
    "print(f\"clicks:           {len(clicks):,}\")\n",
    "print(f\"purchases:        {len(purchases):,}\")\n",
    "print(f\"catalog:          {len(catalog):,}\")\n",
    "\n",
    "# V4: User sampling determinism check (hash bucket distribution)\n",
    "print(\"\\n--- V4: User sampling determinism ---\")\n",
    "unique_users = auctions_users['USER_ID'].unique()\n",
    "print(f\"Unique users in sample: {len(unique_users):,}\")\n",
    "print(f\"Sample fraction: {SAMPLE_FRACTION:.2%}\")\n",
    "\n",
    "# V5: File sizes\n",
    "print(\"\\n--- V5: File sizes ---\")\n",
    "for f in OUTPUT_DIR.glob(\"*.parquet\"):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "total_size = sum(f.stat().st_size for f in OUTPUT_DIR.glob(\"*.parquet\")) / (1024 * 1024 * 1024)\n",
    "print(f\"\\nTotal size: {total_size:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: DATA QUALITY CHECKS (adapted from 02_data_pull.ipynb)\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Q1: Are impression timestamps unique within auction, or batched?\n",
    "print(\"\\n--- Q1: Impression timestamp uniqueness within auction ---\")\n",
    "imp_ts = impressions.groupby('AUCTION_ID')['OCCURRED_AT'].agg(['nunique', 'count'])\n",
    "print(f\"Auctions with >1 impression: {(imp_ts['count'] > 1).sum():,}\")\n",
    "print(f\"Among those, unique timestamps: {imp_ts[imp_ts['count'] > 1]['nunique'].describe().to_dict()}\")\n",
    "print(f\"Batched (all same timestamp): {(imp_ts['nunique'] == 1).sum():,} / {len(imp_ts):,}\")\n",
    "\n",
    "# Q2: Distribution of positions per auction\n",
    "print(\"\\n--- Q2: Positions per auction ---\")\n",
    "positions_per_auction = impressions.groupby('AUCTION_ID')['PRODUCT_ID'].nunique()\n",
    "print(positions_per_auction.describe())\n",
    "\n",
    "# Q3: Maximum rank that receives impression\n",
    "print(\"\\n--- Q3: Max rank receiving impression ---\")\n",
    "imp_with_rank = impressions.merge(\n",
    "    auctions_results[['AUCTION_ID', 'PRODUCT_ID', 'RANKING']],\n",
    "    on=['AUCTION_ID', 'PRODUCT_ID'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"Impressions with ranking: {imp_with_rank['RANKING'].notna().sum():,} / {len(imp_with_rank):,}\")\n",
    "print(f\"Ranking distribution for impressions:\")\n",
    "print(imp_with_rank['RANKING'].describe())\n",
    "print(f\"\\nMax rank shown: {imp_with_rank['RANKING'].max()}\")\n",
    "\n",
    "# Q4: Products appearing at multiple positions\n",
    "print(\"\\n--- Q4: Product position variation across auctions ---\")\n",
    "prod_pos = auctions_results.groupby('PRODUCT_ID')['RANKING'].agg(['mean', 'std', 'count', 'nunique'])\n",
    "prod_pos = prod_pos[prod_pos['count'] >= 5]  # Products with 5+ appearances\n",
    "print(f\"Products with 5+ auctions: {len(prod_pos):,}\")\n",
    "print(f\"Avg unique positions per product: {prod_pos['nunique'].mean():.1f}\")\n",
    "print(f\"Products with position variation (nunique > 1): {(prod_pos['nunique'] > 1).sum():,}\")\n",
    "\n",
    "# Q5: User auction frequency and position variation\n",
    "print(\"\\n--- Q5: User-level auction frequency ---\")\n",
    "user_auctions = auctions_users.groupby('USER_ID').size()\n",
    "print(f\"Auctions per user:\")\n",
    "print(user_auctions.describe())\n",
    "print(f\"\\nUsers with 10+ auctions: {(user_auctions >= 10).sum():,}\")\n",
    "\n",
    "# Q6: Time between auctions for same user (session definition)\n",
    "print(\"\\n--- Q6: Time between auctions (session gaps) ---\")\n",
    "auctions_users_sorted = auctions_users.sort_values(['USER_ID', 'CREATED_AT'])\n",
    "auctions_users_sorted['time_gap'] = auctions_users_sorted.groupby('USER_ID')['CREATED_AT'].diff()\n",
    "gaps = auctions_users_sorted['time_gap'].dropna().dt.total_seconds()\n",
    "print(f\"Gap between auctions (seconds):\")\n",
    "print(gaps.describe())\n",
    "print(f\"\\nMedian gap: {gaps.median():.1f}s = {gaps.median()/60:.1f}min\")\n",
    "print(f\"75th percentile: {gaps.quantile(0.75):.1f}s = {gaps.quantile(0.75)/60:.1f}min\")\n",
    "\n",
    "# Additional: Purchase linkage check\n",
    "print(\"\\n--- Purchase-to-impression linkage ---\")\n",
    "purchase_products = set(purchases['PRODUCT_ID'].unique())\n",
    "impression_products = set(impressions['PRODUCT_ID'].unique())\n",
    "overlap = purchase_products & impression_products\n",
    "print(f\"Unique products in purchases: {len(purchase_products):,}\")\n",
    "print(f\"Unique products in impressions: {len(impression_products):,}\")\n",
    "print(f\"Overlap (promoted purchases): {len(overlap):,}\")\n",
    "print(f\"Organic-only purchases: {len(purchase_products - impression_products):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
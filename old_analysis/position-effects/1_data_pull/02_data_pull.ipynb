{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# CELL 1: IMPORTS + CONNECTION (run once, handles duo)\nimport os\nimport textwrap\nfrom pathlib import Path\nimport warnings\nimport pandas as pd\nfrom dotenv import load_dotenv\nimport snowflake.connector\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\nload_dotenv()\n\nOUTPUT_DIR = Path(\"./data_r3\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nconn = snowflake.connector.connect(\n    user=os.getenv('SNOWFLAKE_USER'),\n    password=os.getenv('SNOWFLAKE_PASSWORD'),\n    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n    database='INCREMENTALITY',\n    schema='INCREMENTALITY_RESEARCH'\n)\nprint(\"[SUCCESS] Connected to Snowflake\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: DATA PULL (Round 3 - expanded parameters)\n# CONFIG\nMINUTES_WINDOW = 60  # 60 minutes (was 15 in R1)\nSAMPLE_FRACTION = 0.10  # 10% of users (expanded from 1%)\nTOTAL_BUCKETS = 10000\nSELECTION_THRESHOLD = int(TOTAL_BUCKETS * SAMPLE_FRACTION)\n\nprint(f\"[Round 3] Pulling {MINUTES_WINDOW}min, ALL placements, {SAMPLE_FRACTION:.0%} users...\")\nprint(f\"Expected yield: ~800K auctions, ~70K users, ~240K clicks, ~7.7M impressions\")\n\n# CTE for deterministic user sampling (all placements)\nCTE_SQL = f\"\"\"\nWITH SAMPLED_USERS AS (\n    SELECT OPAQUE_USER_ID FROM (\n        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), {TOTAL_BUCKETS}) AS bucket\n        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n              WHERE CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP()))\n    ) WHERE bucket < {SELECTION_THRESHOLD}\n)\n\"\"\"\n\n# 1. AUCTIONS_USERS\nprint(\"\\n1/6 AUCTIONS_USERS...\")\nauctions_users = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n       au.OPAQUE_USER_ID AS USER_ID, au.PLACEMENT, au.CREATED_AT\nFROM AUCTIONS_USERS au\nJOIN SAMPLED_USERS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID\nWHERE au.CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(auctions_users):,} rows, {auctions_users['USER_ID'].nunique():,} users\")\nprint(f\"  Placements: {auctions_users['PLACEMENT'].value_counts().to_dict()}\")\n\n# 2. AUCTIONS_RESULTS\nprint(\"\\n2/6 AUCTIONS_RESULTS...\")\nauctions_results = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n       LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n       LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n       LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n       ar.RANKING, ar.IS_WINNER, ar.FINAL_BID, ar.QUALITY,\n       ar.CONVERSION_RATE, ar.PACING, ar.PRICE, ar.CREATED_AT\nFROM AUCTIONS_RESULTS ar\nJOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\nJOIN SAMPLED_USERS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID\nWHERE ar.CREATED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(auctions_results):,} rows\")\n\n# 3. IMPRESSIONS\nprint(\"\\n3/6 IMPRESSIONS...\")\nimpressions = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT i.INTERACTION_ID, LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n       LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID, i.USER_ID,\n       LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n       LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID, i.OCCURRED_AT\nFROM IMPRESSIONS i\nJOIN SAMPLED_USERS s ON i.USER_ID = s.OPAQUE_USER_ID\nWHERE i.OCCURRED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(impressions):,} rows\")\n\n# 4. CLICKS\nprint(\"\\n4/6 CLICKS...\")\nclicks = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT c.INTERACTION_ID, LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n       LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID, c.USER_ID,\n       LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n       LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID, c.OCCURRED_AT\nFROM CLICKS c\nJOIN SAMPLED_USERS s ON c.USER_ID = s.OPAQUE_USER_ID\nWHERE c.OCCURRED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(clicks):,} rows\")\n\n# 5. PURCHASES (new in R2)\nprint(\"\\n5/6 PURCHASES...\")\npurchases = pd.read_sql(CTE_SQL + f\"\"\"\nSELECT p.PURCHASE_ID,\n       LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n       p.USER_ID,\n       p.PURCHASED_AT,\n       p.QUANTITY,\n       p.UNIT_PRICE,\n       p.PURCHASE_LINE\nFROM PURCHASES p\nJOIN SAMPLED_USERS s ON p.USER_ID = s.OPAQUE_USER_ID\nWHERE p.PURCHASED_AT >= DATEADD(minute, -{MINUTES_WINDOW}, CURRENT_TIMESTAMP())\n\"\"\", conn)\nprint(f\"  {len(purchases):,} rows, {purchases['USER_ID'].nunique():,} users\")\nprint(f\"  Total revenue: ${(purchases['QUANTITY'] * purchases['UNIT_PRICE']).sum():,.0f}\")\n\n# 6. CATALOG (only products that received impressions)\nprint(\"\\n6/6 CATALOG...\")\nproduct_ids = impressions['PRODUCT_ID'].dropna().unique().tolist()\nprint(f\"  Products to fetch: {len(product_ids):,}\")\nif len(product_ids) > 0:\n    batch_size = 10000\n    catalog_dfs = []\n    for i in tqdm(range(0, len(product_ids), batch_size), desc=\"Catalog\"):\n        batch = product_ids[i:i+batch_size]\n        placeholders = ', '.join(['%s'] * len(batch))\n        batch_df = pd.read_sql(f\"\"\"\n        SELECT LOWER(TRIM(PRODUCT_ID)) AS PRODUCT_ID, NAME, PRICE AS CATALOG_PRICE,\n               ACTIVE, IS_DELETED, CATEGORIES, DESCRIPTION\n        FROM CATALOG WHERE LOWER(TRIM(PRODUCT_ID)) IN ({placeholders})\n        \"\"\", conn, params=batch)\n        catalog_dfs.append(batch_df)\n    catalog = pd.concat(catalog_dfs, ignore_index=True) if catalog_dfs else pd.DataFrame()\nelse:\n    catalog = pd.DataFrame()\nprint(f\"  {len(catalog):,} rows\")\n\n# SAVE\nprint(\"\\nSaving parquet files...\")\nauctions_results.to_parquet(OUTPUT_DIR / \"auctions_results_r3.parquet\", index=False)\nauctions_users.to_parquet(OUTPUT_DIR / \"auctions_users_r3.parquet\", index=False)\nimpressions.to_parquet(OUTPUT_DIR / \"impressions_r3.parquet\", index=False)\nclicks.to_parquet(OUTPUT_DIR / \"clicks_r3.parquet\", index=False)\npurchases.to_parquet(OUTPUT_DIR / \"purchases_r3.parquet\", index=False)\ncatalog.to_parquet(OUTPUT_DIR / \"catalog_r3.parquet\", index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"DONE - Round 3\")\nprint(\"=\"*50)\nprint(f\"auctions_users:   {len(auctions_users):,} ({auctions_users['USER_ID'].nunique():,} users)\")\nprint(f\"auctions_results: {len(auctions_results):,}\")\nprint(f\"impressions:      {len(impressions):,}\")\nprint(f\"clicks:           {len(clicks):,}\")\nprint(f\"purchases:        {len(purchases):,}\")\nprint(f\"catalog:          {len(catalog):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: DATA QUALITY CHECKS (from notes.md Q1-Q6)\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Q1: Are impression timestamps unique within auction, or batched?\n",
    "print(\"\\n--- Q1: Impression timestamp uniqueness within auction ---\")\n",
    "imp_ts = impressions.groupby('AUCTION_ID')['OCCURRED_AT'].agg(['nunique', 'count'])\n",
    "print(f\"Auctions with >1 impression: {(imp_ts['count'] > 1).sum():,}\")\n",
    "print(f\"Among those, unique timestamps: {imp_ts[imp_ts['count'] > 1]['nunique'].describe().to_dict()}\")\n",
    "print(f\"Batched (all same timestamp): {(imp_ts['nunique'] == 1).sum():,} / {len(imp_ts):,}\")\n",
    "\n",
    "# Q2: Distribution of positions per auction\n",
    "print(\"\\n--- Q2: Positions per auction ---\")\n",
    "positions_per_auction = impressions.groupby('AUCTION_ID')['PRODUCT_ID'].nunique()\n",
    "print(positions_per_auction.describe())\n",
    "\n",
    "# Q3: Maximum rank that receives impression\n",
    "print(\"\\n--- Q3: Max rank receiving impression ---\")\n",
    "# Join impressions to auctions_results to get ranking\n",
    "imp_with_rank = impressions.merge(\n",
    "    auctions_results[['AUCTION_ID', 'PRODUCT_ID', 'RANKING']],\n",
    "    on=['AUCTION_ID', 'PRODUCT_ID'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"Impressions with ranking: {imp_with_rank['RANKING'].notna().sum():,} / {len(imp_with_rank):,}\")\n",
    "print(f\"Ranking distribution for impressions:\")\n",
    "print(imp_with_rank['RANKING'].describe())\n",
    "print(f\"\\nMax rank shown: {imp_with_rank['RANKING'].max()}\")\n",
    "\n",
    "# Q4: Products appearing at multiple positions\n",
    "print(\"\\n--- Q4: Product position variation across auctions ---\")\n",
    "prod_pos = auctions_results.groupby('PRODUCT_ID')['RANKING'].agg(['mean', 'std', 'count', 'nunique'])\n",
    "prod_pos = prod_pos[prod_pos['count'] >= 5]  # Products with 5+ appearances\n",
    "print(f\"Products with 5+ auctions: {len(prod_pos):,}\")\n",
    "print(f\"Avg unique positions per product: {prod_pos['nunique'].mean():.1f}\")\n",
    "print(f\"Products with position variation (nunique > 1): {(prod_pos['nunique'] > 1).sum():,}\")\n",
    "\n",
    "# Q5: User auction frequency and position variation\n",
    "print(\"\\n--- Q5: User-level auction frequency ---\")\n",
    "user_auctions = auctions_users.groupby('USER_ID').size()\n",
    "print(f\"Auctions per user:\")\n",
    "print(user_auctions.describe())\n",
    "print(f\"\\nUsers with 10+ auctions: {(user_auctions >= 10).sum():,}\")\n",
    "\n",
    "# Q6: Time between auctions for same user (session definition)\n",
    "print(\"\\n--- Q6: Time between auctions (session gaps) ---\")\n",
    "auctions_users_sorted = auctions_users.sort_values(['USER_ID', 'CREATED_AT'])\n",
    "auctions_users_sorted['time_gap'] = auctions_users_sorted.groupby('USER_ID')['CREATED_AT'].diff()\n",
    "gaps = auctions_users_sorted['time_gap'].dropna().dt.total_seconds()\n",
    "print(f\"Gap between auctions (seconds):\")\n",
    "print(gaps.describe())\n",
    "print(f\"\\nMedian gap: {gaps.median():.1f}s = {gaps.median()/60:.1f}min\")\n",
    "print(f\"75th percentile: {gaps.quantile(0.75):.1f}s = {gaps.quantile(0.75)/60:.1f}min\")\n",
    "\n",
    "# Additional: Purchase linkage check\n",
    "print(\"\\n--- Purchase-to-impression linkage ---\")\n",
    "purchase_products = set(purchases['PRODUCT_ID'].unique())\n",
    "impression_products = set(impressions['PRODUCT_ID'].unique())\n",
    "overlap = purchase_products & impression_products\n",
    "print(f\"Unique products in purchases: {len(purchase_products):,}\")\n",
    "print(f\"Unique products in impressions: {len(impression_products):,}\")\n",
    "print(f\"Overlap (promoted purchases): {len(overlap):,}\")\n",
    "print(f\"Organic-only purchases: {len(purchase_products - impression_products):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
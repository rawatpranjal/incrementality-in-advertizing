{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Canonical Tables\n",
    "\n",
    "**Purpose:** Build auditable canonical tables for panel construction.\n",
    "\n",
    "**Outputs:**\n",
    "1. `PROMOTED_EVENTS` - One row per promoted click with full auction metadata\n",
    "2. `PURCHASES_MAPPED` - Purchases with vendor attribution (promoted-linked only)\n",
    "3. Session IDs with multiple gap thresholds (1/2/3/5/7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../eda/data')\n",
    "OUTPUT_DIR = Path('data')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Input: {DATA_DIR.resolve()}\")\n",
    "print(f\"Output: {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Source Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading source tables...\")\n",
    "\n",
    "clicks = pd.read_parquet(DATA_DIR / 'clicks_365d.parquet')\n",
    "impressions = pd.read_parquet(DATA_DIR / 'impressions_365d.parquet')\n",
    "bids = pd.read_parquet(DATA_DIR / 'auctions_results_365d.parquet')\n",
    "auctions = pd.read_parquet(DATA_DIR / 'auctions_users_365d.parquet')\n",
    "purchases = pd.read_parquet(DATA_DIR / 'purchases_365d.parquet')\n",
    "\n",
    "print(f\"Clicks: {len(clicks):,}\")\n",
    "print(f\"Impressions: {len(impressions):,}\")\n",
    "print(f\"Bids: {len(bids):,}\")\n",
    "print(f\"Auctions: {len(auctions):,}\")\n",
    "print(f\"Purchases: {len(purchases):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse timestamps\n",
    "print(\"Parsing timestamps...\")\n",
    "clicks['click_time'] = pd.to_datetime(clicks['OCCURRED_AT'])\n",
    "impressions['impression_time'] = pd.to_datetime(impressions['OCCURRED_AT'])\n",
    "bids['bid_time'] = pd.to_datetime(bids['CREATED_AT'])\n",
    "auctions['auction_time'] = pd.to_datetime(auctions['CREATED_AT'])\n",
    "purchases['purchase_time'] = pd.to_datetime(purchases['PURCHASED_AT'])\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build PROMOTED_EVENTS Table\n",
    "\n",
    "Join chain: CLICKS → IMPRESSIONS → BIDS → AUCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BUILDING PROMOTED_EVENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define composite join keys\n",
    "COMPOSITE_KEYS = ['AUCTION_ID', 'PRODUCT_ID', 'VENDOR_ID', 'CAMPAIGN_ID']\n",
    "\n",
    "# Start with clicks\n",
    "promoted_events = clicks[['INTERACTION_ID', 'AUCTION_ID', 'PRODUCT_ID', 'USER_ID', \n",
    "                          'VENDOR_ID', 'CAMPAIGN_ID', 'click_time']].copy()\n",
    "promoted_events.columns = ['click_id', 'auction_id', 'product_id', 'user_id', \n",
    "                           'vendor_id', 'campaign_id', 'click_time']\n",
    "\n",
    "print(f\"Starting clicks: {len(promoted_events):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to impressions to get impression_time\n",
    "print(\"\\nJoining to impressions...\")\n",
    "\n",
    "impressions_slim = impressions[['AUCTION_ID', 'PRODUCT_ID', 'VENDOR_ID', 'CAMPAIGN_ID', 'impression_time']].copy()\n",
    "impressions_slim.columns = ['auction_id', 'product_id', 'vendor_id', 'campaign_id', 'impression_time']\n",
    "\n",
    "# Dedupe impressions (take earliest per composite key)\n",
    "impressions_slim = impressions_slim.sort_values('impression_time').drop_duplicates(\n",
    "    subset=['auction_id', 'product_id', 'vendor_id', 'campaign_id'], keep='first'\n",
    ")\n",
    "\n",
    "promoted_events = promoted_events.merge(\n",
    "    impressions_slim,\n",
    "    on=['auction_id', 'product_id', 'vendor_id', 'campaign_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "imp_match_rate = promoted_events['impression_time'].notna().mean() * 100\n",
    "print(f\"Clicks with impression match: {imp_match_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to bids to get auction metadata\n",
    "print(\"\\nJoining to bids...\")\n",
    "\n",
    "bid_cols = ['AUCTION_ID', 'PRODUCT_ID', 'VENDOR_ID', 'CAMPAIGN_ID', \n",
    "            'RANKING', 'IS_WINNER', 'FINAL_BID', 'QUALITY', 'PACING', \n",
    "            'CONVERSION_RATE', 'PRICE']\n",
    "bid_cols_available = [c for c in bid_cols if c in bids.columns]\n",
    "\n",
    "bids_slim = bids[bid_cols_available].copy()\n",
    "bids_slim.columns = [c.lower() for c in bids_slim.columns]\n",
    "\n",
    "# Dedupe bids (take winner or highest rank)\n",
    "if 'is_winner' in bids_slim.columns:\n",
    "    bids_slim = bids_slim.sort_values(['is_winner', 'ranking'], ascending=[False, True])\n",
    "bids_slim = bids_slim.drop_duplicates(\n",
    "    subset=['auction_id', 'product_id', 'vendor_id', 'campaign_id'], keep='first'\n",
    ")\n",
    "\n",
    "promoted_events = promoted_events.merge(\n",
    "    bids_slim,\n",
    "    on=['auction_id', 'product_id', 'vendor_id', 'campaign_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "bid_match_rate = promoted_events['ranking'].notna().mean() * 100\n",
    "print(f\"Clicks with bid match: {bid_match_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to auctions to get placement\n",
    "print(\"\\nJoining to auctions...\")\n",
    "\n",
    "auctions_slim = auctions[['AUCTION_ID', 'OPAQUE_USER_ID', 'PLACEMENT']].copy() if 'PLACEMENT' in auctions.columns else auctions[['AUCTION_ID', 'OPAQUE_USER_ID']].copy()\n",
    "auctions_slim.columns = ['auction_id', 'opaque_user_id'] + (['placement'] if 'PLACEMENT' in auctions.columns else [])\n",
    "auctions_slim = auctions_slim.drop_duplicates(subset=['auction_id'], keep='first')\n",
    "\n",
    "promoted_events = promoted_events.merge(\n",
    "    auctions_slim,\n",
    "    on=['auction_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "auction_match_rate = promoted_events['opaque_user_id'].notna().mean() * 100\n",
    "print(f\"Clicks with auction match: {auction_match_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final PROMOTED_EVENTS summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROMOTED_EVENTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Rows: {len(promoted_events):,}\")\n",
    "print(f\"Columns: {list(promoted_events.columns)}\")\n",
    "print(f\"\\nUnique users: {promoted_events['user_id'].nunique():,}\")\n",
    "print(f\"Unique vendors: {promoted_events['vendor_id'].nunique():,}\")\n",
    "print(f\"Unique products: {promoted_events['product_id'].nunique():,}\")\n",
    "print(f\"Date range: {promoted_events['click_time'].min()} to {promoted_events['click_time'].max()}\")\n",
    "\n",
    "# Save\n",
    "promoted_events.to_parquet(OUTPUT_DIR / 'promoted_events.parquet', index=False)\n",
    "print(f\"\\nSaved to {OUTPUT_DIR / 'promoted_events.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build PURCHASES_MAPPED Table\n",
    "\n",
    "Map purchases to vendors via promoted journey linkage only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BUILDING PURCHASES_MAPPED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Start with purchases\n",
    "purchases_mapped = purchases[['PURCHASE_ID', 'USER_ID', 'PRODUCT_ID', 'purchase_time', \n",
    "                               'QUANTITY', 'UNIT_PRICE', 'PURCHASE_LINE']].copy()\n",
    "purchases_mapped.columns = ['purchase_id', 'user_id', 'product_id', 'purchase_time',\n",
    "                            'quantity', 'unit_price', 'purchase_line']\n",
    "\n",
    "# Calculate spend (cents to dollars)\n",
    "purchases_mapped['spend'] = purchases_mapped['quantity'] * purchases_mapped['unit_price'] / 100\n",
    "\n",
    "print(f\"Total purchases: {len(purchases_mapped):,}\")\n",
    "print(f\"Total spend: ${purchases_mapped['spend'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get promoted click info for each (user, product)\n",
    "# Take earliest click time for attribution\n",
    "click_info = promoted_events.groupby(['user_id', 'product_id']).agg({\n",
    "    'click_time': 'min',\n",
    "    'vendor_id': 'first',\n",
    "    'campaign_id': 'first',\n",
    "    'ranking': 'first',\n",
    "    'is_winner': 'first'\n",
    "}).reset_index()\n",
    "click_info.columns = ['user_id', 'product_id', 'first_click_time', \n",
    "                      'click_vendor_id', 'click_campaign_id', \n",
    "                      'click_ranking', 'click_is_winner']\n",
    "\n",
    "print(f\"\\nUnique (user, product) pairs with clicks: {len(click_info):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join purchases to click info\n",
    "purchases_mapped = purchases_mapped.merge(\n",
    "    click_info,\n",
    "    on=['user_id', 'product_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Flag promoted-linked purchases\n",
    "purchases_mapped['is_promoted_linked'] = purchases_mapped['first_click_time'].notna()\n",
    "\n",
    "# Calculate click-to-purchase lag\n",
    "purchases_mapped['click_to_purchase_hours'] = np.where(\n",
    "    purchases_mapped['is_promoted_linked'],\n",
    "    (purchases_mapped['purchase_time'] - purchases_mapped['first_click_time']).dt.total_seconds() / 3600,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Flag purchases after click (valid attribution)\n",
    "purchases_mapped['is_post_click'] = (\n",
    "    purchases_mapped['is_promoted_linked'] & \n",
    "    (purchases_mapped['click_to_purchase_hours'] >= 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n--- Mapping Summary ---\")\n",
    "print(f\"Total purchases: {len(purchases_mapped):,}\")\n",
    "print(f\"Promoted-linked: {purchases_mapped['is_promoted_linked'].sum():,} ({purchases_mapped['is_promoted_linked'].mean()*100:.1f}%)\")\n",
    "print(f\"Post-click (valid): {purchases_mapped['is_post_click'].sum():,} ({purchases_mapped['is_post_click'].mean()*100:.1f}%)\")\n",
    "\n",
    "total_spend = purchases_mapped['spend'].sum()\n",
    "linked_spend = purchases_mapped.loc[purchases_mapped['is_promoted_linked'], 'spend'].sum()\n",
    "valid_spend = purchases_mapped.loc[purchases_mapped['is_post_click'], 'spend'].sum()\n",
    "\n",
    "print(f\"\\n--- Spend Coverage ---\")\n",
    "print(f\"Total spend: ${total_spend:,.2f}\")\n",
    "print(f\"Promoted-linked spend: ${linked_spend:,.2f} ({linked_spend/total_spend*100:.1f}%)\")\n",
    "print(f\"Valid post-click spend: ${valid_spend:,.2f} ({valid_spend/total_spend*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag distribution for valid purchases\n",
    "valid_purchases = purchases_mapped[purchases_mapped['is_post_click']].copy()\n",
    "\n",
    "print(\"\\n--- Click-to-Purchase Lag (valid only) ---\")\n",
    "print(valid_purchases['click_to_purchase_hours'].describe())\n",
    "\n",
    "print(\"\\nPercentiles (hours):\")\n",
    "for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "    val = valid_purchases['click_to_purchase_hours'].quantile(p/100)\n",
    "    print(f\"  P{p}: {val:.1f}h ({val/24:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "purchases_mapped.to_parquet(OUTPUT_DIR / 'purchases_mapped.parquet', index=False)\n",
    "print(f\"\\nSaved to {OUTPUT_DIR / 'purchases_mapped.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Session IDs with Multiple Gap Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BUILDING SESSION IDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all user events for sessionization\n",
    "# Events: clicks, impressions, purchases\n",
    "\n",
    "events_list = []\n",
    "\n",
    "# Clicks\n",
    "click_events = promoted_events[['user_id', 'click_time', 'vendor_id', 'product_id']].copy()\n",
    "click_events['event_type'] = 'click'\n",
    "click_events.columns = ['user_id', 'timestamp', 'vendor_id', 'product_id', 'event_type']\n",
    "events_list.append(click_events)\n",
    "\n",
    "# Purchases (only post-click valid ones)\n",
    "purchase_events = purchases_mapped[purchases_mapped['is_post_click']][['user_id', 'purchase_time', 'click_vendor_id', 'product_id', 'spend']].copy()\n",
    "purchase_events['event_type'] = 'purchase'\n",
    "purchase_events.columns = ['user_id', 'timestamp', 'vendor_id', 'product_id', 'spend', 'event_type']\n",
    "events_list.append(purchase_events)\n",
    "\n",
    "# Combine\n",
    "all_events = pd.concat(events_list, ignore_index=True)\n",
    "all_events = all_events.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total events: {len(all_events):,}\")\n",
    "print(f\"Unique users: {all_events['user_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session IDs for multiple gap thresholds\n",
    "SESSION_GAPS = [1, 2, 3, 5, 7]  # days\n",
    "\n",
    "print(\"\\nCreating session IDs for multiple gap thresholds...\")\n",
    "\n",
    "# Calculate time since last event per user\n",
    "all_events['time_since_last'] = all_events.groupby('user_id')['timestamp'].diff()\n",
    "\n",
    "for gap_days in tqdm(SESSION_GAPS, desc=\"Session gaps\"):\n",
    "    gap_threshold = pd.Timedelta(days=gap_days)\n",
    "    col_name = f'session_id_{gap_days}d'\n",
    "    \n",
    "    # New session starts when gap exceeds threshold or first event\n",
    "    all_events['new_session'] = (\n",
    "        (all_events['time_since_last'] > gap_threshold) | \n",
    "        (all_events['time_since_last'].isnull())\n",
    "    )\n",
    "    \n",
    "    # Assign session numbers within user\n",
    "    all_events['session_num'] = all_events.groupby('user_id')['new_session'].cumsum()\n",
    "    \n",
    "    # Create session ID\n",
    "    all_events[col_name] = all_events['user_id'].astype(str) + '_S' + all_events['session_num'].astype(str)\n",
    "\n",
    "# Clean up temp columns\n",
    "all_events = all_events.drop(columns=['time_since_last', 'new_session', 'session_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session counts by threshold\n",
    "print(\"\\n--- Session Counts by Gap Threshold ---\")\n",
    "for gap_days in SESSION_GAPS:\n",
    "    col = f'session_id_{gap_days}d'\n",
    "    n_sessions = all_events[col].nunique()\n",
    "    avg_per_user = n_sessions / all_events['user_id'].nunique()\n",
    "    print(f\"  {gap_days}-day gap: {n_sessions:,} sessions ({avg_per_user:.1f} per user)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add week index\n",
    "all_events['week'] = all_events['timestamp'].dt.isocalendar().week\n",
    "all_events['year'] = all_events['timestamp'].dt.year\n",
    "all_events['year_week'] = all_events['year'].astype(str) + '_W' + all_events['week'].astype(str).str.zfill(2)\n",
    "\n",
    "print(f\"\\nWeeks in data: {all_events['year_week'].nunique()}\")\n",
    "print(f\"Range: {all_events['year_week'].min()} to {all_events['year_week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save events with session IDs\n",
    "all_events.to_parquet(OUTPUT_DIR / 'events_with_sessions.parquet', index=False)\n",
    "print(f\"\\nSaved to {OUTPUT_DIR / 'events_with_sessions.parquet'}\")\n",
    "\n",
    "print(f\"\\nColumns: {list(all_events.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CANONICAL TABLES SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. PROMOTED_EVENTS\")\n",
    "print(f\"   Rows: {len(promoted_events):,}\")\n",
    "print(f\"   Users: {promoted_events['user_id'].nunique():,}\")\n",
    "print(f\"   Vendors: {promoted_events['vendor_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\n2. PURCHASES_MAPPED\")\n",
    "print(f\"   Total purchases: {len(purchases_mapped):,}\")\n",
    "print(f\"   Valid (post-click): {purchases_mapped['is_post_click'].sum():,}\")\n",
    "print(f\"   Valid spend: ${purchases_mapped.loc[purchases_mapped['is_post_click'], 'spend'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\n3. EVENTS_WITH_SESSIONS\")\n",
    "print(f\"   Events: {len(all_events):,}\")\n",
    "print(f\"   Users: {all_events['user_id'].nunique():,}\")\n",
    "print(f\"   Weeks: {all_events['year_week'].nunique()}\")\n",
    "for gap in SESSION_GAPS:\n",
    "    print(f\"   Sessions ({gap}d gap): {all_events[f'session_id_{gap}d'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CANONICAL TABLES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nOutput files in data/:\")\n",
    "for f in OUTPUT_DIR.glob('*.parquet'):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nReady for 03_panel_construction.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

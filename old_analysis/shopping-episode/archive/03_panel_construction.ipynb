{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Panel Construction\n",
    "\n",
    "**Purpose:** Build estimation panels for regression analysis.\n",
    "\n",
    "**Panels:**\n",
    "1. **Panel A: (u, t, v)** - User × Week × Vendor\n",
    "2. **Panel B: (s, t, v)** - Session-Week × Vendor (for each gap threshold)\n",
    "\n",
    "**Variables:**\n",
    "- `C` = sponsored click count\n",
    "- `Y` = spend (promoted-linked only)\n",
    "- `I` = impression count\n",
    "- Controls: avg_rank, share_rank1, avg_pacing, avg_quality, avg_final_bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Canonical Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading canonical tables...\")\n",
    "\n",
    "promoted_events = pd.read_parquet(DATA_DIR / 'promoted_events.parquet')\n",
    "purchases_mapped = pd.read_parquet(DATA_DIR / 'purchases_mapped.parquet')\n",
    "events_with_sessions = pd.read_parquet(DATA_DIR / 'events_with_sessions.parquet')\n",
    "\n",
    "print(f\"Promoted events: {len(promoted_events):,}\")\n",
    "print(f\"Purchases mapped: {len(purchases_mapped):,}\")\n",
    "print(f\"Events with sessions: {len(events_with_sessions):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add week info to promoted_events\n",
    "promoted_events['week'] = pd.to_datetime(promoted_events['click_time']).dt.isocalendar().week\n",
    "promoted_events['year'] = pd.to_datetime(promoted_events['click_time']).dt.year\n",
    "promoted_events['year_week'] = promoted_events['year'].astype(str) + '_W' + promoted_events['week'].astype(str).str.zfill(2)\n",
    "\n",
    "# Add week info to purchases\n",
    "purchases_valid = purchases_mapped[purchases_mapped['is_post_click']].copy()\n",
    "purchases_valid['week'] = pd.to_datetime(purchases_valid['purchase_time']).dt.isocalendar().week\n",
    "purchases_valid['year'] = pd.to_datetime(purchases_valid['purchase_time']).dt.year\n",
    "purchases_valid['year_week'] = purchases_valid['year'].astype(str) + '_W' + purchases_valid['week'].astype(str).str.zfill(2)\n",
    "\n",
    "print(f\"Valid purchases: {len(purchases_valid):,}\")\n",
    "print(f\"Total valid spend: ${purchases_valid['spend'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Panel A: User × Week × Vendor (u, t, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PANEL A: USER × WEEK × VENDOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aggregate clicks to (user, week, vendor)\n",
    "clicks_utv = promoted_events.groupby(['user_id', 'year_week', 'vendor_id']).agg({\n",
    "    'click_id': 'count',\n",
    "    'ranking': ['mean', 'min'],\n",
    "    'is_winner': 'mean',\n",
    "    'final_bid': 'mean',\n",
    "    'quality': 'mean',\n",
    "    'pacing': 'mean',\n",
    "    'conversion_rate': 'mean',\n",
    "    'price': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "clicks_utv.columns = ['user_id', 'year_week', 'vendor_id', \n",
    "                      'C', 'avg_rank', 'min_rank', 'share_winner',\n",
    "                      'avg_final_bid', 'avg_quality', 'avg_pacing',\n",
    "                      'avg_conversion_rate', 'avg_price']\n",
    "\n",
    "# Share rank=1\n",
    "rank1_counts = promoted_events[promoted_events['ranking'] == 1].groupby(\n",
    "    ['user_id', 'year_week', 'vendor_id']\n",
    ").size().reset_index(name='rank1_clicks')\n",
    "\n",
    "clicks_utv = clicks_utv.merge(rank1_counts, on=['user_id', 'year_week', 'vendor_id'], how='left')\n",
    "clicks_utv['rank1_clicks'] = clicks_utv['rank1_clicks'].fillna(0)\n",
    "clicks_utv['share_rank1'] = clicks_utv['rank1_clicks'] / clicks_utv['C']\n",
    "\n",
    "print(f\"Click aggregates: {len(clicks_utv):,} (u,t,v) observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate spend to (user, week, vendor)\n",
    "spend_utv = purchases_valid.groupby(['user_id', 'year_week', 'click_vendor_id']).agg({\n",
    "    'spend': 'sum',\n",
    "    'purchase_id': 'count'\n",
    "}).reset_index()\n",
    "spend_utv.columns = ['user_id', 'year_week', 'vendor_id', 'Y', 'n_purchases']\n",
    "\n",
    "print(f\"Spend aggregates: {len(spend_utv):,} (u,t,v) observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clicks and spend\n",
    "panel_utv = clicks_utv.merge(\n",
    "    spend_utv,\n",
    "    on=['user_id', 'year_week', 'vendor_id'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "panel_utv['C'] = panel_utv['C'].fillna(0).astype(int)\n",
    "panel_utv['Y'] = panel_utv['Y'].fillna(0)\n",
    "panel_utv['n_purchases'] = panel_utv['n_purchases'].fillna(0).astype(int)\n",
    "\n",
    "# Binary conversion indicator\n",
    "panel_utv['D'] = (panel_utv['Y'] > 0).astype(int)\n",
    "\n",
    "# Log spend\n",
    "panel_utv['log_Y'] = np.log1p(panel_utv['Y'])\n",
    "\n",
    "print(f\"\\nPanel A dimensions: {len(panel_utv):,} observations\")\n",
    "print(f\"Unique users: {panel_utv['user_id'].nunique():,}\")\n",
    "print(f\"Unique weeks: {panel_utv['year_week'].nunique()}\")\n",
    "print(f\"Unique vendors: {panel_utv['vendor_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n--- Panel A Summary Statistics ---\")\n",
    "print(f\"\\nClick distribution (C):\")\n",
    "print(panel_utv['C'].describe())\n",
    "print(f\"Zero clicks: {(panel_utv['C'] == 0).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nSpend distribution (Y):\")\n",
    "print(panel_utv['Y'].describe())\n",
    "print(f\"Zero spend: {(panel_utv['Y'] == 0).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nConversion rate: {panel_utv['D'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Panel A\n",
    "panel_utv.to_parquet(DATA_DIR / 'panel_utv.parquet', index=False)\n",
    "print(f\"\\nSaved Panel A to {DATA_DIR / 'panel_utv.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Panel B: Session-Week × Vendor (s, t, v)\n",
    "\n",
    "Build for each session gap threshold (1, 2, 3, 5, 7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PANEL B: SESSION-WEEK × VENDOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SESSION_GAPS = [1, 2, 3, 5, 7]\n",
    "panels_stv = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gap_days in tqdm(SESSION_GAPS, desc=\"Building session panels\"):\n",
    "    session_col = f'session_id_{gap_days}d'\n",
    "    \n",
    "    # Filter to clicks only (for C)\n",
    "    clicks_events = events_with_sessions[events_with_sessions['event_type'] == 'click'].copy()\n",
    "    \n",
    "    # Aggregate clicks to (session, week, vendor)\n",
    "    clicks_stv = clicks_events.groupby([session_col, 'year_week', 'vendor_id']).agg({\n",
    "        'user_id': 'first',  # session belongs to one user\n",
    "        'product_id': 'count'  # count as clicks\n",
    "    }).reset_index()\n",
    "    clicks_stv.columns = ['session_id', 'year_week', 'vendor_id', 'user_id', 'C']\n",
    "    \n",
    "    # Filter to purchases (for Y)\n",
    "    purchase_events = events_with_sessions[events_with_sessions['event_type'] == 'purchase'].copy()\n",
    "    \n",
    "    # Aggregate spend to (session, week, vendor)\n",
    "    spend_stv = purchase_events.groupby([session_col, 'year_week', 'vendor_id']).agg({\n",
    "        'spend': 'sum'\n",
    "    }).reset_index()\n",
    "    spend_stv.columns = ['session_id', 'year_week', 'vendor_id', 'Y']\n",
    "    \n",
    "    # Merge\n",
    "    panel_stv = clicks_stv.merge(\n",
    "        spend_stv,\n",
    "        on=['session_id', 'year_week', 'vendor_id'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Fill missing\n",
    "    panel_stv['C'] = panel_stv['C'].fillna(0).astype(int)\n",
    "    panel_stv['Y'] = panel_stv['Y'].fillna(0)\n",
    "    panel_stv['D'] = (panel_stv['Y'] > 0).astype(int)\n",
    "    panel_stv['log_Y'] = np.log1p(panel_stv['Y'])\n",
    "    \n",
    "    # Fill user_id for spend-only rows\n",
    "    if panel_stv['user_id'].isna().any():\n",
    "        # Extract user from session_id (format: user_id_SX)\n",
    "        panel_stv['user_id'] = panel_stv['user_id'].fillna(\n",
    "            panel_stv['session_id'].str.rsplit('_S', n=1).str[0]\n",
    "        )\n",
    "    \n",
    "    # Store\n",
    "    panels_stv[gap_days] = panel_stv\n",
    "    \n",
    "    print(f\"\\n{gap_days}-day gap panel: {len(panel_stv):,} (s,t,v) observations\")\n",
    "    print(f\"  Sessions: {panel_stv['session_id'].nunique():,}\")\n",
    "    print(f\"  Users: {panel_stv['user_id'].nunique():,}\")\n",
    "    print(f\"  Zero spend: {(panel_stv['Y'] == 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all session panels\n",
    "for gap_days, panel in panels_stv.items():\n",
    "    filename = f'panel_stv_{gap_days}d.parquet'\n",
    "    panel.to_parquet(DATA_DIR / filename, index=False)\n",
    "    print(f\"Saved {filename}: {len(panel):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Impressions (Optional Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ADDING IMPRESSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load impressions\n",
    "impressions = pd.read_parquet(Path('../eda/data/impressions_365d.parquet'))\n",
    "impressions['impression_time'] = pd.to_datetime(impressions['OCCURRED_AT'])\n",
    "impressions['week'] = impressions['impression_time'].dt.isocalendar().week\n",
    "impressions['year'] = impressions['impression_time'].dt.year\n",
    "impressions['year_week'] = impressions['year'].astype(str) + '_W' + impressions['week'].astype(str).str.zfill(2)\n",
    "\n",
    "print(f\"Total impressions: {len(impressions):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate impressions to (user, week, vendor)\n",
    "impressions_utv = impressions.groupby(['USER_ID', 'year_week', 'VENDOR_ID']).size().reset_index(name='I')\n",
    "impressions_utv.columns = ['user_id', 'year_week', 'vendor_id', 'I']\n",
    "\n",
    "print(f\"Impression aggregates: {len(impressions_utv):,} (u,t,v) observations\")\n",
    "\n",
    "# Merge to Panel A\n",
    "panel_utv = pd.read_parquet(DATA_DIR / 'panel_utv.parquet')\n",
    "panel_utv = panel_utv.merge(\n",
    "    impressions_utv,\n",
    "    on=['user_id', 'year_week', 'vendor_id'],\n",
    "    how='left'\n",
    ")\n",
    "panel_utv['I'] = panel_utv['I'].fillna(0).astype(int)\n",
    "\n",
    "# Save updated Panel A\n",
    "panel_utv.to_parquet(DATA_DIR / 'panel_utv.parquet', index=False)\n",
    "print(f\"Updated Panel A with impressions: {len(panel_utv):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Fixed Effect Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING FIXED EFFECT INDICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Panel A\n",
    "panel_utv = pd.read_parquet(DATA_DIR / 'panel_utv.parquet')\n",
    "\n",
    "# Create categorical indices for fixed effects\n",
    "panel_utv['user_fe'] = pd.Categorical(panel_utv['user_id']).codes\n",
    "panel_utv['week_fe'] = pd.Categorical(panel_utv['year_week']).codes\n",
    "panel_utv['vendor_fe'] = pd.Categorical(panel_utv['vendor_id']).codes\n",
    "\n",
    "print(f\"User FE levels: {panel_utv['user_fe'].nunique()}\")\n",
    "print(f\"Week FE levels: {panel_utv['week_fe'].nunique()}\")\n",
    "print(f\"Vendor FE levels: {panel_utv['vendor_fe'].nunique()}\")\n",
    "\n",
    "panel_utv.to_parquet(DATA_DIR / 'panel_utv.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel B (for each gap)\n",
    "for gap_days in SESSION_GAPS:\n",
    "    filename = f'panel_stv_{gap_days}d.parquet'\n",
    "    panel = pd.read_parquet(DATA_DIR / filename)\n",
    "    \n",
    "    panel['session_fe'] = pd.Categorical(panel['session_id']).codes\n",
    "    panel['week_fe'] = pd.Categorical(panel['year_week']).codes\n",
    "    panel['vendor_fe'] = pd.Categorical(panel['vendor_id']).codes\n",
    "    panel['user_fe'] = pd.Categorical(panel['user_id']).codes\n",
    "    \n",
    "    panel.to_parquet(DATA_DIR / filename, index=False)\n",
    "    print(f\"{filename}: session_fe={panel['session_fe'].nunique()}, week_fe={panel['week_fe'].nunique()}, vendor_fe={panel['vendor_fe'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Panel Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PANEL CONSTRUCTION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Panel A: (u, t, v) ---\")\n",
    "panel_utv = pd.read_parquet(DATA_DIR / 'panel_utv.parquet')\n",
    "print(f\"Observations: {len(panel_utv):,}\")\n",
    "print(f\"Columns: {list(panel_utv.columns)}\")\n",
    "print(f\"C range: [{panel_utv['C'].min()}, {panel_utv['C'].max()}], mean={panel_utv['C'].mean():.2f}\")\n",
    "print(f\"Y range: [${panel_utv['Y'].min():.2f}, ${panel_utv['Y'].max():.2f}], mean=${panel_utv['Y'].mean():.2f}\")\n",
    "print(f\"I range: [{panel_utv['I'].min()}, {panel_utv['I'].max()}], mean={panel_utv['I'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n--- Panel B: (s, t, v) by gap threshold ---\")\n",
    "for gap_days in SESSION_GAPS:\n",
    "    panel = pd.read_parquet(DATA_DIR / f'panel_stv_{gap_days}d.parquet')\n",
    "    print(f\"\\n{gap_days}-day gap:\")\n",
    "    print(f\"  Observations: {len(panel):,}\")\n",
    "    print(f\"  Sessions: {panel['session_id'].nunique():,}\")\n",
    "    print(f\"  Conversion rate: {panel['D'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "print(\"\\n--- Output Files ---\")\n",
    "for f in sorted(DATA_DIR.glob('panel_*.parquet')):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nReady for 04_main_regressions.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_data_processing.ipynb\n",
    "## Session-Based Data Processing for Causal Transformer\n",
    "\n",
    "This notebook transforms the raw unified event stream into model-ready sessions.\n",
    "\n",
    "### Workflow:\n",
    "1. Load the unified event stream from checkpoint\n",
    "2. Sessionize events based on inactivity threshold (24 hours)\n",
    "3. Create vocabulary mappings for events and products\n",
    "4. Transform sessions into (X, T, Y) format for causal modeling\n",
    "5. Split data into train/validation/test sets at user level\n",
    "6. Save processed datasets for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Session gap: 24 hours\n",
      "  Min session length: 2 events\n",
      "  Max session length: 500 events\n",
      "  Data split: 70% train / 15% val / 15% test\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Session configuration\n",
    "SESSION_GAP_HOURS = 24  # Hours of inactivity that define a new session\n",
    "MIN_SESSION_LENGTH = 2  # Minimum number of events in a valid session\n",
    "MAX_SESSION_LENGTH = 500  # Maximum number of events to keep per session\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = '[PAD]'\n",
    "AUCTION_TOKEN = '[AUCTION]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "# Data split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Session gap: {SESSION_GAP_HOURS} hours\")\n",
    "print(f\"  Min session length: {MIN_SESSION_LENGTH} events\")\n",
    "print(f\"  Max session length: {MAX_SESSION_LENGTH} events\")\n",
    "print(f\"  Data split: {TRAIN_RATIO:.0%} train / {VAL_RATIO:.0%} val / {TEST_RATIO:.0%} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/unified_events.parquet\n",
      "Loaded 5,408 events\n",
      "Unique users: 24\n",
      "Date range: 2025-09-05 00:05:17.255000 to 2025-09-06 23:59:37\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD DATA ---\n",
    "\n",
    "# Use fixed filename (no timestamp)\n",
    "data_dir = Path('./data')\n",
    "unified_events_file = data_dir / 'unified_events.parquet'\n",
    "\n",
    "if not unified_events_file.exists():\n",
    "    raise FileNotFoundError(\"No unified_events.parquet file found. Please run 01_data_pull.ipynb first.\")\n",
    "\n",
    "print(f\"Loading data from: {unified_events_file}\")\n",
    "\n",
    "df_events = pd.read_parquet(unified_events_file)\n",
    "print(f\"Loaded {len(df_events):,} events\")\n",
    "print(f\"Unique users: {df_events['USER_ID'].nunique():,}\")\n",
    "print(f\"Date range: {df_events['event_timestamp'].min()} to {df_events['event_timestamp'].max()}\")\n",
    "\n",
    "# Ensure timestamp is datetime\n",
    "df_events['event_timestamp'] = pd.to_datetime(df_events['event_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessionization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize_user_events(user_df: pd.DataFrame, gap_hours: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a user's event stream into sessions based on inactivity gaps.\n",
    "    \n",
    "    Args:\n",
    "        user_df: DataFrame of events for a single user, sorted by timestamp\n",
    "        gap_hours: Hours of inactivity that define a new session\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with session_id column added\n",
    "    \"\"\"\n",
    "    user_df = user_df.sort_values('event_timestamp').copy()\n",
    "    \n",
    "    # Calculate time gaps between consecutive events\n",
    "    user_df['time_gap'] = user_df['event_timestamp'].diff()\n",
    "    \n",
    "    # Mark session boundaries where gap exceeds threshold\n",
    "    session_gap = pd.Timedelta(hours=gap_hours)\n",
    "    user_df['new_session'] = (user_df['time_gap'] > session_gap) | user_df['time_gap'].isna()\n",
    "    \n",
    "    # Assign session IDs using cumsum\n",
    "    user_df['session_id'] = user_df['new_session'].cumsum()\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    user_df = user_df.drop(['time_gap', 'new_session'], axis=1)\n",
    "    \n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sessionizing user events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|██████████| 24/24 [00:00<00:00, 841.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sessions created: 24\n",
      "Valid sessions (>= 2 events): 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform sessionization for all users\n",
    "print(\"\\nSessionizing user events...\")\n",
    "\n",
    "# Apply sessionization per user\n",
    "sessionized_dfs = []\n",
    "\n",
    "for user_id in tqdm(df_events['USER_ID'].unique(), desc=\"Processing users\"):\n",
    "    user_events = df_events[df_events['USER_ID'] == user_id]\n",
    "    user_sessions = sessionize_user_events(user_events, SESSION_GAP_HOURS)\n",
    "    # Create global session ID by combining user_id and session_id\n",
    "    user_sessions['global_session_id'] = user_id + '_' + user_sessions['session_id'].astype(str)\n",
    "    sessionized_dfs.append(user_sessions)\n",
    "\n",
    "df_sessionized = pd.concat(sessionized_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal sessions created: {df_sessionized['global_session_id'].nunique():,}\")\n",
    "\n",
    "# Filter out sessions that are too short\n",
    "session_lengths = df_sessionized.groupby('global_session_id').size()\n",
    "valid_sessions = session_lengths[session_lengths >= MIN_SESSION_LENGTH].index\n",
    "df_sessionized = df_sessionized[df_sessionized['global_session_id'].isin(valid_sessions)]\n",
    "\n",
    "print(f\"Valid sessions (>= {MIN_SESSION_LENGTH} events): {df_sessionized['global_session_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabularies(df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Create vocabulary mappings for event types and product IDs.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing event and item vocabularies\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating vocabularies...\")\n",
    "    \n",
    "    # Event type vocabulary (fixed ordering)\n",
    "    event_types = ['auction', 'impression', 'click', 'purchase']\n",
    "    event_to_int = {event: i for i, event in enumerate(event_types)}\n",
    "    int_to_event = {i: event for event, i in event_to_int.items()}\n",
    "    \n",
    "    print(f\"Event types: {len(event_to_int)}\")\n",
    "    for event, idx in event_to_int.items():\n",
    "        print(f\"  {event}: {idx}\")\n",
    "    \n",
    "    # Product/Item vocabulary\n",
    "    # Reserve special indices\n",
    "    item_to_int = {\n",
    "        PAD_TOKEN: 0,\n",
    "        AUCTION_TOKEN: 1,\n",
    "        UNK_TOKEN: 2\n",
    "    }\n",
    "    \n",
    "    # Get all unique product IDs\n",
    "    unique_products = df['product_id'].dropna().unique()\n",
    "    print(f\"\\nUnique products: {len(unique_products):,}\")\n",
    "    \n",
    "    # Assign indices to products\n",
    "    for i, product in enumerate(sorted(unique_products), start=3):\n",
    "        item_to_int[product] = i\n",
    "    \n",
    "    int_to_item = {i: item for item, i in item_to_int.items()}\n",
    "    \n",
    "    print(f\"Total vocabulary size: {len(item_to_int):,}\")\n",
    "    print(f\"  Special tokens: {PAD_TOKEN}(0), {AUCTION_TOKEN}(1), {UNK_TOKEN}(2)\")\n",
    "    print(f\"  Product IDs: 3-{len(item_to_int)-1}\")\n",
    "    \n",
    "    vocab = {\n",
    "        'event_to_int': event_to_int,\n",
    "        'int_to_event': int_to_event,\n",
    "        'item_to_int': item_to_int,\n",
    "        'int_to_item': int_to_item,\n",
    "        'special_tokens': {\n",
    "            'pad': PAD_TOKEN,\n",
    "            'auction': AUCTION_TOKEN,\n",
    "            'unk': UNK_TOKEN\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating vocabularies...\n",
      "Event types: 4\n",
      "  auction: 0\n",
      "  impression: 1\n",
      "  click: 2\n",
      "  purchase: 3\n",
      "\n",
      "Unique products: 2,952\n",
      "Total vocabulary size: 2,955\n",
      "  Special tokens: [PAD](0), [AUCTION](1), [UNK](2)\n",
      "  Product IDs: 3-2954\n",
      "\n",
      "Vocabulary saved to models/vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Create vocabularies\n",
    "vocab = create_vocabularies(df_sessionized)\n",
    "\n",
    "# Save vocabularies\n",
    "models_dir = Path('./models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "vocab_path = models_dir / 'vocab.json'\n",
    "with open(vocab_path, 'w') as f:\n",
    "    # Convert int keys to strings for JSON serialization\n",
    "    vocab_serializable = {\n",
    "        'event_to_int': vocab['event_to_int'],\n",
    "        'int_to_event': {str(k): v for k, v in vocab['int_to_event'].items()},\n",
    "        'item_to_int': {str(k): v for k, v in vocab['item_to_int'].items()},\n",
    "        'int_to_item': {str(k): v for k, v in vocab['int_to_item'].items()},\n",
    "        'special_tokens': vocab['special_tokens']\n",
    "    }\n",
    "    json.dump(vocab_serializable, f, indent=2)\n",
    "print(f\"\\nVocabulary saved to {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Sessions to Model Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_session_to_sequence(\n",
    "    session_df: pd.DataFrame,\n",
    "    vocab: Dict[str, Dict]\n",
    ") -> Tuple[List[Tuple[int, int, float]], int, int]:\n",
    "    \"\"\"\n",
    "    Transform a session DataFrame into the (X, T, Y) format for modeling.\n",
    "    \n",
    "    CRITICAL: X must only contain pre-treatment events to avoid conditioning on bad controls!\n",
    "    \n",
    "    Args:\n",
    "        session_df: DataFrame of events in a single session\n",
    "        vocab: Vocabulary mappings\n",
    "    \n",
    "    Returns:\n",
    "        X: List of (event_type_id, item_id, timedelta_minutes) tuples (PRE-TREATMENT ONLY)\n",
    "        T: Binary treatment indicator (1 if any click in session)\n",
    "        Y: Binary outcome indicator (1 if any purchase in session)\n",
    "    \"\"\"\n",
    "    session_df = session_df.sort_values('event_timestamp').copy()\n",
    "    \n",
    "    # Get first timestamp as reference\n",
    "    first_timestamp = session_df['event_timestamp'].iloc[0]\n",
    "    \n",
    "    # Find the first click event (if any)\n",
    "    click_mask = session_df['event_type'] == 'click'\n",
    "    has_click = click_mask.any()\n",
    "    \n",
    "    # Determine treatment\n",
    "    T = 1 if has_click else 0\n",
    "    \n",
    "    # Determine outcome (ANY purchase in the full session)\n",
    "    has_purchase = (session_df['event_type'] == 'purchase').any()\n",
    "    Y = 1 if has_purchase else 0\n",
    "    \n",
    "    # CRITICAL: Select events for X based on treatment status\n",
    "    if has_click:\n",
    "        # Find index of first click\n",
    "        first_click_idx = click_mask.idxmax()  # Gets index of first True value\n",
    "        first_click_position = session_df.index.get_loc(first_click_idx)\n",
    "        \n",
    "        # Use only events BEFORE the first click for X\n",
    "        pre_treatment_df = session_df.iloc[:first_click_position]\n",
    "        \n",
    "        # If no events before click, use a minimal sequence\n",
    "        if len(pre_treatment_df) == 0:\n",
    "            # At least include the auction that led to the click\n",
    "            pre_treatment_df = session_df.iloc[:1]\n",
    "    else:\n",
    "        # No click - use all events for X\n",
    "        pre_treatment_df = session_df\n",
    "    \n",
    "    # Build sequence X from pre-treatment events only\n",
    "    X = []\n",
    "    for _, row in pre_treatment_df.iterrows():\n",
    "        # Get event type ID\n",
    "        event_type = row['event_type']\n",
    "        event_type_id = vocab['event_to_int'][event_type]\n",
    "        \n",
    "        # Get item ID\n",
    "        if event_type == 'auction':\n",
    "            item_id = vocab['item_to_int'][AUCTION_TOKEN]\n",
    "        elif pd.isna(row['product_id']):\n",
    "            item_id = vocab['item_to_int'][UNK_TOKEN]\n",
    "        else:\n",
    "            product_id = row['product_id']\n",
    "            item_id = vocab['item_to_int'].get(product_id, vocab['item_to_int'][UNK_TOKEN])\n",
    "        \n",
    "        # Calculate time delta in minutes\n",
    "        timedelta_minutes = (row['event_timestamp'] - first_timestamp).total_seconds() / 60.0\n",
    "        \n",
    "        # Append to sequence\n",
    "        X.append((event_type_id, item_id, timedelta_minutes))\n",
    "    \n",
    "    # Ensure minimum sequence length\n",
    "    if len(X) < MIN_SESSION_LENGTH:\n",
    "        # Pad with auction events if too short\n",
    "        while len(X) < MIN_SESSION_LENGTH:\n",
    "            X.append((vocab['event_to_int']['auction'], vocab['item_to_int'][AUCTION_TOKEN], 0.0))\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(X) > MAX_SESSION_LENGTH:\n",
    "        X = X[:MAX_SESSION_LENGTH]\n",
    "    \n",
    "    return X, T, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming sessions to model format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████| 22/22 [00:00<00:00, 670.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sessions: 22\n",
      "Sessions with treatment (click): 16 (72.7%)\n",
      "Sessions with outcome (purchase): 16 (72.7%)\n",
      "\n",
      "Sequence length statistics:\n",
      "  Mean: 46.7\n",
      "  Median: 26.0\n",
      "  Min: 2\n",
      "  Max: 250\n",
      "  95th percentile: 111.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform all sessions\n",
    "print(\"\\nTransforming sessions to model format...\")\n",
    "\n",
    "session_data = []\n",
    "\n",
    "for session_id in tqdm(df_sessionized['global_session_id'].unique(), desc=\"Processing sessions\"):\n",
    "    session_df = df_sessionized[df_sessionized['global_session_id'] == session_id]\n",
    "    \n",
    "    X, T, Y = transform_session_to_sequence(session_df, vocab)\n",
    "    \n",
    "    # Extract user_id from global_session_id\n",
    "    user_id = session_id.rsplit('_', 1)[0]\n",
    "    \n",
    "    session_data.append({\n",
    "        'session_id': session_id,\n",
    "        'user_id': user_id,\n",
    "        'sequence': X,\n",
    "        'sequence_length': len(X),\n",
    "        'treatment': T,\n",
    "        'outcome': Y\n",
    "    })\n",
    "\n",
    "df_sessions = pd.DataFrame(session_data)\n",
    "\n",
    "print(f\"\\nTotal sessions: {len(df_sessions):,}\")\n",
    "print(f\"Sessions with treatment (click): {df_sessions['treatment'].sum():,} ({df_sessions['treatment'].mean():.1%})\")\n",
    "print(f\"Sessions with outcome (purchase): {df_sessions['outcome'].sum():,} ({df_sessions['outcome'].mean():.1%})\")\n",
    "\n",
    "# Analyze sequence lengths\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"  Mean: {df_sessions['sequence_length'].mean():.1f}\")\n",
    "print(f\"  Median: {df_sessions['sequence_length'].median():.1f}\")\n",
    "print(f\"  Min: {df_sessions['sequence_length'].min()}\")\n",
    "print(f\"  Max: {df_sessions['sequence_length'].max()}\")\n",
    "print(f\"  95th percentile: {df_sessions['sequence_length'].quantile(0.95):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_level_split(\n",
    "    df: pd.DataFrame,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    test_ratio: float = 0.15,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data at the user level to prevent leakage.\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Get unique users\n",
    "    unique_users = df['user_id'].unique()\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(unique_users)\n",
    "    \n",
    "    # Calculate split points\n",
    "    n_users = len(unique_users)\n",
    "    train_end = int(n_users * train_ratio)\n",
    "    val_end = int(n_users * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split users\n",
    "    train_users = unique_users[:train_end]\n",
    "    val_users = unique_users[train_end:val_end]\n",
    "    test_users = unique_users[val_end:]\n",
    "    \n",
    "    # Split dataframe\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df = df[df['user_id'].isin(val_users)].copy()\n",
    "    test_df = df[df['user_id'].isin(test_users)].copy()\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data at user level...\n",
      "\n",
      "Train set:\n",
      "  Sessions: 15\n",
      "  Users: 15\n",
      "  Treatment rate: 66.7%\n",
      "  Outcome rate: 80.0%\n",
      "\n",
      "Validation set:\n",
      "  Sessions: 3\n",
      "  Users: 3\n",
      "  Treatment rate: 66.7%\n",
      "  Outcome rate: 33.3%\n",
      "\n",
      "Test set:\n",
      "  Sessions: 4\n",
      "  Users: 4\n",
      "  Treatment rate: 100.0%\n",
      "  Outcome rate: 75.0%\n",
      "\n",
      "✓ No user overlap between splits\n"
     ]
    }
   ],
   "source": [
    "# Perform train/val/test split\n",
    "print(\"\\nSplitting data at user level...\")\n",
    "\n",
    "train_df, val_df, test_df = user_level_split(\n",
    "    df_sessions,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  Sessions: {len(train_df):,}\")\n",
    "print(f\"  Users: {train_df['user_id'].nunique():,}\")\n",
    "print(f\"  Treatment rate: {train_df['treatment'].mean():.1%}\")\n",
    "print(f\"  Outcome rate: {train_df['outcome'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Sessions: {len(val_df):,}\")\n",
    "print(f\"  Users: {val_df['user_id'].nunique():,}\")\n",
    "print(f\"  Treatment rate: {val_df['treatment'].mean():.1%}\")\n",
    "print(f\"  Outcome rate: {val_df['outcome'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Sessions: {len(test_df):,}\")\n",
    "print(f\"  Users: {test_df['user_id'].nunique():,}\")\n",
    "print(f\"  Treatment rate: {test_df['treatment'].mean():.1%}\")\n",
    "print(f\"  Outcome rate: {test_df['outcome'].mean():.1%}\")\n",
    "\n",
    "# Verify no user overlap\n",
    "train_users = set(train_df['user_id'].unique())\n",
    "val_users = set(val_df['user_id'].unique())\n",
    "test_users = set(test_df['user_id'].unique())\n",
    "\n",
    "assert len(train_users & val_users) == 0, \"Train and validation sets have user overlap!\"\n",
    "assert len(train_users & test_users) == 0, \"Train and test sets have user overlap!\"\n",
    "assert len(val_users & test_users) == 0, \"Validation and test sets have user overlap!\"\n",
    "print(\"\\n✓ No user overlap between splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving processed datasets...\n",
      "  Train saved to: train_sessions.parquet\n",
      "  Validation saved to: validation_sessions.parquet\n",
      "  Test saved to: test_sessions.parquet\n",
      "\n",
      "  Also saved as pickle for compatibility:\n",
      "  Train: train_sessions.pkl\n",
      "  Val: validation_sessions.pkl\n",
      "  Test: test_sessions.pkl\n",
      "\n",
      "  Metadata saved to: processing_metadata.json\n",
      "\n",
      "================================================================================\n",
      "DATA PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Ready for model training!\n",
      "Vocabulary: models/vocab.json\n",
      "Train data: data/train_sessions.parquet\n",
      "Validation data: data/validation_sessions.parquet\n",
      "Test data: data/test_sessions.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets with FIXED FILENAMES (no timestamps)\n",
    "print(\"\\nSaving processed datasets...\")\n",
    "\n",
    "# Use fixed filenames that overwrite\n",
    "train_path = data_dir / 'train_sessions.parquet'\n",
    "val_path = data_dir / 'validation_sessions.parquet'\n",
    "test_path = data_dir / 'test_sessions.parquet'\n",
    "\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "val_df.to_parquet(val_path, index=False)\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "\n",
    "print(f\"  Train saved to: {train_path.name}\")\n",
    "print(f\"  Validation saved to: {val_path.name}\")\n",
    "print(f\"  Test saved to: {test_path.name}\")\n",
    "\n",
    "# ALSO SAVE AS PICKLE FOR COMPATIBILITY\n",
    "train_pkl_path = data_dir / 'train_sessions.pkl'\n",
    "val_pkl_path = data_dir / 'validation_sessions.pkl'\n",
    "test_pkl_path = data_dir / 'test_sessions.pkl'\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(train_pkl_path, 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "with open(val_pkl_path, 'wb') as f:\n",
    "    pickle.dump(val_df, f)\n",
    "with open(test_pkl_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n",
    "\n",
    "print(f\"\\n  Also saved as pickle for compatibility:\")\n",
    "print(f\"  Train: {train_pkl_path.name}\")\n",
    "print(f\"  Val: {val_pkl_path.name}\")\n",
    "print(f\"  Test: {test_pkl_path.name}\")\n",
    "\n",
    "# Save metadata (fixed filename)\n",
    "metadata = {\n",
    "    'session_gap_hours': SESSION_GAP_HOURS,\n",
    "    'min_session_length': MIN_SESSION_LENGTH,\n",
    "    'max_session_length': MAX_SESSION_LENGTH,\n",
    "    'vocab_size': {\n",
    "        'events': len(vocab['event_to_int']),\n",
    "        'items': len(vocab['item_to_int'])\n",
    "    },\n",
    "    'data_splits': {\n",
    "        'train': {\n",
    "            'sessions': len(train_df),\n",
    "            'users': train_df['user_id'].nunique(),\n",
    "            'treatment_rate': float(train_df['treatment'].mean()),\n",
    "            'outcome_rate': float(train_df['outcome'].mean())\n",
    "        },\n",
    "        'validation': {\n",
    "            'sessions': len(val_df),\n",
    "            'users': val_df['user_id'].nunique(),\n",
    "            'treatment_rate': float(val_df['treatment'].mean()),\n",
    "            'outcome_rate': float(val_df['outcome'].mean())\n",
    "        },\n",
    "        'test': {\n",
    "            'sessions': len(test_df),\n",
    "            'users': test_df['user_id'].nunique(),\n",
    "            'treatment_rate': float(test_df['treatment'].mean()),\n",
    "            'outcome_rate': float(test_df['outcome'].mean())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = data_dir / 'processing_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n  Metadata saved to: {metadata_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReady for model training!\")\n",
    "print(f\"Vocabulary: {vocab_path}\")\n",
    "print(f\"Train data: {train_path}\")\n",
    "print(f\"Validation data: {val_path}\")\n",
    "print(f\"Test data: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment-Outcome Analysis:\n",
      "==================================================\n",
      "\n",
      "Train Set:\n",
      "  Control (no click) purchase rate: 80.00%\n",
      "  Treated (click) purchase rate: 80.00%\n",
      "  Naive lift: 0.00%\n",
      "\n",
      "Validation Set:\n",
      "  Control (no click) purchase rate: 0.00%\n",
      "  Treated (click) purchase rate: 50.00%\n",
      "  Naive lift: 50.00%\n",
      "\n",
      "Test Set:\n",
      "  Control (no click) purchase rate: 0.00%\n",
      "  Treated (click) purchase rate: 75.00%\n",
      "  Naive lift: 75.00%\n",
      "\n",
      "Note: This naive lift includes selection bias and is not the true causal effect.\n",
      "The Causal Transformer will estimate the true effect.\n"
     ]
    }
   ],
   "source": [
    "# Analyze treatment effect correlation\n",
    "print(\"\\nTreatment-Outcome Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, df in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    # Calculate purchase rates by treatment status\n",
    "    treated = df[df['treatment'] == 1]\n",
    "    control = df[df['treatment'] == 0]\n",
    "    \n",
    "    treated_purchase_rate = treated['outcome'].mean() if len(treated) > 0 else 0\n",
    "    control_purchase_rate = control['outcome'].mean() if len(control) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{name} Set:\")\n",
    "    print(f\"  Control (no click) purchase rate: {control_purchase_rate:.2%}\")\n",
    "    print(f\"  Treated (click) purchase rate: {treated_purchase_rate:.2%}\")\n",
    "    print(f\"  Naive lift: {(treated_purchase_rate - control_purchase_rate):.2%}\")\n",
    "    \n",
    "print(\"\\nNote: This naive lift includes selection bias and is not the true causal effect.\")\n",
    "print(\"The Causal Transformer will estimate the true effect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Sequences:\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "  Session ID: ext1:0bd275fb-ce99-4062-b49b-9ed813f01791_1\n",
      "  Treatment: 1, Outcome: 1\n",
      "  Sequence length: 21\n",
      "  First 5 events:\n",
      "    1. auction (item=1, time=0.0min)\n",
      "    2. auction (item=1, time=0.8min)\n",
      "    3. auction (item=1, time=5.4min)\n",
      "    4. impression (item=129, time=5.4min)\n",
      "    5. impression (item=72, time=5.4min)\n",
      "    ... (16 more events)\n",
      "\n",
      "Example 2:\n",
      "  Session ID: ext1:369ec0f1-6f2b-477c-b571-cbf43a4fda65_1\n",
      "  Treatment: 1, Outcome: 0\n",
      "  Sequence length: 100\n",
      "  First 5 events:\n",
      "    1. auction (item=1, time=0.0min)\n",
      "    2. impression (item=690, time=0.0min)\n",
      "    3. impression (item=1083, time=0.0min)\n",
      "    4. auction (item=1, time=0.1min)\n",
      "    5. auction (item=1, time=0.3min)\n",
      "    ... (95 more events)\n",
      "\n",
      "Example 3:\n",
      "  Session ID: ext1:91f1cbc8-4881-4e1e-9cdd-1a2ecd3f094c_1\n",
      "  Treatment: 0, Outcome: 1\n",
      "  Sequence length: 250\n",
      "  First 5 events:\n",
      "    1. auction (item=1, time=0.0min)\n",
      "    2. impression (item=1356, time=0.0min)\n",
      "    3. impression (item=1913, time=0.0min)\n",
      "    4. impression (item=2405, time=0.0min)\n",
      "    5. impression (item=1982, time=0.0min)\n",
      "    ... (245 more events)\n"
     ]
    }
   ],
   "source": [
    "# Sample sequence inspection\n",
    "print(\"\\nSample Sequences:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get a few interesting examples\n",
    "examples = [\n",
    "    train_df[train_df['outcome'] == 1].iloc[0],  # Purchase example\n",
    "    train_df[(train_df['treatment'] == 1) & (train_df['outcome'] == 0)].iloc[0],  # Click but no purchase\n",
    "    train_df[(train_df['treatment'] == 0) & (train_df['outcome'] == 1)].iloc[0] if len(train_df[(train_df['treatment'] == 0) & (train_df['outcome'] == 1)]) > 0 else None  # Purchase without click\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    if example is None:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Session ID: {example['session_id']}\")\n",
    "    print(f\"  Treatment: {example['treatment']}, Outcome: {example['outcome']}\")\n",
    "    print(f\"  Sequence length: {example['sequence_length']}\")\n",
    "    print(f\"  First 5 events:\")\n",
    "    \n",
    "    for j, (event_type_id, item_id, timedelta) in enumerate(example['sequence'][:5]):\n",
    "        event_name = vocab['int_to_event'][event_type_id]\n",
    "        print(f\"    {j+1}. {event_name} (item={item_id}, time={timedelta:.1f}min)\")\n",
    "    \n",
    "    if len(example['sequence']) > 5:\n",
    "        print(f\"    ... ({len(example['sequence']) - 5} more events)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_data_pull.ipynb\n",
    "## Snowflake Data Extraction with Unified Event Stream\n",
    "\n",
    "This notebook handles all data extraction from Snowflake and saves checkpoints for analysis.\n",
    "\n",
    "### Workflow:\n",
    "1. Connect to Snowflake\n",
    "2. Sample users deterministically using hash-based bucketing\n",
    "3. Extract raw event data (auctions, impressions, clicks, purchases)\n",
    "4. Create unified event stream for sequential modeling\n",
    "5. Extract catalog data\n",
    "6. Save all data as timestamped checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import textwrap\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress the specific pandas UserWarning for non-SQLAlchemy connections\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    message='pandas only supports SQLAlchemy connectable.*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (SCALED UP):\n",
      "  Analysis period: 2025-09-05 to 2025-09-07\n",
      "  Historical period: 2025-09-02 to 2025-09-05\n",
      "  Sampling fraction: 0.0%\n",
      "  Expected users: ~5000\n",
      "  Journey window: 168 hours\n",
      "  Session gap: 24 hours\n",
      "  SCALED UP: 2 days, 0.0% of users\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION & HYPERPARAMETERS ---\n",
    "\n",
    "# Load environment variables from the .env file for secure credential management\n",
    "load_dotenv()\n",
    "\n",
    "# ANALYSIS_END_DATE: The last date to be included in our analysis window\n",
    "ANALYSIS_END_DATE = date(2025, 9, 7)\n",
    "\n",
    "# DAYS_WINDOW: Number of days of data to pull, counting back from ANALYSIS_END_DATE\n",
    "# Using 2 days as a balance between data volume and speed\n",
    "DAYS_WINDOW = 2  # 2 days - middle ground\n",
    "\n",
    "# SAMPLING_FRACTION: Percentage of users to include in the analysis\n",
    "# 5% to get ~5000 users - enough for meaningful model training\n",
    "SAMPLING_FRACTION = 0.0002  # 5% of users (~5000 users)\n",
    "\n",
    "# JOURNEY_WINDOW_HOURS: Duration that defines a single user journey/session\n",
    "JOURNEY_WINDOW_HOURS = 168  # 7 days\n",
    "\n",
    "# SESSION_GAP_HOURS: Hours of inactivity that define a new session within a journey\n",
    "SESSION_GAP_HOURS = 24  # 24 hours for sessionization\n",
    "\n",
    "# HISTORICAL_DAYS: Days of historical data to pull for feature engineering\n",
    "# Reduced to speed up extraction\n",
    "HISTORICAL_DAYS = 3  # 3 days of history\n",
    "\n",
    "# Calculate date ranges\n",
    "ANALYSIS_START_DATE = ANALYSIS_END_DATE - timedelta(days=DAYS_WINDOW)\n",
    "HISTORICAL_START_DATE = ANALYSIS_START_DATE - timedelta(days=HISTORICAL_DAYS)\n",
    "\n",
    "print(\"Configuration (SCALED UP):\")\n",
    "print(f\"  Analysis period: {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")\n",
    "print(f\"  Historical period: {HISTORICAL_START_DATE} to {ANALYSIS_START_DATE}\")\n",
    "print(f\"  Sampling fraction: {SAMPLING_FRACTION:.1%}\")\n",
    "print(f\"  Expected users: ~5000\")\n",
    "print(f\"  Journey window: {JOURNEY_WINDOW_HOURS} hours\")\n",
    "print(f\"  Session gap: {SESSION_GAP_HOURS} hours\")\n",
    "print(f\"  SCALED UP: {DAYS_WINDOW} days, {SAMPLING_FRACTION:.1%} of users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Snowflake connection established.\n"
     ]
    }
   ],
   "source": [
    "# --- SNOWFLAKE CONNECTION ---\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"[SUCCESS] Snowflake connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] Could not connect to Snowflake: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sampling_cte(start_date: str, end_date: str, sampling_fraction: float) -> str:\n",
    "    \"\"\"\n",
    "    Build CTE for deterministic user sampling using hash-based bucketing.\n",
    "    This ensures reproducible sampling across runs.\n",
    "    \"\"\"\n",
    "    total_buckets = 10000\n",
    "    selection_threshold = int(total_buckets * sampling_fraction)\n",
    "    \n",
    "    return textwrap.dedent(f\"\"\"\n",
    "        WITH SAMPLED_USER_IDS AS (\n",
    "            WITH REPEAT_PURCHASERS AS (\n",
    "                SELECT USER_ID\n",
    "                FROM PURCHASES\n",
    "                WHERE PURCHASED_AT BETWEEN '{start_date}'\n",
    "                  AND '{end_date}'\n",
    "                GROUP BY USER_ID\n",
    "                HAVING COUNT(DISTINCT PURCHASE_ID) >= 2\n",
    "            ),\n",
    "            BUCKETED_USERS AS (\n",
    "                SELECT\n",
    "                    USER_ID,\n",
    "                    MOD(ABS(HASH(USER_ID)), {total_buckets}) AS bucket\n",
    "                FROM REPEAT_PURCHASERS\n",
    "            )\n",
    "            SELECT USER_ID\n",
    "            FROM BUCKETED_USERS\n",
    "            WHERE bucket < {selection_threshold}\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_users(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_USERS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_USERS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            au.OPAQUE_USER_ID AS USER_ID,\n",
    "            au.CREATED_AT\n",
    "        FROM AUCTIONS_USERS au\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"AUCTIONS_USERS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} auction records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_results(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_RESULTS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_RESULTS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "            LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n",
    "            LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            ar.RANKING,\n",
    "            ar.IS_WINNER,\n",
    "            ar.CREATED_AT\n",
    "        FROM AUCTIONS_RESULTS ar\n",
    "        JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"AUCTIONS_RESULTS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} bid records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impressions(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract IMPRESSIONS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting IMPRESSIONS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            i.INTERACTION_ID,\n",
    "            LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            i.USER_ID,\n",
    "            LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            i.OCCURRED_AT\n",
    "        FROM IMPRESSIONS i\n",
    "        JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "        WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"IMPRESSIONS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} impression records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clicks(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract CLICKS table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting CLICKS...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            c.INTERACTION_ID,\n",
    "            LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.USER_ID,\n",
    "            LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            c.OCCURRED_AT\n",
    "        FROM CLICKS c\n",
    "        JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "        WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"CLICKS\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} click records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purchases(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract PURCHASES table with proper ID standardization.\"\"\"\n",
    "    print(\"\\nExtracting PURCHASES...\")\n",
    "    \n",
    "    sampling_cte = build_sampling_cte(HISTORICAL_START_DATE, end_date, sampling_fraction)\n",
    "    \n",
    "    query = sampling_cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            p.PURCHASE_ID,\n",
    "            p.PURCHASED_AT,\n",
    "            LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            p.QUANTITY,\n",
    "            p.UNIT_PRICE,\n",
    "            p.USER_ID,\n",
    "            p.PURCHASE_LINE\n",
    "        FROM PURCHASES p\n",
    "        JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "        WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    with tqdm(desc=\"PURCHASES\") as pbar:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        pbar.update(len(df))\n",
    "    \n",
    "    print(f\"  Extracted {len(df):,} purchase records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_catalog_with_products(conn, start_date: str, end_date: str, sampling_fraction: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract catalog data for products that sampled users interacted with.\n",
    "    Uses a complete CTE chain: SAMPLED_USER_IDS -> ALL_PRODUCT_IDS -> CATALOG\n",
    "    \"\"\"\n",
    "    print(\"\\nExtracting CATALOG with full CTE chain...\")\n",
    "    \n",
    "    # Build the base sampling CTE\n",
    "    sampling_cte = build_sampling_cte(start_date, end_date, sampling_fraction)\n",
    "    \n",
    "    # Build the complete query with proper CTE chaining\n",
    "    # Note the comma after the first CTE and no WITH for subsequent CTEs\n",
    "    query = sampling_cte + f\"\"\",\n",
    "        ALL_PRODUCT_IDS AS (\n",
    "            -- Get products from impressions\n",
    "            SELECT DISTINCT LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM IMPRESSIONS i\n",
    "            JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.USER_ID\n",
    "            WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND i.PRODUCT_ID IS NOT NULL\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            -- Get products from clicks\n",
    "            SELECT DISTINCT LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM CLICKS c\n",
    "            JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.USER_ID\n",
    "            WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND c.PRODUCT_ID IS NOT NULL\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            -- Get products from purchases\n",
    "            SELECT DISTINCT LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID\n",
    "            FROM PURCHASES p\n",
    "            JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.USER_ID\n",
    "            WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND p.PRODUCT_ID IS NOT NULL\n",
    "        )\n",
    "        -- Now fetch catalog for those products\n",
    "        SELECT\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) as PRODUCT_ID,\n",
    "            c.NAME,\n",
    "            c.PRICE,\n",
    "            c.ACTIVE,\n",
    "            c.IS_DELETED,\n",
    "            c.DESCRIPTION,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'brand%%'), ''), '#', 2) AS BRAND,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'department%%'), ''), '#', 2) AS DEPARTMENT_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category%%'), ''), '#', 2) AS CATEGORY_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'color%%'), ''), '#', 2) AS PRIMARY_COLOR\n",
    "        FROM CATALOG c\n",
    "        JOIN ALL_PRODUCT_IDS ap ON LOWER(TRIM(c.PRODUCT_ID)) = ap.PRODUCT_ID\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with tqdm(desc=\"CATALOG (Full CTE)\") as pbar:\n",
    "            df = pd.read_sql(query, conn)\n",
    "            pbar.update(len(df))\n",
    "        \n",
    "        print(f\"  Extracted {len(df):,} catalog records\")\n",
    "        print(f\"  Products with descriptions: {df['DESCRIPTION'].notna().sum():,}\")\n",
    "        print(f\"  Products with price data: {df['PRICE'].notna().sum():,}\")\n",
    "        \n",
    "        # Create price buckets for categorical encoding\n",
    "        if 'PRICE' in df.columns and df['PRICE'].notna().any():\n",
    "            # Use quantiles for price bucketing, handling missing values\n",
    "            df['PRICE_BUCKET'] = pd.qcut(df['PRICE'].fillna(df['PRICE'].median()), \n",
    "                                         q=10, labels=False, duplicates='drop')\n",
    "        else:\n",
    "            df['PRICE_BUCKET'] = 0\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting catalog: {e}\")\n",
    "        print(\"  Returning empty catalog dataframe\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'PRODUCT_ID', 'NAME', 'PRICE', 'ACTIVE', 'IS_DELETED', \n",
    "            'DESCRIPTION', 'BRAND', 'DEPARTMENT_ID', 'CATEGORY_ID', \n",
    "            'PRIMARY_COLOR', 'PRICE_BUCKET'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_event_stream(\n",
    "    auctions: pd.DataFrame,\n",
    "    impressions: pd.DataFrame,\n",
    "    clicks: pd.DataFrame,\n",
    "    purchases: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines individual event DataFrames into a single, chronologically sorted stream.\n",
    "    This is critical for sequential modeling with the Causal Transformer.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Unified Event Stream from pulled data ---\")\n",
    "    \n",
    "    # 1. Standardize AUCTIONS_USERS to our event format\n",
    "    df_auctions = auctions[['USER_ID', 'CREATED_AT']].copy()\n",
    "    df_auctions.rename(columns={'CREATED_AT': 'event_timestamp'}, inplace=True)\n",
    "    df_auctions['event_type'] = 'auction'\n",
    "    df_auctions['product_id'] = None  # No specific product for auction event\n",
    "    print(f\"  Processed {len(df_auctions):,} auction events.\")\n",
    "\n",
    "    # 2. Standardize IMPRESSIONS\n",
    "    df_impressions = impressions[['USER_ID', 'OCCURRED_AT', 'PRODUCT_ID']].copy()\n",
    "    df_impressions.rename(columns={'OCCURRED_AT': 'event_timestamp', 'PRODUCT_ID': 'product_id'}, inplace=True)\n",
    "    df_impressions['event_type'] = 'impression'\n",
    "    print(f\"  Processed {len(df_impressions):,} impression events.\")\n",
    "\n",
    "    # 3. Standardize CLICKS\n",
    "    df_clicks = clicks[['USER_ID', 'OCCURRED_AT', 'PRODUCT_ID']].copy()\n",
    "    df_clicks.rename(columns={'OCCURRED_AT': 'event_timestamp', 'PRODUCT_ID': 'product_id'}, inplace=True)\n",
    "    df_clicks['event_type'] = 'click'\n",
    "    print(f\"  Processed {len(df_clicks):,} click events.\")\n",
    "    \n",
    "    # 4. Standardize PURCHASES\n",
    "    df_purchases = purchases[['USER_ID', 'PURCHASED_AT', 'PRODUCT_ID']].copy()\n",
    "    df_purchases.rename(columns={'PURCHASED_AT': 'event_timestamp', 'PRODUCT_ID': 'product_id'}, inplace=True)\n",
    "    df_purchases['event_type'] = 'purchase'\n",
    "    print(f\"  Processed {len(df_purchases):,} purchase events.\")\n",
    "\n",
    "    # 5. Concatenate all standardized DataFrames\n",
    "    print(\"\\n  Concatenating all event types...\")\n",
    "    unified_df = pd.concat([df_auctions, df_impressions, df_clicks, df_purchases], ignore_index=True)\n",
    "    \n",
    "    # 6. Sort the entire stream by user and time (CRITICAL for sequential modeling)\n",
    "    print(\"  Sorting the unified stream by user and timestamp...\")\n",
    "    unified_df.sort_values(by=['USER_ID', 'event_timestamp'], inplace=True)\n",
    "    unified_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 7. Final type casting for memory efficiency\n",
    "    unified_df['event_type'] = unified_df['event_type'].astype('category')\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] Created unified event stream with {len(unified_df):,} total events.\")\n",
    "    return unified_df[['USER_ID', 'event_timestamp', 'event_type', 'product_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Data Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING DATA EXTRACTION PIPELINE FOR CAUSAL TRANSFORMER\n",
      "================================================================================\n",
      "\n",
      "--- Extracting main analysis period data ---\n",
      "\n",
      "Extracting AUCTIONS_USERS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUCTIONS_USERS: 1115it [00:07, 159.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1,115 auction records\n",
      "\n",
      "Extracting IMPRESSIONS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMPRESSIONS: 4147it [00:07, 575.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 4,147 impression records\n",
      "\n",
      "Extracting CLICKS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLICKS: 97it [00:02, 34.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 97 click records\n",
      "\n",
      "Extracting PURCHASES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PURCHASES: 49it [00:03, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 49 purchase records\n",
      "\n",
      "--- Creating Unified Event Stream from pulled data ---\n",
      "  Processed 1,115 auction events.\n",
      "  Processed 4,147 impression events.\n",
      "  Processed 97 click events.\n",
      "  Processed 49 purchase events.\n",
      "\n",
      "  Concatenating all event types...\n",
      "  Sorting the unified stream by user and timestamp...\n",
      "\n",
      "[SUCCESS] Created unified event stream with 5,408 total events.\n",
      "\n",
      "Extracting CATALOG with full CTE chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CATALOG (Full CTE): 1310it [00:52, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1,310 catalog records\n",
      "  Products with descriptions: 1,310\n",
      "  Products with price data: 1,310\n",
      "\n",
      "--- Enriching events with product features ---\n",
      "  Events with price data: 1,742\n",
      "  Events with category data: 1,742\n",
      "  Events with descriptions: 1,742\n",
      "\n",
      "[SUCCESS] Snowflake connection closed\n",
      "\n",
      "--- Saving data ---\n",
      "  Saved enriched unified_events: 5,408 rows to unified_events.parquet\n",
      "  Saved catalog: 1,310 rows to catalog.parquet\n",
      "  Saved metadata to metadata.json\n",
      "\n",
      "================================================================================\n",
      "DATA EXTRACTION COMPLETE - ENRICHED WITH FEATURES!\n",
      "Extracted 5,408 events with product features\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN DATA EXTRACTION PIPELINE ---\n",
    "\n",
    "if conn:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING DATA EXTRACTION PIPELINE FOR CAUSAL TRANSFORMER\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # NO TIMESTAMP - use fixed filenames that overwrite\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Convert dates to strings for SQL\n",
    "    start_date_str = ANALYSIS_START_DATE.strftime('%Y-%m-%d')\n",
    "    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n",
    "    hist_start_str = HISTORICAL_START_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # --- Extract ONLY what we need for unified events ---\n",
    "    print(\"\\n--- Extracting main analysis period data ---\")\n",
    "    auctions_users = extract_auctions_users(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    # auctions_results = extract_auctions_results(conn, start_date_str, end_date_str, SAMPLING_FRACTION)  # SKIP - too large\n",
    "    impressions = extract_impressions(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    clicks = extract_clicks(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    purchases = extract_purchases(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # --- Create unified event stream for sequential modeling ---\n",
    "    unified_events = create_unified_event_stream(\n",
    "        auctions=auctions_users,\n",
    "        impressions=impressions,\n",
    "        clicks=clicks,\n",
    "        purchases=purchases\n",
    "    )\n",
    "    \n",
    "    # --- Extract catalog using full CTE chain ---\n",
    "    catalog = extract_catalog_with_products(conn, start_date_str, end_date_str, SAMPLING_FRACTION)\n",
    "    \n",
    "    # --- Enrich unified events with product features ---\n",
    "    print(\"\\n--- Enriching events with product features ---\")\n",
    "    if len(catalog) > 0:\n",
    "        # Merge catalog features with unified events\n",
    "        unified_events = unified_events.merge(\n",
    "            catalog[['PRODUCT_ID', 'PRICE', 'PRICE_BUCKET', 'CATEGORY_ID', 'NAME', 'DESCRIPTION']], \n",
    "            left_on='product_id', \n",
    "            right_on='PRODUCT_ID', \n",
    "            how='left'\n",
    "        )\n",
    "        # Drop duplicate PRODUCT_ID column\n",
    "        unified_events = unified_events.drop('PRODUCT_ID', axis=1)\n",
    "        \n",
    "        # Fill missing values\n",
    "        unified_events['PRICE'] = unified_events['PRICE'].fillna(0)\n",
    "        unified_events['PRICE_BUCKET'] = unified_events['PRICE_BUCKET'].fillna(0).astype(int)\n",
    "        unified_events['CATEGORY_ID'] = unified_events['CATEGORY_ID'].fillna('unknown')\n",
    "        unified_events['NAME'] = unified_events['NAME'].fillna('')\n",
    "        unified_events['DESCRIPTION'] = unified_events['DESCRIPTION'].fillna('')\n",
    "        \n",
    "        print(f\"  Events with price data: {(unified_events['PRICE'] > 0).sum():,}\")\n",
    "        print(f\"  Events with category data: {(unified_events['CATEGORY_ID'] != 'unknown').sum():,}\")\n",
    "        print(f\"  Events with descriptions: {(unified_events['DESCRIPTION'] != '').sum():,}\")\n",
    "    else:\n",
    "        print(\"  Warning: No catalog data extracted, proceeding without product features\")\n",
    "        # Add empty columns to maintain consistency\n",
    "        unified_events['PRICE'] = 0\n",
    "        unified_events['PRICE_BUCKET'] = 0\n",
    "        unified_events['CATEGORY_ID'] = 'unknown'\n",
    "        unified_events['NAME'] = ''\n",
    "        unified_events['DESCRIPTION'] = ''\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"\\n[SUCCESS] Snowflake connection closed\")\n",
    "    \n",
    "    # --- Save ONLY essential data (no timestamps, overwrite) ---\n",
    "    print(\"\\n--- Saving data ---\")\n",
    "    output_dir = Path(\"./data\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save enriched unified_events\n",
    "    unified_path = output_dir / \"unified_events.parquet\"\n",
    "    unified_events.to_parquet(unified_path, index=False)\n",
    "    print(f\"  Saved enriched unified_events: {len(unified_events):,} rows to {unified_path.name}\")\n",
    "    \n",
    "    # Save catalog for later reference\n",
    "    if len(catalog) > 0:\n",
    "        catalog_path = output_dir / \"catalog.parquet\"\n",
    "        catalog.to_parquet(catalog_path, index=False)\n",
    "        print(f\"  Saved catalog: {len(catalog):,} rows to {catalog_path.name}\")\n",
    "    \n",
    "    # Save metadata for reference (also fixed filename)\n",
    "    # Convert all numpy types to native Python types for JSON serialization\n",
    "    metadata = {\n",
    "        'analysis_start_date': start_date_str,\n",
    "        'analysis_end_date': end_date_str,\n",
    "        'historical_start_date': hist_start_str,\n",
    "        'sampling_fraction': float(SAMPLING_FRACTION),\n",
    "        'journey_window_hours': int(JOURNEY_WINDOW_HOURS),\n",
    "        'session_gap_hours': int(SESSION_GAP_HOURS),\n",
    "        'total_events': int(len(unified_events)),\n",
    "        'unique_users': int(unified_events['USER_ID'].nunique()),\n",
    "        'unique_products': int(unified_events['product_id'].nunique()),\n",
    "        'unique_categories': int(unified_events['CATEGORY_ID'].nunique()) if 'CATEGORY_ID' in unified_events.columns else 0,\n",
    "        'events_with_features': int((unified_events['PRICE'] > 0).sum()) if 'PRICE' in unified_events.columns else 0,\n",
    "        'catalog_products': int(len(catalog)) if len(catalog) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    metadata_path = output_dir / \"metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  Saved metadata to {metadata_path.name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION COMPLETE - ENRICHED WITH FEATURES!\")\n",
    "    print(f\"Extracted {len(unified_events):,} events with product features\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] No Snowflake connection available. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA EXTRACTION SUMMARY FOR SEQUENTIAL MODELING\n",
      "================================================================================\n",
      "\n",
      "Unified Event Stream:\n",
      "  Total events: 5,408\n",
      "  Unique users: 24\n",
      "  Date range: 2025-09-05 00:05:17.255000 to 2025-09-06 23:59:37\n",
      "\n",
      "Event Type Distribution:\n",
      "  impression: 4,147 (76.7%)\n",
      "  auction: 1,115 (20.6%)\n",
      "  click: 97 (1.8%)\n",
      "  purchase: 49 (0.9%)\n",
      "\n",
      "Main Analysis Period:\n",
      "  Total auctions: 1,115\n",
      "  Total impressions: 4,147\n",
      "  Total clicks: 97\n",
      "  Total purchases: 49\n",
      "  Catalog products: 1,310\n",
      "\n",
      "Data Quality Checks:\n",
      "  Null USER_IDs: 0 (0.0000%)\n",
      "  Null timestamps: 0 (0.0000%)\n",
      "  Events correctly sorted by user and time: True\n",
      "  Product catalog coverage: 44.3%\n",
      "\n",
      "Feature Enrichment:\n",
      "  Events with product features: 1,742 / 5,408\n",
      "  Events with categories: 1,742 / 5,408\n",
      "  Events with descriptions: 1,742 / 5,408\n",
      "\n",
      "================================================================================\n",
      "Ready for sessionization and sequential modeling!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data extraction summary and quality checks\n",
    "if 'unified_events' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION SUMMARY FOR SEQUENTIAL MODELING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nUnified Event Stream:\")\n",
    "    print(f\"  Total events: {len(unified_events):,}\")\n",
    "    print(f\"  Unique users: {unified_events['USER_ID'].nunique():,}\")\n",
    "    print(f\"  Date range: {unified_events['event_timestamp'].min()} to {unified_events['event_timestamp'].max()}\")\n",
    "    \n",
    "    print(\"\\nEvent Type Distribution:\")\n",
    "    event_counts = unified_events['event_type'].value_counts()\n",
    "    for event_type, count in event_counts.items():\n",
    "        print(f\"  {event_type}: {count:,} ({count/len(unified_events)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nMain Analysis Period:\")\n",
    "    print(f\"  Total auctions: {len(auctions_users):,}\")\n",
    "    # print(f\"  Total bids: {len(auctions_results):,}\")  # Skipped - too large\n",
    "    print(f\"  Total impressions: {len(impressions):,}\")\n",
    "    print(f\"  Total clicks: {len(clicks):,}\")\n",
    "    print(f\"  Total purchases: {len(purchases):,}\")\n",
    "    print(f\"  Catalog products: {len(catalog):,}\")\n",
    "    \n",
    "    # Basic data quality checks\n",
    "    print(\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for null values in critical columns\n",
    "    null_users = unified_events['USER_ID'].isnull().sum()\n",
    "    null_timestamps = unified_events['event_timestamp'].isnull().sum()\n",
    "    print(f\"  Null USER_IDs: {null_users} ({null_users/len(unified_events)*100:.4f}%)\")\n",
    "    print(f\"  Null timestamps: {null_timestamps} ({null_timestamps/len(unified_events)*100:.4f}%)\")\n",
    "    \n",
    "    # Verify sort order\n",
    "    is_sorted = True\n",
    "    for user_id in unified_events['USER_ID'].unique()[:10]:  # Check first 10 users\n",
    "        user_events = unified_events[unified_events['USER_ID'] == user_id]['event_timestamp']\n",
    "        if not user_events.is_monotonic_increasing:\n",
    "            is_sorted = False\n",
    "            break\n",
    "    print(f\"  Events correctly sorted by user and time: {is_sorted}\")\n",
    "    \n",
    "    # Check product coverage in catalog\n",
    "    products_in_events = set(unified_events[unified_events['product_id'].notna()]['product_id'].unique())\n",
    "    products_in_catalog = set(catalog['PRODUCT_ID'].unique())\n",
    "    coverage = len(products_in_catalog & products_in_events) / len(products_in_events) if products_in_events else 0\n",
    "    print(f\"  Product catalog coverage: {coverage:.1%}\")\n",
    "    \n",
    "    # Check enrichment success\n",
    "    print(\"\\nFeature Enrichment:\")\n",
    "    print(f\"  Events with product features: {(unified_events['PRICE'] > 0).sum():,} / {len(unified_events):,}\")\n",
    "    print(f\"  Events with categories: {(unified_events['CATEGORY_ID'] != 'unknown').sum():,} / {len(unified_events):,}\")\n",
    "    print(f\"  Events with descriptions: {(unified_events['DESCRIPTION'] != '').sum():,} / {len(unified_events):,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Ready for sessionization and sequential modeling!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

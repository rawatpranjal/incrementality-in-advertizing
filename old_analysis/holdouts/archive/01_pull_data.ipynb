{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_pull_data.ipynb: Hyper-Efficient Data Extraction for Bounded Causal Analysis\n",
    "\n",
    "## Objective\n",
    "Extract aggregated counts from PURCHASES and AUCTIONS_USERS tables only, avoiding complex joins.\n",
    "\n",
    "## Key Design Decisions\n",
    "- Period 1 (Segmentation): 2025-03-10 to 2025-06-30\n",
    "- Period 2 (Outcomes): 2025-07-01 onwards\n",
    "- Analysis Cohort: Users with ≥1 purchase in Period 1\n",
    "- Treatment: Presence in AUCTIONS_USERS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period 1 (Segmentation): 2025-03-10 to 2025-06-30\n",
      "Period 2 (Outcomes): 2025-07-01 to 2025-09-15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Time period definitions\n",
    "PERIOD1_START = '2025-03-10'\n",
    "PERIOD1_END = '2025-06-30'\n",
    "PERIOD2_START = '2025-07-01'\n",
    "PERIOD2_END = '2025-09-15'\n",
    "\n",
    "print(f\"Period 1 (Segmentation): {PERIOD1_START} to {PERIOD1_END}\")\n",
    "print(f\"Period 2 (Outcomes): {PERIOD2_START} to {PERIOD2_END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection to Snowflake successful!\n",
      "   Snowflake version: 9.28.1\n"
     ]
    }
   ],
   "source": [
    "# Establish Snowflake connection\n",
    "conn = None\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"✅ Connection to Snowflake successful!\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT CURRENT_VERSION()\")\n",
    "    version = cursor.fetchone()\n",
    "    print(f\"   Snowflake version: {version[0]}\")\n",
    "    cursor.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Could not connect to Snowflake.\", file=sys.stderr)\n",
    "    print(f\"   Details: {e}\", file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build Analysis Cohort and Segmentation\n",
    "\n",
    "Extract all users with purchases in Period 1 and calculate segmentation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Period 1 user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_13913/2832571098.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_period1_features = pd.read_sql(period1_features_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 3,430,304 users with Period 1 purchases\n",
      "   Time taken: 34.53 seconds\n",
      "\n",
      "Sample of Period 1 features:\n",
      "                                     USER_ID  PURCHASE_FREQUENCY  TOTAL_SPEND  \\\n",
      "0  ext1:46e99e8d-b96a-4eab-ba68-33c1197e9f68                   1         45.0   \n",
      "1  ext1:93530b45-e549-43ff-aeea-9ade41b2fc7d                   1         60.0   \n",
      "2  ext1:3ded7e05-a29d-4e7a-8ffd-50258e596160                   1        132.0   \n",
      "3  ext1:f100c7ab-337b-47be-b425-e1ca028549d7                   1        250.0   \n",
      "4  ext1:d0ca3816-e99f-44a5-968b-3566638b1029                   1        115.0   \n",
      "\n",
      "   AVG_ORDER_VALUE FIRST_PURCHASE_DATE  PURCHASE_SPAN_DAYS  \n",
      "0             45.0 2025-03-31 16:43:32                   0  \n",
      "1             60.0 2025-05-27 13:42:38                   0  \n",
      "2            132.0 2025-03-15 19:12:00                   0  \n",
      "3            250.0 2025-06-18 05:20:24                   0  \n",
      "4            115.0 2025-03-23 04:02:34                   0  \n",
      "\n",
      "Summary statistics:\n",
      "       PURCHASE_FREQUENCY   TOTAL_SPEND  AVG_ORDER_VALUE  \\\n",
      "count        3.430304e+06  3.430304e+06     3.430304e+06   \n",
      "mean         3.679964e+00  1.651085e+02     4.894611e+01   \n",
      "min          1.000000e+00  3.000000e+00     3.000000e+00   \n",
      "25%          1.000000e+00  2.700000e+01     1.800000e+01   \n",
      "50%          1.000000e+00  5.600000e+01     2.800000e+01   \n",
      "75%          3.000000e+00  1.360000e+02     4.900000e+01   \n",
      "max          2.278000e+03  2.540590e+05     4.250000e+04   \n",
      "std          9.353022e+00  8.123553e+02     1.326394e+02   \n",
      "\n",
      "                 FIRST_PURCHASE_DATE  PURCHASE_SPAN_DAYS  \n",
      "count                        3430304        3.430304e+06  \n",
      "mean   2025-04-27 03:28:06.615835392        1.948189e+01  \n",
      "min              2025-03-14 00:00:01        0.000000e+00  \n",
      "25%    2025-03-29 11:46:31.750000128        0.000000e+00  \n",
      "50%       2025-04-22 17:44:09.500000        0.000000e+00  \n",
      "75%       2025-05-23 12:16:57.500000        3.200000e+01  \n",
      "max              2025-06-29 23:59:59        1.070000e+02  \n",
      "std                              NaN        3.116172e+01  \n"
     ]
    }
   ],
   "source": [
    "# Query to extract Period 1 user features for segmentation\n",
    "print(\"Extracting Period 1 user features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "period1_features_query = f\"\"\"\n",
    "WITH period1_purchases AS (\n",
    "    SELECT \n",
    "        USER_ID,\n",
    "        PURCHASE_ID,\n",
    "        PURCHASED_AT,\n",
    "        QUANTITY,\n",
    "        UNIT_PRICE,\n",
    "        (QUANTITY * UNIT_PRICE / 100.0) AS purchase_value_dollars\n",
    "    FROM PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{PERIOD1_START}' \n",
    "      AND PURCHASED_AT <= '{PERIOD1_END}'\n",
    "),\n",
    "user_features AS (\n",
    "    SELECT\n",
    "        USER_ID,\n",
    "        COUNT(DISTINCT PURCHASE_ID) AS purchase_frequency,\n",
    "        SUM(purchase_value_dollars) AS total_spend,\n",
    "        AVG(purchase_value_dollars) AS avg_order_value,\n",
    "        MIN(PURCHASED_AT) AS first_purchase_date,\n",
    "        DATEDIFF('day', MIN(PURCHASED_AT), MAX(PURCHASED_AT)) AS purchase_span_days\n",
    "    FROM period1_purchases\n",
    "    GROUP BY USER_ID\n",
    ")\n",
    "SELECT * FROM user_features\n",
    "\"\"\"\n",
    "\n",
    "df_period1_features = pd.read_sql(period1_features_query, conn)\n",
    "print(f\"✅ Extracted {len(df_period1_features):,} users with Period 1 purchases\")\n",
    "print(f\"   Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"\\nSample of Period 1 features:\")\n",
    "print(df_period1_features.head())\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(df_period1_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create User Segments\n",
    "\n",
    "Define segments based on Period 1 purchase behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user segments...\n",
      "\n",
      "Available columns: ['USER_ID', 'PURCHASE_FREQUENCY', 'TOTAL_SPEND', 'AVG_ORDER_VALUE', 'FIRST_PURCHASE_DATE', 'PURCHASE_SPAN_DAYS']\n",
      "Columns after lowercase conversion: ['user_id', 'purchase_frequency', 'total_spend', 'avg_order_value', 'first_purchase_date', 'purchase_span_days']\n",
      "\n",
      "Segment distributions:\n",
      "\n",
      "Spend Quintiles:\n",
      "spend_quintile\n",
      "Q1_Low     690501\n",
      "Q2         683756\n",
      "Q3         687322\n",
      "Q4         683264\n",
      "Q5_High    685461\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency Groups:\n",
      "frequency_group\n",
      "1_purchase        1823163\n",
      "2-3_purchases      822747\n",
      "4-10_purchases     564330\n",
      "11+_purchases      220064\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tenure Groups:\n",
      "tenure_group\n",
      "0-30_days     1310754\n",
      "31-60_days     915513\n",
      "61-90_days     748294\n",
      "91+_days       455743\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AOV Quintiles:\n",
      "aov_quintile\n",
      "AOV_Q1_Low     710103\n",
      "AOV_Q2         679787\n",
      "AOV_Q3         668430\n",
      "AOV_Q4         689551\n",
      "AOV_Q5_High    682433\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create segmentation variables\n",
    "print(\"Creating user segments...\")\n",
    "print(f\"\\nAvailable columns: {df_period1_features.columns.tolist()}\")\n",
    "\n",
    "# Snowflake returns uppercase column names - convert to lowercase\n",
    "df_period1_features.columns = [col.lower() for col in df_period1_features.columns]\n",
    "print(f\"Columns after lowercase conversion: {df_period1_features.columns.tolist()}\")\n",
    "\n",
    "# Monetary value quintiles\n",
    "df_period1_features['spend_quintile'] = pd.qcut(\n",
    "    df_period1_features['total_spend'], \n",
    "    q=5, \n",
    "    labels=['Q1_Low', 'Q2', 'Q3', 'Q4', 'Q5_High'],\n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "# Purchase frequency groups\n",
    "freq_bins = [0, 1, 3, 10, float('inf')]\n",
    "freq_labels = ['1_purchase', '2-3_purchases', '4-10_purchases', '11+_purchases']\n",
    "df_period1_features['frequency_group'] = pd.cut(\n",
    "    df_period1_features['purchase_frequency'],\n",
    "    bins=freq_bins,\n",
    "    labels=freq_labels,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Tenure segments (days from period start to first purchase)\n",
    "period1_start_date = pd.to_datetime(PERIOD1_START)\n",
    "df_period1_features['first_purchase_date'] = pd.to_datetime(df_period1_features['first_purchase_date'])\n",
    "df_period1_features['days_since_period_start'] = (\n",
    "    df_period1_features['first_purchase_date'] - period1_start_date\n",
    ").dt.days\n",
    "\n",
    "tenure_bins = [-1, 30, 60, 90, float('inf')]  # Changed to start from -1 to include day 0\n",
    "tenure_labels = ['0-30_days', '31-60_days', '61-90_days', '91+_days']\n",
    "df_period1_features['tenure_group'] = pd.cut(\n",
    "    df_period1_features['days_since_period_start'],\n",
    "    bins=tenure_bins,\n",
    "    labels=tenure_labels,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Average order value quintiles\n",
    "df_period1_features['aov_quintile'] = pd.qcut(\n",
    "    df_period1_features['avg_order_value'],\n",
    "    q=5,\n",
    "    labels=['AOV_Q1_Low', 'AOV_Q2', 'AOV_Q3', 'AOV_Q4', 'AOV_Q5_High'],\n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "print(\"\\nSegment distributions:\")\n",
    "print(\"\\nSpend Quintiles:\")\n",
    "print(df_period1_features['spend_quintile'].value_counts().sort_index())\n",
    "print(\"\\nFrequency Groups:\")\n",
    "print(df_period1_features['frequency_group'].value_counts().sort_index())\n",
    "print(\"\\nTenure Groups:\")\n",
    "print(df_period1_features['tenure_group'].value_counts().sort_index())\n",
    "print(\"\\nAOV Quintiles:\")\n",
    "print(df_period1_features['aov_quintile'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify Treatment and Control Groups\n",
    "\n",
    "Check which users appear in AUCTIONS_USERS table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying treatment group (users in AUCTIONS_USERS)...\n",
      "   Checking for users exposed to ads (auctions)...\n",
      "   Querying for users with ad exposure (impressions or clicks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_13913/3282503318.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_treatment_users = pd.read_sql(treatment_query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Get all users who have been in auctions (treatment group)\n",
    "print(\"Identifying treatment group (users in AUCTIONS_USERS)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# The challenge: AUCTIONS_USERS has OPAQUE_USER_ID, PURCHASES has USER_ID\n",
    "# We need to identify which purchase users were exposed to auctions\n",
    "\n",
    "print(\"   Checking for users exposed to ads (auctions)...\")\n",
    "\n",
    "# Since we know from the previous holdout detection that impressions can be linked to users,\n",
    "# let's use a more direct approach through impressions/clicks which have USER_ID\n",
    "treatment_query = \"\"\"\n",
    "WITH auction_exposed_users AS (\n",
    "    -- Get users who received impressions (these are the treated users)\n",
    "    SELECT DISTINCT USER_ID\n",
    "    FROM IMPRESSIONS\n",
    "    WHERE USER_ID IS NOT NULL\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    -- Also include users who clicked (subset of impressions but for completeness)\n",
    "    SELECT DISTINCT USER_ID  \n",
    "    FROM CLICKS\n",
    "    WHERE USER_ID IS NOT NULL\n",
    ")\n",
    "SELECT DISTINCT USER_ID\n",
    "FROM auction_exposed_users\n",
    "\"\"\"\n",
    "\n",
    "print(\"   Querying for users with ad exposure (impressions or clicks)...\")\n",
    "df_treatment_users = pd.read_sql(treatment_query, conn)\n",
    "df_treatment_users.columns = [col.lower() for col in df_treatment_users.columns]\n",
    "treatment_users_set = set(df_treatment_users['user_id'])\n",
    "\n",
    "print(f\"✅ Found {len(treatment_users_set):,} unique users with ad exposure\")\n",
    "print(f\"   Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Add treatment indicator to features dataframe\n",
    "# Match treatment users with our analysis cohort\n",
    "cohort_user_ids = set(df_period1_features['user_id'])\n",
    "treatment_in_cohort = treatment_users_set.intersection(cohort_user_ids)\n",
    "\n",
    "df_period1_features['is_treated'] = df_period1_features['user_id'].isin(treatment_in_cohort).astype(int)\n",
    "\n",
    "print(f\"\\nTreatment assignment in analysis cohort:\")\n",
    "print(f\"   Treated (D=1): {df_period1_features['is_treated'].sum():,} users\")\n",
    "print(f\"   Control (D=0): {(1 - df_period1_features['is_treated']).sum():,} users\")\n",
    "print(f\"   Treatment rate: {df_period1_features['is_treated'].mean():.4f}\")\n",
    "\n",
    "# Sanity check\n",
    "if df_period1_features['is_treated'].sum() == 0:\n",
    "    print(\"\\n⚠️ WARNING: No treated users found in the analysis cohort!\")\n",
    "    print(\"   This could mean all purchasers are in the holdout group.\")\n",
    "elif df_period1_features['is_treated'].mean() > 0.99:\n",
    "    print(\"\\n⚠️ WARNING: Almost all users are treated!\")\n",
    "    print(\"   Control group is very small - estimates may be unstable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Period 2 Outcomes\n",
    "\n",
    "Check which users made purchases in Period 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Period 2 purchase outcomes\n",
    "print(\"Extracting Period 2 outcomes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "period2_outcomes_query = f\"\"\"\n",
    "SELECT DISTINCT USER_ID, 1 AS purchased_p2\n",
    "FROM PURCHASES\n",
    "WHERE PURCHASED_AT >= '{PERIOD2_START}'\n",
    "  AND PURCHASED_AT <= '{PERIOD2_END}'\n",
    "\"\"\"\n",
    "\n",
    "df_period2_outcomes = pd.read_sql(period2_outcomes_query, conn)\n",
    "# Convert columns to lowercase\n",
    "df_period2_outcomes.columns = [col.lower() for col in df_period2_outcomes.columns]\n",
    "period2_purchasers = set(df_period2_outcomes['user_id'])\n",
    "\n",
    "print(f\"✅ Found {len(period2_purchasers):,} users who purchased in Period 2\")\n",
    "print(f\"   Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Add outcome to features dataframe\n",
    "df_period1_features['purchased_p2'] = df_period1_features['user_id'].isin(period2_purchasers).astype(int)\n",
    "\n",
    "print(f\"\\nPeriod 2 purchase rates:\")\n",
    "print(f\"   Overall: {df_period1_features['purchased_p2'].mean():.4f}\")\n",
    "print(f\"   Treated: {df_period1_features[df_period1_features['is_treated']==1]['purchased_p2'].mean():.4f}\")\n",
    "print(f\"   Control: {df_period1_features[df_period1_features['is_treated']==0]['purchased_p2'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Aggregate Counts by Segment\n",
    "\n",
    "Create the aggregated counts table needed for bounded estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_segment(df, segment_col, segment_type):\n",
    "    \"\"\"\n",
    "    Aggregate counts for a specific segmentation variable.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - segment_type: Type of segmentation\n",
    "    - segment_name: Name of the segment\n",
    "    - N_1: Count of treated users\n",
    "    - n_11: Count of treated users who purchased in P2\n",
    "    - n_01: Count of control users who purchased in P2\n",
    "    - N_0: Count of control users\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for segment_value in df[segment_col].dropna().unique():\n",
    "        segment_df = df[df[segment_col] == segment_value]\n",
    "        \n",
    "        treated = segment_df[segment_df['is_treated'] == 1]\n",
    "        control = segment_df[segment_df['is_treated'] == 0]\n",
    "        \n",
    "        N_1 = len(treated)\n",
    "        n_11 = treated['purchased_p2'].sum()\n",
    "        N_0 = len(control)\n",
    "        n_01 = control['purchased_p2'].sum()\n",
    "        \n",
    "        results.append({\n",
    "            'segment_type': segment_type,\n",
    "            'segment_name': str(segment_value),\n",
    "            'N_1': N_1,\n",
    "            'n_11': n_11,\n",
    "            'N_0': N_0,\n",
    "            'n_01': n_01\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Aggregate for each segment type\n",
    "print(\"Aggregating counts by segment...\")\n",
    "\n",
    "segment_configs = [\n",
    "    ('spend_quintile', 'Spend_Quintile'),\n",
    "    ('frequency_group', 'Purchase_Frequency'),\n",
    "    ('tenure_group', 'Tenure'),\n",
    "    ('aov_quintile', 'AOV_Quintile')\n",
    "]\n",
    "\n",
    "all_aggregates = []\n",
    "\n",
    "for segment_col, segment_type in tqdm(segment_configs, desc=\"Processing segments\"):\n",
    "    segment_agg = aggregate_by_segment(df_period1_features, segment_col, segment_type)\n",
    "    all_aggregates.append(segment_agg)\n",
    "    print(f\"\\n{segment_type}:\")\n",
    "    print(segment_agg.to_string(index=False))\n",
    "\n",
    "# Add overall cohort\n",
    "overall_treated = df_period1_features[df_period1_features['is_treated'] == 1]\n",
    "overall_control = df_period1_features[df_period1_features['is_treated'] == 0]\n",
    "\n",
    "overall_agg = pd.DataFrame([{\n",
    "    'segment_type': 'Overall_Cohort',\n",
    "    'segment_name': 'All_Users',\n",
    "    'N_1': len(overall_treated),\n",
    "    'n_11': overall_treated['purchased_p2'].sum(),\n",
    "    'N_0': len(overall_control),\n",
    "    'n_01': overall_control['purchased_p2'].sum()\n",
    "}])\n",
    "\n",
    "all_aggregates.append(overall_agg)\n",
    "\n",
    "# Combine all aggregates\n",
    "df_final_aggregates = pd.concat(all_aggregates, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL AGGREGATED COUNTS TABLE:\")\n",
    "print(\"=\"*80)\n",
    "print(df_final_aggregates.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated counts\n",
    "output_file = 'causal_estimation_counts.csv'\n",
    "df_final_aggregates.to_csv(output_file, index=False)\n",
    "print(f\"✅ Saved aggregated counts to {output_file}\")\n",
    "\n",
    "# Save detailed user-level data for validation\n",
    "user_level_file = 'user_level_features.parquet'\n",
    "df_period1_features.to_parquet(user_level_file, index=False)\n",
    "print(f\"✅ Saved user-level features to {user_level_file}\")\n",
    "\n",
    "# Generate summary report\n",
    "with open('data_extraction_summary.txt', 'w') as f:\n",
    "    f.write(\"DATA EXTRACTION SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Period 1 (Segmentation): {PERIOD1_START} to {PERIOD1_END}\\n\")\n",
    "    f.write(f\"Period 2 (Outcomes): {PERIOD2_START} to {PERIOD2_END}\\n\\n\")\n",
    "    \n",
    "    f.write(\"COHORT STATISTICS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Total users in analysis cohort: {len(df_period1_features):,}\\n\")\n",
    "    f.write(f\"Treated users (D=1): {df_period1_features['is_treated'].sum():,}\\n\")\n",
    "    f.write(f\"Control users (D=0): {(1 - df_period1_features['is_treated']).sum():,}\\n\")\n",
    "    f.write(f\"Treatment rate: {df_period1_features['is_treated'].mean():.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"OUTCOME STATISTICS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Overall P2 purchase rate: {df_period1_features['purchased_p2'].mean():.4f}\\n\")\n",
    "    f.write(f\"Treated P2 purchase rate: {df_period1_features[df_period1_features['is_treated']==1]['purchased_p2'].mean():.4f}\\n\")\n",
    "    f.write(f\"Control P2 purchase rate: {df_period1_features[df_period1_features['is_treated']==0]['purchased_p2'].mean():.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"AGGREGATED COUNTS BY SEGMENT\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(df_final_aggregates.to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ Data extraction complete!\")\n",
    "print(f\"   - Aggregated counts: {output_file}\")\n",
    "print(f\"   - User-level features: {user_level_file}\")\n",
    "print(f\"   - Summary report: data_extraction_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Stock Panel Construction\n",
    "\n",
    "This notebook processes the raw data from notebook 01 and engineers the ad stock features for causal analysis.\n",
    "\n",
    "**Key Features:**\n",
    "- Macro-session construction with 3-day inactivity threshold\n",
    "- Ad stock calculation with exponential decay (λ=0.15, half-life ~4.5 days)\n",
    "- User-vendor-product-session panel construction\n",
    "- Comprehensive feature engineering for fixed effects LPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize timing\n",
    "start_time = datetime.now()\n",
    "print(f\"Script started at: {start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# Session Definition Parameters\n",
    "SESSION_GAP_DAYS = 3  # New session starts after 3 days of user inactivity\n",
    "\n",
    "# Ad Stock Parameters\n",
    "AD_STOCK_DECAY_LAMBDA = 0.15  # Decay rate (half-life of ~4.5 days)\n",
    "AD_STOCK_WEIGHT_IMPRESSION = 0.5  # Weight for impressions\n",
    "AD_STOCK_WEIGHT_CLICK = 1.0  # Weight for clicks (higher signal)\n",
    "\n",
    "# Prior History Window\n",
    "PRIOR_USER_HISTORY_DAYS = 30  # Days of user history to calculate before each session\n",
    "\n",
    "# Data Quality Parameters\n",
    "WINSORIZE_PERCENTILE = 99  # Cap extreme values at this percentile\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('./data')\n",
    "OUTPUT_FILE = DATA_DIR / 'adstock_analysis_panel.parquet'\n",
    "METADATA_FILE = DATA_DIR / 'adstock_panel_metadata.json'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AD STOCK CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Session Definition:\")\n",
    "print(f\"  Inactivity gap threshold: {SESSION_GAP_DAYS} days\")\n",
    "print(f\"\\nAd Stock Parameters:\")\n",
    "print(f\"  Decay λ: {AD_STOCK_DECAY_LAMBDA} (half-life: {np.log(2)/AD_STOCK_DECAY_LAMBDA:.1f} days)\")\n",
    "print(f\"  Impression weight: {AD_STOCK_WEIGHT_IMPRESSION}\")\n",
    "print(f\"  Click weight: {AD_STOCK_WEIGHT_CLICK}\")\n",
    "print(f\"\\nOther Parameters:\")\n",
    "print(f\"  Prior history window: {PRIOR_USER_HISTORY_DAYS} days\")\n",
    "print(f\"  Winsorizing at: {WINSORIZE_PERCENTILE}th percentile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = DATA_DIR / 'metadata_adstock.json'\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"\\nLoading data from extraction: {metadata['timestamp']}\")\n",
    "    print(f\"Period: {metadata['analysis_start_date']} to {metadata['analysis_end_date']}\")\n",
    "    print(f\"Power vendors: {metadata['n_power_vendors']:,}\")\n",
    "    print(f\"Power users: {metadata['n_power_users']:,}\")\n",
    "else:\n",
    "    print(\"Warning: No metadata file found. Make sure to run notebook 01 first.\")\n",
    "\n",
    "# Load all parquet files using Polars\n",
    "data_files = {\n",
    "    'auctions_users': 'auctions_users_adstock.parquet',\n",
    "    'impressions': 'impressions_adstock.parquet',\n",
    "    'clicks': 'clicks_adstock.parquet',\n",
    "    'purchases': 'purchases_adstock.parquet',\n",
    "    'catalog': 'catalog_adstock.parquet',\n",
    "    'power_vendors': 'power_vendors.parquet',\n",
    "    'power_users': 'power_users.parquet'\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for name, filename in tqdm(data_files.items(), desc=\"Loading data files\"):\n",
    "    filepath = DATA_DIR / filename\n",
    "    if filepath.exists():\n",
    "        data[name] = pl.read_parquet(filepath)\n",
    "        print(f\"  {name}: {data[name].height:,} rows loaded\")\n",
    "    else:\n",
    "        print(f\"  Warning: {filename} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Unified Event Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: CREATE UNIFIED EVENT STREAM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "events_list = []\n",
    "\n",
    "# Process impressions\n",
    "print(\"\\nProcessing impressions...\")\n",
    "impressions_events = data['impressions'].select([\n",
    "    pl.col('user_id'),\n",
    "    pl.col('vendor_id'),\n",
    "    pl.col('product_id'),\n",
    "    pl.col('impression_time').alias('timestamp'),\n",
    "    pl.lit('impression').alias('event_type'),\n",
    "    pl.col('auction_id')\n",
    "])\n",
    "events_list.append(impressions_events)\n",
    "\n",
    "# Process clicks\n",
    "print(\"Processing clicks...\")\n",
    "clicks_events = data['clicks'].select([\n",
    "    pl.col('user_id'),\n",
    "    pl.col('vendor_id'),\n",
    "    pl.col('product_id'),\n",
    "    pl.col('click_time').alias('timestamp'),\n",
    "    pl.lit('click').alias('event_type'),\n",
    "    pl.col('auction_id')\n",
    "])\n",
    "events_list.append(clicks_events)\n",
    "\n",
    "# Process purchases\n",
    "print(\"Processing purchases...\")\n",
    "purchases_events = data['purchases'].with_columns([\n",
    "    (pl.col('quantity') * pl.col('unit_price')).alias('revenue')\n",
    "]).select([\n",
    "    pl.col('user_id'),\n",
    "    pl.lit(None).cast(pl.Utf8).alias('vendor_id'),  # Purchases don't have vendor\n",
    "    pl.col('product_id'),\n",
    "    pl.col('purchase_time').alias('timestamp'),\n",
    "    pl.lit('purchase').alias('event_type'),\n",
    "    pl.lit(None).cast(pl.Utf8).alias('auction_id'),\n",
    "    pl.col('revenue').cast(pl.Int64)\n",
    "])\n",
    "events_list.append(purchases_events)\n",
    "\n",
    "# Combine all events\n",
    "print(\"\\nCombining all events...\")\n",
    "all_events = pl.concat(events_list, how=\"diagonal\")\n",
    "\n",
    "# Sort by user and time\n",
    "print(\"Sorting events chronologically...\")\n",
    "all_events = all_events.sort(['user_id', 'timestamp'])\n",
    "\n",
    "print(f\"\\nTotal events in stream: {all_events.height:,}\")\n",
    "print(f\"Unique users: {all_events['user_id'].n_unique():,}\")\n",
    "print(f\"Unique vendors: {all_events.filter(pl.col('vendor_id').is_not_null())['vendor_id'].n_unique():,}\")\n",
    "print(f\"Date range: {all_events['timestamp'].min()} to {all_events['timestamp'].max()}\")\n",
    "\n",
    "# Event type breakdown\n",
    "event_counts = all_events.group_by('event_type').count().sort('count', descending=True)\n",
    "print(\"\\nEvent type breakdown:\")\n",
    "for row in event_counts.iter_rows():\n",
    "    print(f\"  {row[0]}: {row[1]:,}\")\n",
    "\n",
    "# Free memory\n",
    "del events_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sessionization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PHASE 3: SESSIONIZATION (using {SESSION_GAP_DAYS}-day gap)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Calculating time gaps between user events...\")\n",
    "\n",
    "# Calculate time gaps between consecutive events for each user\n",
    "all_events = all_events.with_columns(\n",
    "    pl.col('timestamp').diff().over('user_id').alias('time_since_last_event')\n",
    ")\n",
    "\n",
    "# Identify session breakpoints\n",
    "print(\"Identifying session breakpoints...\")\n",
    "all_events = all_events.with_columns(\n",
    "    (\n",
    "        pl.col('time_since_last_event').is_null() |\n",
    "        (pl.col('time_since_last_event').dt.total_seconds() > (SESSION_GAP_DAYS * 86400))\n",
    "    ).alias('is_new_session')\n",
    ")\n",
    "\n",
    "# Assign unique session IDs\n",
    "print(\"Assigning unique session IDs...\")\n",
    "all_events = all_events.with_columns(\n",
    "    pl.col('is_new_session').cast(pl.Int32).cum_sum().over('user_id').alias('session_num')\n",
    ")\n",
    "\n",
    "# Create final macro_session_id\n",
    "all_events = all_events.with_columns(\n",
    "    (pl.col('user_id') + '_s_' + pl.col('session_num').cast(pl.Utf8)).alias('macro_session_id')\n",
    ")\n",
    "\n",
    "# Clean up intermediate columns\n",
    "all_events = all_events.drop(['time_since_last_event', 'is_new_session', 'session_num'])\n",
    "\n",
    "# Calculate session metadata\n",
    "session_times = all_events.group_by('macro_session_id').agg([\n",
    "    pl.col('timestamp').min().alias('session_start'),\n",
    "    pl.col('timestamp').max().alias('session_end'),\n",
    "    pl.col('user_id').first()\n",
    "])\n",
    "\n",
    "print(f\"\\n✓ Successfully created {session_times.height:,} unique sessions\")\n",
    "\n",
    "# Session duration statistics\n",
    "session_times = session_times.with_columns(\n",
    "    ((pl.col('session_end') - pl.col('session_start')).dt.total_seconds() / 3600).alias('duration_hours')\n",
    ")\n",
    "avg_duration = session_times['duration_hours'].mean()\n",
    "median_duration = session_times['duration_hours'].median()\n",
    "print(f\"  Average session duration: {avg_duration:.1f} hours\")\n",
    "print(f\"  Median session duration: {median_duration:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Ad Stock\n",
    "\n",
    "This is the critical step where we calculate the decayed carryover effects of past advertising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: AD STOCK CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nCalculating ad stock with parameters:\")\n",
    "print(f\"  Decay λ = {AD_STOCK_DECAY_LAMBDA}\")\n",
    "print(f\"  Impression weight = {AD_STOCK_WEIGHT_IMPRESSION}\")\n",
    "print(f\"  Click weight = {AD_STOCK_WEIGHT_CLICK}\")\n",
    "\n",
    "# Filter to only ad events (impressions and clicks) for ad stock calculation\n",
    "ad_events = all_events.filter(\n",
    "    pl.col('event_type').is_in(['impression', 'click']) & \n",
    "    pl.col('vendor_id').is_not_null()\n",
    ")\n",
    "\n",
    "print(f\"\\nAd events for stock calculation: {ad_events.height:,}\")\n",
    "\n",
    "# For each session, calculate ad stock from all prior events\n",
    "print(\"\\nCalculating ad stock for each user-vendor-session combination...\")\n",
    "\n",
    "# Get unique user-vendor-session combinations\n",
    "user_vendor_sessions = ad_events.group_by(['user_id', 'vendor_id', 'macro_session_id']).agg(\n",
    "    pl.col('timestamp').min().alias('session_start')\n",
    ").sort(['user_id', 'vendor_id', 'session_start'])\n",
    "\n",
    "print(f\"  Unique (user, vendor, session) combinations: {user_vendor_sessions.height:,}\")\n",
    "\n",
    "# Function to calculate ad stock for a batch of sessions\n",
    "def calculate_ad_stock_vectorized(ad_events_df, sessions_df):\n",
    "    \"\"\"Calculate ad stock using vectorized operations.\"\"\"\n",
    "    \n",
    "    # Join sessions with all prior ad events\n",
    "    joined = sessions_df.join(\n",
    "        ad_events_df.select(['user_id', 'vendor_id', 'timestamp', 'event_type']),\n",
    "        on=['user_id', 'vendor_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Filter to only prior events\n",
    "    prior_events = joined.filter(\n",
    "        pl.col('timestamp') < pl.col('session_start')\n",
    "    )\n",
    "    \n",
    "    # Calculate time difference and decay\n",
    "    prior_events = prior_events.with_columns([\n",
    "        ((pl.col('session_start') - pl.col('timestamp')).dt.total_seconds() / 86400).alias('days_ago'),\n",
    "    ])\n",
    "    \n",
    "    prior_events = prior_events.with_columns([\n",
    "        (-AD_STOCK_DECAY_LAMBDA * pl.col('days_ago')).exp().alias('decay_factor'),\n",
    "        pl.when(pl.col('event_type') == 'impression')\n",
    "          .then(AD_STOCK_WEIGHT_IMPRESSION)\n",
    "          .otherwise(AD_STOCK_WEIGHT_CLICK)\n",
    "          .alias('event_weight')\n",
    "    ])\n",
    "    \n",
    "    # Calculate weighted decayed value\n",
    "    prior_events = prior_events.with_columns(\n",
    "        (pl.col('event_weight') * pl.col('decay_factor')).alias('ad_stock_contribution')\n",
    "    )\n",
    "    \n",
    "    # Aggregate to get total ad stock\n",
    "    ad_stock_by_session = prior_events.group_by(['user_id', 'vendor_id', 'macro_session_id']).agg(\n",
    "        pl.col('ad_stock_contribution').sum().alias('ad_stock'),\n",
    "        pl.col('event_type').filter(pl.col('event_type') == 'impression').count().alias('prior_impressions'),\n",
    "        pl.col('event_type').filter(pl.col('event_type') == 'click').count().alias('prior_clicks')\n",
    "    )\n",
    "    \n",
    "    return ad_stock_by_session\n",
    "\n",
    "# Calculate ad stock in batches to manage memory\n",
    "print(\"\\nCalculating ad stock (this may take a few minutes)...\")\n",
    "ad_stock_results = calculate_ad_stock_vectorized(ad_events, user_vendor_sessions)\n",
    "\n",
    "# Add ad stock back to the session metadata\n",
    "user_vendor_sessions = user_vendor_sessions.join(\n",
    "    ad_stock_results,\n",
    "    on=['user_id', 'vendor_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill nulls with 0 for sessions with no prior history\n",
    "user_vendor_sessions = user_vendor_sessions.with_columns([\n",
    "    pl.col('ad_stock').fill_null(0),\n",
    "    pl.col('prior_impressions').fill_null(0),\n",
    "    pl.col('prior_clicks').fill_null(0)\n",
    "])\n",
    "\n",
    "print(f\"\\n✓ Ad stock calculation complete\")\n",
    "print(f\"  Sessions with ad stock > 0: {(user_vendor_sessions['ad_stock'] > 0).sum():,}\")\n",
    "print(f\"  Mean ad stock: {user_vendor_sessions['ad_stock'].mean():.4f}\")\n",
    "print(f\"  Max ad stock: {user_vendor_sessions['ad_stock'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct Analysis Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5: PANEL CONSTRUCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create base panel from impressions (unit of analysis: user-vendor-product-session)\n",
    "print(\"\\nCreating base panel from impressions...\")\n",
    "\n",
    "impression_events = all_events.filter(\n",
    "    (pl.col('event_type') == 'impression') & \n",
    "    pl.col('product_id').is_not_null() &\n",
    "    pl.col('vendor_id').is_not_null()\n",
    ")\n",
    "\n",
    "# Create unique (user, vendor, product, session) combinations\n",
    "base_panel = impression_events.group_by(['user_id', 'vendor_id', 'product_id', 'macro_session_id']).agg(\n",
    "    pl.col('timestamp').min().alias('first_impression_time')\n",
    ")\n",
    "\n",
    "print(f\"  Base panel created: {base_panel.height:,} observations\")\n",
    "print(f\"  Unique users: {base_panel['user_id'].n_unique():,}\")\n",
    "print(f\"  Unique vendors: {base_panel['vendor_id'].n_unique():,}\")\n",
    "print(f\"  Unique products: {base_panel['product_id'].n_unique():,}\")\n",
    "print(f\"  Unique sessions: {base_panel['macro_session_id'].n_unique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Within-Session Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Adding within-session treatment variables ---\")\n",
    "\n",
    "# Count impressions and clicks for each product within each session\n",
    "within_session_impressions = all_events.filter(\n",
    "    pl.col('event_type') == 'impression'\n",
    ").group_by(['user_id', 'vendor_id', 'product_id', 'macro_session_id']).agg(\n",
    "    pl.count().alias('impressions_on_product')\n",
    ")\n",
    "\n",
    "within_session_clicks = all_events.filter(\n",
    "    pl.col('event_type') == 'click'\n",
    ").group_by(['user_id', 'vendor_id', 'product_id', 'macro_session_id']).agg(\n",
    "    pl.count().alias('clicks_on_product')\n",
    ")\n",
    "\n",
    "# Merge into base panel\n",
    "panel = base_panel.join(\n",
    "    within_session_impressions,\n",
    "    on=['user_id', 'vendor_id', 'product_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "panel = panel.join(\n",
    "    within_session_clicks,\n",
    "    on=['user_id', 'vendor_id', 'product_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill nulls\n",
    "panel = panel.with_columns([\n",
    "    pl.col('impressions_on_product').fill_null(0),\n",
    "    pl.col('clicks_on_product').fill_null(0)\n",
    "])\n",
    "\n",
    "print(f\"  Products with impressions: {(panel['impressions_on_product'] > 0).sum():,}\")\n",
    "print(f\"  Products with clicks: {(panel['clicks_on_product'] > 0).sum():,}\")\n",
    "print(f\"  Click-through rate: {(panel['clicks_on_product'] > 0).sum() / panel.height * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add Purchase Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Adding purchase outcomes ---\")\n",
    "\n",
    "# Get purchases within sessions\n",
    "purchase_events = all_events.filter(\n",
    "    pl.col('event_type') == 'purchase'\n",
    ").select(['user_id', 'product_id', 'macro_session_id', 'revenue'])\n",
    "\n",
    "# Aggregate purchases by product within session\n",
    "purchases_by_product = purchase_events.group_by(['user_id', 'product_id', 'macro_session_id']).agg([\n",
    "    pl.count().alias('purchases_of_product'),\n",
    "    pl.col('revenue').sum().alias('revenue_from_product')\n",
    "])\n",
    "\n",
    "# Join to panel (note: vendor is not in purchases, so we join on user-product-session)\n",
    "panel = panel.join(\n",
    "    purchases_by_product,\n",
    "    on=['user_id', 'product_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create binary purchase outcome and fill nulls\n",
    "panel = panel.with_columns([\n",
    "    (pl.col('purchases_of_product') > 0).cast(pl.Int8).fill_null(0).alias('purchased'),\n",
    "    pl.col('purchases_of_product').fill_null(0),\n",
    "    pl.col('revenue_from_product').fill_null(0)\n",
    "])\n",
    "\n",
    "print(f\"  Sessions with purchases: {panel['purchased'].sum():,}\")\n",
    "print(f\"  Purchase rate: {panel['purchased'].mean() * 100:.2f}%\")\n",
    "print(f\"  Total revenue: ${panel['revenue_from_product'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge Ad Stock Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Merging ad stock features ---\")\n",
    "\n",
    "# Join ad stock data to panel\n",
    "panel = panel.join(\n",
    "    user_vendor_sessions.select(['user_id', 'vendor_id', 'macro_session_id', 'ad_stock', \n",
    "                                 'prior_impressions', 'prior_clicks']),\n",
    "    on=['user_id', 'vendor_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill nulls for sessions with no prior history\n",
    "panel = panel.with_columns([\n",
    "    pl.col('ad_stock').fill_null(0),\n",
    "    pl.col('prior_impressions').fill_null(0),\n",
    "    pl.col('prior_clicks').fill_null(0)\n",
    "])\n",
    "\n",
    "print(f\"  Observations with ad stock > 0: {(panel['ad_stock'] > 0).sum():,}\")\n",
    "print(f\"  Mean ad stock: {panel['ad_stock'].mean():.4f}\")\n",
    "print(f\"  Correlation between ad stock and purchase: {panel['ad_stock'].corr(panel['purchased']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Add Control Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Adding control variables ---\")\n",
    "\n",
    "# Add session metadata\n",
    "panel = panel.join(\n",
    "    session_times.select(['macro_session_id', 'session_start', 'session_end', 'duration_hours']),\n",
    "    on='macro_session_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add catalog information (price)\n",
    "catalog_features = data['catalog'].select(['product_id', 'catalog_price', 'brand', 'department_id'])\n",
    "panel = panel.join(catalog_features, on='product_id', how='left')\n",
    "\n",
    "# Create time-based fixed effects identifiers\n",
    "panel = panel.with_columns([\n",
    "    pl.col('session_start').dt.week().alias('week_of_year'),\n",
    "    pl.col('session_start').dt.year().alias('year'),\n",
    "    (pl.col('catalog_price').fill_null(pl.col('catalog_price').median())).alias('price')\n",
    "])\n",
    "\n",
    "# Log transformations\n",
    "panel = panel.with_columns([\n",
    "    (pl.col('price') + 1).log().alias('log_price'),\n",
    "    (pl.col('duration_hours') + 1).log().alias('log_duration')\n",
    "])\n",
    "\n",
    "# Winsorize extreme durations\n",
    "duration_cap = panel['duration_hours'].quantile(WINSORIZE_PERCENTILE / 100)\n",
    "panel = panel.with_columns(\n",
    "    pl.col('duration_hours').clip(upper_bound=duration_cap).alias('duration_winsorized')\n",
    ")\n",
    "\n",
    "# Calculate session-level activity metrics\n",
    "session_activity = all_events.group_by(['user_id', 'macro_session_id']).agg([\n",
    "    pl.col('product_id').n_unique().alias('distinct_products_viewed'),\n",
    "    pl.col('event_type').count().alias('total_events_in_session')\n",
    "])\n",
    "\n",
    "panel = panel.join(\n",
    "    session_activity,\n",
    "    on=['user_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"  Control variables added successfully\")\n",
    "print(f\"  Final panel shape: {panel.height:,} rows × {panel.width} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Add Prior 30-Day User Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Calculating prior 30-day user activity ---\")\n",
    "\n",
    "# Get unique user-session combinations with start times\n",
    "user_sessions = panel.select(['user_id', 'macro_session_id', 'session_start']).unique()\n",
    "\n",
    "# Prepare purchase events for historical calculation\n",
    "historical_purchases = all_events.filter(\n",
    "    pl.col('event_type') == 'purchase'\n",
    ").select(['user_id', 'timestamp'])\n",
    "\n",
    "# Join to find all purchases before each session\n",
    "print(\"  Finding historical purchases...\")\n",
    "historical = user_sessions.join(\n",
    "    historical_purchases,\n",
    "    on='user_id',\n",
    "    how='inner'\n",
    ").filter(\n",
    "    pl.col('timestamp') < pl.col('session_start')\n",
    ")\n",
    "\n",
    "# Filter to 30-day window\n",
    "historical_30d = historical.filter(\n",
    "    pl.col('timestamp') >= (pl.col('session_start') - pl.duration(days=PRIOR_USER_HISTORY_DAYS))\n",
    ")\n",
    "\n",
    "# Aggregate by session\n",
    "prior_30d_purchases = historical_30d.group_by(['user_id', 'macro_session_id']).agg(\n",
    "    pl.count().alias('prior_30d_purchases')\n",
    ")\n",
    "\n",
    "# Join back to panel\n",
    "panel = panel.join(\n",
    "    prior_30d_purchases,\n",
    "    on=['user_id', 'macro_session_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "panel = panel.with_columns(\n",
    "    pl.col('prior_30d_purchases').fill_null(0)\n",
    ")\n",
    "\n",
    "print(f\"  Prior 30-day purchases calculated\")\n",
    "print(f\"  Mean prior purchases: {panel['prior_30d_purchases'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Feature Selection and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Finalizing dataset ---\")\n",
    "\n",
    "# Select final columns for the model\n",
    "FINAL_COLUMNS = [\n",
    "    # Identifiers\n",
    "    'user_id',\n",
    "    'vendor_id',\n",
    "    'product_id',\n",
    "    'macro_session_id',\n",
    "    \n",
    "    # Outcome\n",
    "    'purchased',\n",
    "    'revenue_from_product',\n",
    "    \n",
    "    # Treatment variables (within-session)\n",
    "    'impressions_on_product',\n",
    "    'clicks_on_product',\n",
    "    \n",
    "    # Ad stock (key variable)\n",
    "    'ad_stock',\n",
    "    'prior_impressions',\n",
    "    'prior_clicks',\n",
    "    \n",
    "    # Control variables\n",
    "    'log_price',\n",
    "    'duration_winsorized',\n",
    "    'distinct_products_viewed',\n",
    "    'prior_30d_purchases',\n",
    "    \n",
    "    # Fixed effects identifiers\n",
    "    'week_of_year',\n",
    "    'year',\n",
    "    \n",
    "    # Additional metadata\n",
    "    'session_start',\n",
    "    'price',\n",
    "    'brand',\n",
    "    'department_id'\n",
    "]\n",
    "\n",
    "# Select only columns that exist\n",
    "available_columns = [col for col in FINAL_COLUMNS if col in panel.columns]\n",
    "final_panel = panel.select(available_columns)\n",
    "\n",
    "print(f\"\\n✓ Final panel prepared with {len(available_columns)} columns\")\n",
    "print(f\"  Shape: {final_panel.height:,} rows × {final_panel.width} columns\")\n",
    "\n",
    "# Save the panel\n",
    "final_panel.write_parquet(OUTPUT_FILE)\n",
    "print(f\"\\n✓ Panel saved to {OUTPUT_FILE}\")\n",
    "print(f\"  File size: {OUTPUT_FILE.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create Summary Statistics and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = {\n",
    "    'panel_shape': {'rows': final_panel.height, 'columns': final_panel.width},\n",
    "    'unique_counts': {\n",
    "        'users': final_panel['user_id'].n_unique(),\n",
    "        'vendors': final_panel['vendor_id'].n_unique(),\n",
    "        'products': final_panel['product_id'].n_unique(),\n",
    "        'sessions': final_panel['macro_session_id'].n_unique()\n",
    "    },\n",
    "    'outcome_stats': {\n",
    "        'purchase_rate': float(final_panel['purchased'].mean()),\n",
    "        'total_revenue': float(final_panel['revenue_from_product'].sum()),\n",
    "        'mean_revenue_if_purchased': float(\n",
    "            final_panel.filter(pl.col('purchased') == 1)['revenue_from_product'].mean()\n",
    "        ) if final_panel.filter(pl.col('purchased') == 1).height > 0 else 0\n",
    "    },\n",
    "    'treatment_stats': {\n",
    "        'mean_impressions': float(final_panel['impressions_on_product'].mean()),\n",
    "        'mean_clicks': float(final_panel['clicks_on_product'].mean()),\n",
    "        'click_through_rate': float((final_panel['clicks_on_product'] > 0).mean())\n",
    "    },\n",
    "    'ad_stock_stats': {\n",
    "        'mean_ad_stock': float(final_panel['ad_stock'].mean()),\n",
    "        'pct_with_ad_stock': float((final_panel['ad_stock'] > 0).mean()),\n",
    "        'correlation_with_purchase': float(final_panel['ad_stock'].corr(final_panel['purchased'])),\n",
    "        '25th_percentile': float(final_panel['ad_stock'].quantile(0.25)),\n",
    "        '50th_percentile': float(final_panel['ad_stock'].quantile(0.50)),\n",
    "        '75th_percentile': float(final_panel['ad_stock'].quantile(0.75)),\n",
    "        '95th_percentile': float(final_panel['ad_stock'].quantile(0.95))\n",
    "    },\n",
    "    'parameters': {\n",
    "        'session_gap_days': SESSION_GAP_DAYS,\n",
    "        'ad_stock_decay_lambda': AD_STOCK_DECAY_LAMBDA,\n",
    "        'ad_stock_weight_impression': AD_STOCK_WEIGHT_IMPRESSION,\n",
    "        'ad_stock_weight_click': AD_STOCK_WEIGHT_CLICK,\n",
    "        'prior_history_days': PRIOR_USER_HISTORY_DAYS,\n",
    "        'winsorize_percentile': WINSORIZE_PERCENTILE\n",
    "    },\n",
    "    'generated_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPanel Statistics:\")\n",
    "print(f\"  Observations: {summary_stats['panel_shape']['rows']:,}\")\n",
    "print(f\"  Unique users: {summary_stats['unique_counts']['users']:,}\")\n",
    "print(f\"  Unique vendors: {summary_stats['unique_counts']['vendors']:,}\")\n",
    "print(f\"  Purchase rate: {summary_stats['outcome_stats']['purchase_rate']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nAd Stock Distribution:\")\n",
    "print(f\"  Mean: {summary_stats['ad_stock_stats']['mean_ad_stock']:.4f}\")\n",
    "print(f\"  Median: {summary_stats['ad_stock_stats']['50th_percentile']:.4f}\")\n",
    "print(f\"  75th percentile: {summary_stats['ad_stock_stats']['75th_percentile']:.4f}\")\n",
    "print(f\"  % with ad stock > 0: {summary_stats['ad_stock_stats']['pct_with_ad_stock']*100:.1f}%\")\n",
    "print(f\"  Correlation with purchase: {summary_stats['ad_stock_stats']['correlation_with_purchase']:.4f}\")\n",
    "\n",
    "# Save metadata\n",
    "with open(METADATA_FILE, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"\\n✓ Metadata saved to {METADATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Specification Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINEAR PROBABILITY MODEL SPECIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Equation:\n",
    "Purchased_uvps = β₁·AdStock_uvs + β₂·Clicks_uvps + β₃·Impressions_uvps + X'_uvps·Γ + α_u + δ_v + γ_t + ε_uvps\n",
    "\n",
    "Where:\n",
    "  - Purchased_uvps: Binary outcome (0/1)\n",
    "  - AdStock_uvs: Decayed carryover effect from past sessions\n",
    "  - Clicks_uvps: Within-session click count\n",
    "  - Impressions_uvps: Within-session impression count\n",
    "  - X'_uvps: Control variables (price, duration, etc.)\n",
    "  - α_u: User fixed effects\n",
    "  - δ_v: Vendor fixed effects\n",
    "  - γ_t: Week fixed effects\n",
    "\n",
    "Key Coefficient Interpretations:\n",
    "  - β₁: Change in purchase probability per unit increase in ad stock (carryover effect)\n",
    "  - β₂: Change in purchase probability per additional click (immediate effect)\n",
    "  - β₃: Change in purchase probability per additional impression (immediate effect)\n",
    "\n",
    "Next Steps:\n",
    "  1. Estimate model using pyfixest or similar high-dimensional FE package\n",
    "  2. Cluster standard errors at user level\n",
    "  3. Test robustness with different decay parameters\n",
    "  4. Calculate marginal effects and elasticities\n",
    "\"\"\")\n",
    "\n",
    "# Calculate runtime\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f\"\\nTotal runtime: {runtime}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PANEL CONSTRUCTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
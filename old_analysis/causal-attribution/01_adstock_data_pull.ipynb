{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Stock Analysis - Data Pull\n",
    "\n",
    "This notebook pulls data for ad stock model analysis, focusing on power players (top vendors and users).\n",
    "\n",
    "**Key Features:**\n",
    "- Identifies power vendors (80% of ad spend) and power users (80% of purchases)\n",
    "- Uses efficient CTE-based sampling at Snowflake level\n",
    "- Standardizes all IDs for consistent joining\n",
    "- Saves data as Parquet files for efficient processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Suppress pandas SQLAlchemy warning\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    message='pandas only supports SQLAlchemy connectable.*'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Analysis period: 2024-09-02 to 2025-09-02\n",
      "  Total days: 365\n",
      "  Sampling fraction: 0.10%\n",
      "  Power vendor percentile: 80%\n",
      "  Power user percentile: 80%\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Time windows - 365 day window for comprehensive ad stock analysis\n",
    "ANALYSIS_END_DATE = date(2025, 9, 2)\n",
    "TOTAL_PULL_DAYS = 365\n",
    "ANALYSIS_START_DATE = ANALYSIS_END_DATE - timedelta(days=TOTAL_PULL_DAYS)\n",
    "\n",
    "# Sampling parameters - focused on power players\n",
    "SAMPLING_FRACTION = 0.001  # 0.1% of users for initial testing\n",
    "POWER_VENDOR_PERCENTILE = 80  # Top vendors accounting for 80% of spend\n",
    "POWER_USER_PERCENTILE = 80    # Top users accounting for 80% of purchases\n",
    "\n",
    "# Output paths\n",
    "DATA_DIR = Path('./data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Analysis period: {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")\n",
    "print(f\"  Total days: {TOTAL_PULL_DAYS}\")\n",
    "print(f\"  Sampling fraction: {SAMPLING_FRACTION:.2%}\")\n",
    "print(f\"  Power vendor percentile: {POWER_VENDOR_PERCENTILE}%\")\n",
    "print(f\"  Power user percentile: {POWER_USER_PERCENTILE}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Snowflake connection established.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"[SUCCESS] Snowflake connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] Could not connect to Snowflake: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Power Players\n",
    "\n",
    "First, we identify the top vendors and users who drive the majority of platform activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_power_vendors(conn, start_date: str, end_date: str, percentile: int = 80) -> pd.DataFrame:\n",
    "    \"\"\"Identify top vendors accounting for X% of platform ad activity.\"\"\"\n",
    "    print(f\"\\nIdentifying power vendors (top {percentile}% of activity)...\")\n",
    "    \n",
    "    query = textwrap.dedent(f\"\"\"\n",
    "        WITH VENDOR_ACTIVITY AS (\n",
    "            SELECT\n",
    "                LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "                COUNT(DISTINCT ar.AUCTION_ID) AS total_bids,\n",
    "                SUM(CASE WHEN ar.IS_WINNER THEN 1 ELSE 0 END) AS winning_bids,\n",
    "                COUNT(DISTINCT ar.CAMPAIGN_ID) AS total_campaigns\n",
    "            FROM AUCTIONS_RESULTS ar\n",
    "            WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "            GROUP BY 1\n",
    "        ),\n",
    "        VENDOR_RANKED AS (\n",
    "            SELECT\n",
    "                VENDOR_ID,\n",
    "                total_bids,\n",
    "                winning_bids,\n",
    "                total_campaigns,\n",
    "                SUM(winning_bids) OVER (ORDER BY winning_bids DESC) AS cumulative_wins,\n",
    "                SUM(winning_bids) OVER () AS total_wins\n",
    "            FROM VENDOR_ACTIVITY\n",
    "        )\n",
    "        SELECT\n",
    "            VENDOR_ID,\n",
    "            total_bids,\n",
    "            winning_bids,\n",
    "            total_campaigns,\n",
    "            ROUND(100.0 * cumulative_wins / total_wins, 2) AS cumulative_pct\n",
    "        FROM VENDOR_RANKED\n",
    "        WHERE cumulative_pct <= {percentile}\n",
    "        ORDER BY winning_bids DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(f\"  Found {len(df):,} power vendors accounting for top {percentile}% of activity\")\n",
    "    print(f\"  Top vendor has {df.iloc[0]['winning_bids']:,} winning bids\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_power_users(conn, start_date: str, end_date: str, percentile: int = 80) -> pd.DataFrame:\n",
    "    \"\"\"Identify top users accounting for X% of platform purchases.\"\"\"\n",
    "    print(f\"\\nIdentifying power users (top {percentile}% of purchases)...\")\n",
    "    \n",
    "    query = textwrap.dedent(f\"\"\"\n",
    "        WITH USER_ACTIVITY AS (\n",
    "            SELECT\n",
    "                USER_ID,\n",
    "                COUNT(DISTINCT PURCHASE_ID) AS total_purchases,\n",
    "                SUM(QUANTITY * UNIT_PRICE) AS total_revenue,\n",
    "                COUNT(DISTINCT PRODUCT_ID) AS distinct_products\n",
    "            FROM PURCHASES\n",
    "            WHERE PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "            GROUP BY 1\n",
    "        ),\n",
    "        USER_RANKED AS (\n",
    "            SELECT\n",
    "                USER_ID,\n",
    "                total_purchases,\n",
    "                total_revenue,\n",
    "                distinct_products,\n",
    "                SUM(total_revenue) OVER (ORDER BY total_revenue DESC) AS cumulative_revenue,\n",
    "                SUM(total_revenue) OVER () AS platform_revenue\n",
    "            FROM USER_ACTIVITY\n",
    "        )\n",
    "        SELECT\n",
    "            USER_ID,\n",
    "            total_purchases,\n",
    "            total_revenue,\n",
    "            distinct_products,\n",
    "            ROUND(100.0 * cumulative_revenue / platform_revenue, 2) AS cumulative_pct\n",
    "        FROM USER_RANKED\n",
    "        WHERE cumulative_pct <= {percentile}\n",
    "        ORDER BY total_revenue DESC\n",
    "        LIMIT 50000  -- Cap for memory efficiency\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(f\"  Found {len(df):,} power users accounting for top {percentile}% of revenue\")\n",
    "    print(f\"  Top user has ${df.iloc[0]['total_revenue']:,.2f} in purchases\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying power vendors (top 80% of activity)...\n"
     ]
    }
   ],
   "source": [
    "# Identify power players\n",
    "if conn:\n",
    "    start_date_str = ANALYSIS_START_DATE.strftime('%Y-%m-%d')\n",
    "    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    power_vendors = identify_power_vendors(conn, start_date_str, end_date_str, POWER_VENDOR_PERCENTILE)\n",
    "    power_users = identify_power_users(conn, start_date_str, end_date_str, POWER_USER_PERCENTILE)\n",
    "    \n",
    "    # Save power player lists\n",
    "    power_vendors.to_parquet(DATA_DIR / 'power_vendors.parquet', index=False)\n",
    "    power_users.to_parquet(DATA_DIR / 'power_users.parquet', index=False)\n",
    "    \n",
    "    print(\"\\nPower players identified and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build CTE for Power Player Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_power_player_cte(start_date: str, end_date: str,\n                           power_vendor_percentile: int = 80,\n                           power_user_percentile: int = 80,\n                           sampling_fraction: float = 0.001) -> str:\n    \"\"\"Build CTE that identifies power players inline without passing IDs through VALUES clause.\"\"\"\n\n    total_buckets = 10000\n    selection_threshold = int(total_buckets * sampling_fraction)\n\n    return textwrap.dedent(f\"\"\"\n        WITH POWER_VENDORS AS (\n            -- Re-calculate power vendors inline to avoid SQL length limits\n            WITH VENDOR_ACTIVITY AS (\n                SELECT\n                    LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n                    SUM(CASE WHEN ar.IS_WINNER THEN 1 ELSE 0 END) AS winning_bids\n                FROM AUCTIONS_RESULTS ar\n                WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n                GROUP BY 1\n            ),\n            VENDOR_RANKED AS (\n                SELECT\n                    VENDOR_ID,\n                    winning_bids,\n                    SUM(winning_bids) OVER (ORDER BY winning_bids DESC) AS cumulative_wins,\n                    SUM(winning_bids) OVER () AS total_wins\n                FROM VENDOR_ACTIVITY\n            )\n            SELECT VENDOR_ID\n            FROM VENDOR_RANKED\n            WHERE (100.0 * cumulative_wins / NULLIF(total_wins, 0)) <= {power_vendor_percentile}\n        ),\n        POWER_USERS AS (\n            -- Re-calculate power users inline to avoid SQL length limits\n            WITH USER_ACTIVITY AS (\n                SELECT\n                    USER_ID,\n                    SUM(QUANTITY * UNIT_PRICE) AS total_revenue\n                FROM PURCHASES\n                WHERE PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n                GROUP BY 1\n            ),\n            USER_RANKED AS (\n                SELECT\n                    USER_ID,\n                    total_revenue,\n                    SUM(total_revenue) OVER (ORDER BY total_revenue DESC) AS cumulative_revenue,\n                    SUM(total_revenue) OVER () AS platform_revenue\n                FROM USER_ACTIVITY\n            )\n            SELECT USER_ID\n            FROM USER_RANKED\n            WHERE (100.0 * cumulative_revenue / NULLIF(platform_revenue, 0)) <= {power_user_percentile}\n            LIMIT 50000  -- Safety limit\n        ),\n        SAMPLED_USERS AS (\n            -- Combine power users with sampled regular users\n            SELECT USER_ID FROM POWER_USERS\n            UNION\n            -- Sample additional users based on hash\n            SELECT DISTINCT au.OPAQUE_USER_ID AS USER_ID\n            FROM AUCTIONS_USERS au\n            WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n              AND MOD(ABS(HASH(au.OPAQUE_USER_ID)), {total_buckets}) < {selection_threshold}\n              AND au.OPAQUE_USER_ID NOT IN (SELECT USER_ID FROM POWER_USERS)\n        )\n    \"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Core Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_auctions_users(conn, start_date: str, end_date: str, cte: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract AUCTIONS_USERS table with power player filtering.\"\"\"\n",
    "    print(\"\\nExtracting AUCTIONS_USERS...\")\n",
    "    \n",
    "    query = cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "            au.OPAQUE_USER_ID AS USER_ID,\n",
    "            au.CREATED_AT,\n",
    "            DATE(au.CREATED_AT) AS auction_date,\n",
    "            HOUR(au.CREATED_AT) AS auction_hour,\n",
    "            DAYOFWEEK(au.CREATED_AT) AS auction_dow,\n",
    "            WEEKOFYEAR(au.CREATED_AT) AS auction_week\n",
    "        FROM AUCTIONS_USERS au\n",
    "        JOIN SAMPLED_USERS s ON au.OPAQUE_USER_ID = s.USER_ID\n",
    "        WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY au.OPAQUE_USER_ID, au.CREATED_AT\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    print(f\"  Extracted {len(df):,} auction records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impressions(conn, start_date: str, end_date: str, cte: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract IMPRESSIONS table focusing on power players.\"\"\"\n",
    "    print(\"\\nExtracting IMPRESSIONS...\")\n",
    "    \n",
    "    query = cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            i.INTERACTION_ID AS impression_id,\n",
    "            LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            i.USER_ID,\n",
    "            LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            i.OCCURRED_AT AS impression_time\n",
    "        FROM IMPRESSIONS i\n",
    "        WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "          AND (\n",
    "              i.USER_ID IN (SELECT USER_ID FROM SAMPLED_USERS)\n",
    "              OR LOWER(REPLACE(i.VENDOR_ID, '-', '')) IN (SELECT VENDOR_ID FROM POWER_VENDORS)\n",
    "          )\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    print(f\"  Extracted {len(df):,} impression records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clicks(conn, start_date: str, end_date: str, cte: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract CLICKS table focusing on power players.\"\"\"\n",
    "    print(\"\\nExtracting CLICKS...\")\n",
    "    \n",
    "    query = cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            c.INTERACTION_ID AS click_id,\n",
    "            LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.USER_ID,\n",
    "            LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "            LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "            c.OCCURRED_AT AS click_time\n",
    "        FROM CLICKS c\n",
    "        WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "          AND (\n",
    "              c.USER_ID IN (SELECT USER_ID FROM SAMPLED_USERS)\n",
    "              OR LOWER(REPLACE(c.VENDOR_ID, '-', '')) IN (SELECT VENDOR_ID FROM POWER_VENDORS)\n",
    "          )\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    print(f\"  Extracted {len(df):,} click records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purchases(conn, start_date: str, end_date: str, cte: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract PURCHASES table for sampled users.\"\"\"\n",
    "    print(\"\\nExtracting PURCHASES...\")\n",
    "    \n",
    "    query = cte + textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            p.PURCHASE_ID,\n",
    "            p.PURCHASED_AT AS purchase_time,\n",
    "            LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            p.QUANTITY,\n",
    "            p.UNIT_PRICE,\n",
    "            p.USER_ID,\n",
    "            p.PURCHASE_LINE,\n",
    "            (p.QUANTITY * p.UNIT_PRICE) AS revenue\n",
    "        FROM PURCHASES p\n",
    "        JOIN SAMPLED_USERS s ON p.USER_ID = s.USER_ID\n",
    "        WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    print(f\"  Extracted {len(df):,} purchase records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_catalog(conn, product_ids: set) -> pd.DataFrame:\n",
    "    \"\"\"Extract CATALOG for relevant products.\"\"\"\n",
    "    print(\"\\nExtracting CATALOG...\")\n",
    "    \n",
    "    # Convert product IDs to SQL-friendly format (limit to prevent SQL too long)\n",
    "    product_ids_str = \"','\" .join(list(product_ids)[:50000])\n",
    "    \n",
    "    query = textwrap.dedent(f\"\"\"\n",
    "        SELECT\n",
    "            LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "            c.NAME AS product_name,\n",
    "            c.PRICE AS catalog_price,\n",
    "            c.ACTIVE AS is_active,\n",
    "            c.IS_DELETED,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'brand#%%'), ''), '#', 2) AS BRAND,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'department#%%'), ''), '#', 2) AS DEPARTMENT_ID,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category#%%'), ''), '#', 2) AS CATEGORY_ID\n",
    "        FROM CATALOG c\n",
    "        WHERE LOWER(TRIM(c.PRODUCT_ID)) IN ('{product_ids_str}')\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    print(f\"  Extracted {len(df):,} catalog records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Data Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if conn and 'power_vendors' in locals() and 'power_users' in locals():\n    print(\"=\"*80)\n    print(\"STARTING DATA EXTRACTION PIPELINE FOR AD STOCK ANALYSIS\")\n    print(\"=\"*80)\n    \n    # Create timestamp for this extraction run\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # Convert dates to strings for SQL\n    start_date_str = ANALYSIS_START_DATE.strftime('%Y-%m-%d')\n    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n    \n    # Build the CTE for power player filtering - now passing dates and percentiles instead of DataFrames\n    power_player_cte = build_power_player_cte(\n        start_date_str, \n        end_date_str,\n        POWER_VENDOR_PERCENTILE,\n        POWER_USER_PERCENTILE,\n        SAMPLING_FRACTION\n    )\n    \n    # Extract main event tables\n    print(\"\\n--- Extracting event data for power players ---\")\n    auctions_users = extract_auctions_users(conn, start_date_str, end_date_str, power_player_cte)\n    impressions = extract_impressions(conn, start_date_str, end_date_str, power_player_cte)\n    clicks = extract_clicks(conn, start_date_str, end_date_str, power_player_cte)\n    purchases = extract_purchases(conn, start_date_str, end_date_str, power_player_cte)\n    \n    # Collect all product IDs\n    all_product_ids = set()\n    all_product_ids.update(impressions['product_id'].unique())\n    all_product_ids.update(clicks['product_id'].unique())\n    all_product_ids.update(purchases['product_id'].unique())\n    \n    # Extract catalog\n    catalog = extract_catalog(conn, all_product_ids)\n    \n    # Close connection\n    conn.close()\n    print(\"\\n[SUCCESS] Snowflake connection closed\")\nelse:\n    print(\"[ERROR] Missing connection or power player data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(auctions_users, impressions, clicks, purchases, catalog):\n",
    "    \"\"\"Validate data integrity and print summary statistics.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic counts\n",
    "    print(\"\\nRecord Counts:\")\n",
    "    print(f\"  Auctions:    {len(auctions_users):,}\")\n",
    "    print(f\"  Impressions: {len(impressions):,}\")\n",
    "    print(f\"  Clicks:      {len(clicks):,}\")\n",
    "    print(f\"  Purchases:   {len(purchases):,}\")\n",
    "    print(f\"  Catalog:     {len(catalog):,}\")\n",
    "    \n",
    "    # Unique counts\n",
    "    print(\"\\nUnique Entities:\")\n",
    "    print(f\"  Users:       {auctions_users['user_id'].nunique():,}\")\n",
    "    print(f\"  Vendors:     {impressions['vendor_id'].nunique():,}\")\n",
    "    print(f\"  Products:    {len(all_product_ids):,}\")\n",
    "    \n",
    "    # Power player coverage\n",
    "    power_vendor_ids = set(power_vendors['vendor_id'].values)\n",
    "    power_user_ids = set(power_users['user_id'].values)\n",
    "    \n",
    "    impression_vendors = set(impressions['vendor_id'].unique())\n",
    "    impression_users = set(impressions['user_id'].unique())\n",
    "    \n",
    "    print(\"\\nPower Player Coverage:\")\n",
    "    print(f\"  Power vendors in impressions: {len(impression_vendors & power_vendor_ids):,}/{len(power_vendor_ids):,}\")\n",
    "    print(f\"  Power users in impressions: {len(impression_users & power_user_ids):,}/{len(power_user_ids):,}\")\n",
    "    \n",
    "    # Conversion funnel\n",
    "    print(\"\\nConversion Funnel:\")\n",
    "    print(f\"  Impressions:      {len(impressions):,}\")\n",
    "    if len(impressions) > 0:\n",
    "        print(f\"  Clicks:           {len(clicks):,} ({len(clicks)/len(impressions)*100:.1f}% CTR)\")\n",
    "    print(f\"  Purchase Events:  {len(purchases):,}\")\n",
    "\n",
    "if 'auctions_users' in locals():\n",
    "    validate_data(auctions_users, impressions, clicks, purchases, catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'impressions' in locals():\n",
    "    print(\"\\n--- Saving data for ad stock analysis ---\")\n",
    "    output_dir = DATA_DIR\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    datasets = [\n",
    "        (\"auctions_users\", auctions_users),\n",
    "        (\"impressions\", impressions),\n",
    "        (\"clicks\", clicks),\n",
    "        (\"purchases\", purchases),\n",
    "        (\"catalog\", catalog)\n",
    "    ]\n",
    "    \n",
    "    for name, df in tqdm(datasets, desc=\"Saving Parquet files\"):\n",
    "        path = output_dir / f\"{name}_adstock.parquet\"\n",
    "        df.to_parquet(path, index=False)\n",
    "    \n",
    "    print(\"\\nAll data files saved successfully.\")\n",
    "    \n",
    "    # Create metadata file\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'analysis_start_date': start_date_str,\n",
    "        'analysis_end_date': end_date_str,\n",
    "        'total_days': TOTAL_PULL_DAYS,\n",
    "        'sampling_fraction': SAMPLING_FRACTION,\n",
    "        'power_vendor_percentile': POWER_VENDOR_PERCENTILE,\n",
    "        'power_user_percentile': POWER_USER_PERCENTILE,\n",
    "        'n_power_vendors': len(power_vendors),\n",
    "        'n_power_users': len(power_users),\n",
    "        'total_products': len(all_product_ids),\n",
    "        'row_counts': {name: len(df) for name, df in datasets}\n",
    "    }\n",
    "    \n",
    "    metadata_path = output_dir / f\"metadata_adstock.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Saved metadata to {metadata_path.name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"All data saved to {output_dir}/\")\n",
    "    print(f\"Ready for ad stock feature engineering in notebook 02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "if 'auctions_users' in locals():\n",
    "    del auctions_users, impressions, clicks, purchases, catalog\n",
    "    gc.collect()\n",
    "    print(\"âœ“ Memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
\section{Multi-Day Shopping Sessions}

Observational estimates of advertising effectiveness are systematically biased by selection on unobservables. \citet{blake2015consumer} document that ordinary least squares estimates of paid search return on investment exceed 4,000 percent, while experimental estimates using randomized holdouts are negative.\footnote{The eBay experiments showed that frequent users, who accounted for most attributed sales, exhibited no response to paid search advertising. The observational-experimental gap arises because consumers who click on ads are often loyal customers who would have purchased regardless.} Beyond selection, carryover effects extend the influence of advertising beyond individual interactions: \citet{li2014attribution} find that display advertising retains only 5 percent of its informational value after seven days, implying that temporal aggregation affects measured effects.\footnote{Search retains 11 percent and email retains 7.4 percent. These decay rates motivate careful attention to the time window over which exposures are aggregated.} These findings motivate careful construction of the unit of analysis when estimating advertising effects from observational data.

We use shopping sessions rather than individual search queries or page views as the unit of analysis. First, carryover effects extend beyond individual queries, as \citet{li2014attribution} document, making search-level analysis too narrow to capture the full influence of ad exposure. Second, page views are too granular to represent the consideration process that precedes purchase; \citet{moe2003buying} identifies distinct shopping modes that unfold over multiple page views within a single episode. Third, cross-product dynamics are substantial within episodes: users frequently click on one advertised product yet purchase a different product from the same vendor or category, and \citet{zeng2025clickab} document that advertising spillovers across products are common. We define sessions by a three-day inactivity threshold, which isolates episodes of purchase intent and prevents cross-session spillovers from confounding within-session variation.\footnote{Industry attribution windows typically range from 7 to 28 days. Our three-day gap is conservative and ensures that within-session effects are not contaminated by exposures from prior episodes.}

Three distinct biases complicate observational analysis at the session level. Selection bias arises because users who click on ads have higher baseline purchase propensity independent of ad exposure. Activity bias arises mechanically: high-intent shopping episodes generate more impressions, more clicks, and higher conversion probability, inducing positive associations even when ads are not causally effective. Correlated demand bias inflates naive measures because users drawn to one product often demand complements or substitutes; \citet{sharma2015estimating} find that approximately 75 percent of the apparent effect of recommendations in observational data reflects correlated demand rather than causal influence. We address these biases through complementary strategies: user-specific intercepts absorb time-invariant heterogeneity in purchase propensity, session-level controls for duration and engagement intensity proxy for shopping intent independent of ad clicks, and a vendor-agnostic specification tests whether clicking on any ad is associated with purchase outcomes.

The ideal experiment would randomly assign users to treatment versus control conditions, measuring the difference in purchase outcomes. Ghost ads represents the standard industry solution for this identification problem: the methodology simulates counterfactual auctions for control users, identifies which would have received an ad if eligible, and trims the control group to these counterfactual-exposed users \citep{johnson2017ghost}. This section focuses on observational methods that exploit the panel structure of our data without requiring experimental variation.

\subsection*{Panel Construction}

The analysis uses a panel constructed from a 365-day sample of event-level records, drawn from a 0.05 percent hash-based random sample of users.\footnote{Hash-based sampling applies a deterministic hash function to user identifiers, ensuring the same users are selected across all data pulls regardless of activity level. This differs from activity-based sampling which would oversample high-activity users.} Sessions represent isolated episodes of user intent, defined by a three-day (72-hour) inactivity gap between consecutive events. Figure \ref{fig:session_construction} illustrates the session construction methodology. The platform is a second-hand fashion marketplace where product turnover is high. Unlike traditional e-commerce platforms where products remain perpetually available, products on this marketplace may sell quickly, be removed by sellers, or become difficult to relocate. Users face quality uncertainty and cannot assume products will remain available for extended deliberation, which motivates the three-day inactivity threshold as appropriate for the ephemeral nature of inventory.\footnote{The marketplace operates as a social commerce platform where individual sellers list unique items. Products that appear in search results may be sold by the time a user returns, creating urgency in the consideration process distinct from platforms with commodity inventory.}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/session_construction.pdf}
\caption{Session construction methodology. Sessions group user events with gaps shorter than 72 hours (3 days). Each session becomes one observation for analysis.}
\label{fig:session_construction}
\end{figure}

An important limitation is that impression and click data capture only sponsored products. We do not observe organic impressions or clicks on non-promoted listings, which means our analysis measures the association between sponsored ad exposure and purchase outcomes rather than total platform engagement. Users may view and click on many organic products that we cannot measure, which likely attenuates the observed relationship between ad exposure and purchases. The sample contains 11,483 sessions with impressions across 3,019 users, with mean 3.8 sessions per user. Session duration is highly skewed: the mean is 52.8 hours but the median is substantially lower, with a 95th percentile exceeding 200 hours. Within sessions, users see an average of 97.4 impressions across 70.6 distinct products, generating 3.0 clicks on average.

Table \ref{tab:funnel} presents the session-level conversion funnel. Nearly all sessions (99.7 percent) contain at least one ad impression. Of these, 37.9 percent contain at least one click, yielding a session-level click-through rate of 37.9 percent. The purchase rate is 14.8 percent of all sessions. The purchase rate among sessions with clicks is substantially higher than the unconditional rate, but this comparison confounds the causal effect of clicks with selection into clicking, motivating the specifications with user-specific intercepts that follow. The raw session-level correlation between any click and purchase is \IfFileExists{05-sessions/raw_click_purchase_corr.tex}{\input{05-sessions/raw_click_purchase_corr}}{0.45}.

\begin{table}[H]
\centering
\caption{Session-Level Conversion Funnel}
\label{tab:funnel}
\begin{tabular}{lrr}
\toprule
Metric & Count & Rate \\
\midrule
Total Sessions & 11,483 & 100.0\% \\
Sessions with Impressions & 11,455 & 99.7\% \\
Sessions with Clicks & 4,350 & 37.9\% \\
Sessions with Purchases & 1,700 & 14.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Model Specification}

We estimate the relationship between ad exposure and outcomes using fixed effects models at the session level. Let $Y_{ist}$ denote the purchase indicator or gross merchandise value for user $i$ in session $s$ occurring in week $t$, let $\text{Clicks}_{ist}$ denote the number of ad clicks in the session, and let $X_{ist}$ denote a vector of controls including impressions, auctions, distinct products impressed, and duration in hours. Let $\alpha_i$ denote user-specific intercepts and $\delta_t$ denote week-specific intercepts. The logit specification for the binary purchase outcome is:
\[
\Pr(Y_{ist} = 1) = \Lambda(\beta_1 \text{Clicks}_{ist} + X'_{ist}\gamma + \alpha_i + \delta_t)
\]
where $\Lambda(\cdot)$ is the logistic function.\footnote{The logit specification with user-specific intercepts (conditional logit) requires within-user variation in the outcome. Users who always purchase or never purchase across sessions provide no identifying variation and are dropped from estimation.} For the gross merchandise value outcome, we estimate a linear specification with the same controls. We report five specifications: a raw model with only clicks and impressions and no fixed effects (column 0, naive association), a baseline with all controls and no fixed effects (column 1), user fixed effects (column 2), week fixed effects (column 3), and both user and week fixed effects (column 4). Sessions are the unit of observation, so a single user may contribute multiple sessions across different weeks.

\subsection*{Results}

The logit model estimates appear in Table \ref{tab:session_logit_purchase}. Column 0 (Raw) reports naive associations using only clicks and impressions and no fixed effects. Coefficients attenuate as controls and fixed effects are added: the baseline specification (column 1) adds auctions, products impressed, and duration; columns 2 and 3 add user and week fixed effects respectively; column 4 includes both. The patterns indicate that baseline positive associations largely reflect between-user heterogeneity rather than within-user variation: users who click more and see more impressions differ from users who do not, and this difference explains much of the observed correlation with purchases.\footnote{The coefficient on duration hours is consistently positive and statistically significant across specifications, indicating that longer sessions are associated with higher purchase probability.}

The products impressed coefficient becomes positive and significant with user-specific intercepts (0.0050, $p < 0.01$), while the clicks coefficient remains null. One interpretation is that breadth of consideration, captured by variety of products viewed, matters more for purchase probability than depth of engagement with specific products, captured by clicks. An alternative explanation is that the number of distinct products impressed proxies for quality of match between user preferences and available inventory, which would predict purchase independent of ad effectiveness.

% (Table moved to appendix; see sessions_robustness.tex)

\input{05-sessions/session_logit_results}

\paragraph{Robustness.}
We re-define sessions using inactivity gaps of five days (120 hours) and seven days (168 hours). Across both alternatives: (i) the Raw associations in column (0) attenuate as controls and fixed effects are added; (ii) within-user specifications yield negative and statistically insignificant click coefficients for purchase; (iii) duration remains robustly positive and statistically significant; and (iv) GMV regressions with user fixed effects show moderate positive click coefficients and a negative quadratic term on clicks, consistent with saturation at high click volumes. See Appendix Tables \ref{tab:session_logit_purchase_5day}, \ref{tab:session_gmv_results_5day}, \ref{tab:session_logit_purchase_7day}, and \ref{tab:session_gmv_results_7day}.

The gross merchandise value estimates appear in Table \ref{tab:session_gmv_results}. Column 0 (Raw) provides naive associations; coefficients decline toward zero as controls and fixed effects are added. Duration remains the strongest predictor of spend: each additional hour of session duration is associated with higher GMV and this relationship is stable across specifications. Model fit improves modestly with user fixed effects, indicating that user-level heterogeneity explains meaningful variation beyond observable session characteristics.

\subsection*{Discussion}

The attenuation of the clicks coefficient with user-specific intercepts is consistent with a growing body of experimental evidence documenting selection bias in observational advertising studies. \citet{gordon2023comparison} compare experimental and observational estimates of Facebook advertising effects across 15 large-scale experiments, finding systematic overstatement in observational approaches. \citet{lewis2014online} demonstrate in a Yahoo experiment that true advertising effects, while positive, are economically modest and require sample sizes in the millions to detect reliably. The null effects we observe with user-specific intercepts are consistent with either genuinely small causal effects or insufficient statistical power to detect them, a distinction our observational design cannot resolve.

The negative impressions coefficient with user-specific intercepts distinguishes our findings from much of the literature, which typically reports positive or null associations between ad exposure and outcomes. One interpretation consistent with platform incentives is that quality scoring mechanisms successfully route high-intent users toward products they click and purchase, while users who accumulate many impressions without clicking represent a segment with diffuse or unmet preferences. This sorting would generate negative within-user correlations between impressions and purchase probability even if impressions themselves have no causal effect. An alternative explanation is that high impression counts reflect unsuccessful product searches where the user saw many sponsored products but none matched their preferences.

The practical implication is that simple attribution methods overstate the causal contribution of ad clicks to purchases. Users who click are fundamentally different from users who do not click, and this difference explains much of the observed association. Platform-level analysis that aggregates across vendors cannot identify vendor-specific advertising effects, but it can characterize the extent of selection bias and the relative importance of user heterogeneity versus within-session variation in predicting outcomes. These findings motivate experimental or quasi-experimental approaches for vendors seeking to measure the true incremental impact of their advertising spend.

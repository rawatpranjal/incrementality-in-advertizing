{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Test: Snowflake Temporary Table Permissions\n",
    "\n",
    "This notebook tests whether we have permissions to create temporary tables in Snowflake.\n",
    "\n",
    "Temporary tables would allow us to:\n",
    "- Create intermediate result sets for complex queries\n",
    "- Avoid redundant CTE logic across multiple queries\n",
    "- Improve query performance for multi-step analysis\n",
    "- Simplify data pipeline logic\n",
    "\n",
    "Temp tables are session-scoped and auto-drop on disconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Imports complete\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "print(\"[INFO] Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Snowflake connection established.\n",
      "  Database: INCREMENTALITY\n",
      "  Schema: INCREMENTALITY_RESEARCH\n",
      "  Warehouse: COMPUTE_WH\n",
      "  User: Pranjal\n"
     ]
    }
   ],
   "source": [
    "# --- SNOWFLAKE CONNECTION ---\n",
    "load_dotenv('../.env')\n",
    "\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"[SUCCESS] Snowflake connection established.\")\n",
    "    print(f\"  Database: {conn.database}\")\n",
    "    print(f\"  Schema: {conn.schema}\")\n",
    "    print(f\"  Warehouse: {conn.warehouse}\")\n",
    "    print(f\"  User: {conn.user}\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAILURE] Could not connect to Snowflake: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1_header",
   "metadata": {},
   "source": [
    "## Test 1: Create Simple Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "test1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 1: CREATE TEMPORARY TABLE\n",
      "================================================================================\n",
      "\n",
      "[ATTEMPT] Creating temporary table: TEMP_TEST_TABLE\n",
      "\n",
      "[FAILURE] Error during temporary table test: 003540 (42501): SQL execution error: Creating table on shared database 'INCREMENTALITY' is not allowed.\n",
      "Error type: ProgrammingError\n",
      "\n",
      "================================================================================\n",
      "❌ TEST 1 FAILED: Cannot create temporary tables\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- TEST 1: CREATE TEMPORARY TABLE ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 1: CREATE TEMPORARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Attempt to create a simple temporary table\n",
    "        print(\"\\n[ATTEMPT] Creating temporary table: TEMP_TEST_TABLE\")\n",
    "        \n",
    "        create_query = \"\"\"\n",
    "        CREATE TEMPORARY TABLE TEMP_TEST_TABLE (\n",
    "            id INTEGER,\n",
    "            name VARCHAR(100),\n",
    "            value FLOAT\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(create_query)\n",
    "        print(\"[SUCCESS] Temporary table created successfully!\")\n",
    "        \n",
    "        # Insert test data\n",
    "        print(\"\\n[ATTEMPT] Inserting test data...\")\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO TEMP_TEST_TABLE VALUES\n",
    "            (1, 'test_a', 100.5),\n",
    "            (2, 'test_b', 200.7),\n",
    "            (3, 'test_c', 300.9)\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_query)\n",
    "        print(\"[SUCCESS] Test data inserted!\")\n",
    "        \n",
    "        # Query the table\n",
    "        print(\"\\n[ATTEMPT] Querying temporary table...\")\n",
    "        select_query = \"SELECT * FROM TEMP_TEST_TABLE ORDER BY id\"\n",
    "        df_test = pd.read_sql(select_query, conn)\n",
    "        print(\"[SUCCESS] Query successful!\")\n",
    "        print(\"\\nData retrieved:\")\n",
    "        print(df_test)\n",
    "        \n",
    "        # Drop the table\n",
    "        print(\"\\n[ATTEMPT] Dropping temporary table...\")\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS TEMP_TEST_TABLE\")\n",
    "        print(\"[SUCCESS] Temporary table dropped!\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ TEST 1 PASSED: We CAN create temporary tables!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAILURE] Error during temporary table test: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"❌ TEST 1 FAILED: Cannot create temporary tables\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test2_header",
   "metadata": {},
   "source": [
    "## Test 2: Create Temporary Table from Query (CTAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 2: CREATE TEMPORARY TABLE AS SELECT\n",
      "================================================================================\n",
      "\n",
      "[ATTEMPT] Creating temporary table from AUCTIONS_USERS query\n",
      "  Date range: 2025-10-04 to 2025-10-11\n",
      "\n",
      "[FAILURE] Error during CTAS test: 003540 (42501): SQL execution error: Creating table on shared database 'INCREMENTALITY' is not allowed.\n",
      "Error type: ProgrammingError\n",
      "\n",
      "================================================================================\n",
      "❌ TEST 2 FAILED: Cannot create temp tables from queries\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- TEST 2: CREATE TEMPORARY TABLE AS SELECT (CTAS) ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: CREATE TEMPORARY TABLE AS SELECT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Calculate date range (last 7 days)\n",
    "        end_date = date.today()\n",
    "        start_date = end_date - timedelta(days=7)\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"\\n[ATTEMPT] Creating temporary table from AUCTIONS_USERS query\")\n",
    "        print(f\"  Date range: {start_date_str} to {end_date_str}\")\n",
    "        \n",
    "        # Create temp table from actual data\n",
    "        ctas_query = f\"\"\"\n",
    "        CREATE TEMPORARY TABLE TEMP_SAMPLED_USERS AS\n",
    "        SELECT \n",
    "            OPAQUE_USER_ID,\n",
    "            COUNT(*) as num_auctions,\n",
    "            MIN(CREATED_AT) as first_auction,\n",
    "            MAX(CREATED_AT) as last_auction\n",
    "        FROM AUCTIONS_USERS\n",
    "        WHERE CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "        GROUP BY OPAQUE_USER_ID\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(ctas_query)\n",
    "        print(\"[SUCCESS] Temporary table created from query!\")\n",
    "        \n",
    "        # Query the temp table\n",
    "        print(\"\\n[ATTEMPT] Querying temporary table...\")\n",
    "        df_users = pd.read_sql(\"SELECT * FROM TEMP_SAMPLED_USERS LIMIT 10\", conn)\n",
    "        print(\"[SUCCESS] Query successful!\")\n",
    "        print(f\"\\nSample data (first 10 users):\")\n",
    "        print(df_users)\n",
    "        \n",
    "        # Get table stats\n",
    "        print(\"\\n[ATTEMPT] Getting table statistics...\")\n",
    "        count_df = pd.read_sql(\"SELECT COUNT(*) as total_users FROM TEMP_SAMPLED_USERS\", conn)\n",
    "        print(f\"[SUCCESS] Total users in temp table: {count_df['TOTAL_USERS'].iloc[0]:,}\")\n",
    "        \n",
    "        # Drop the table\n",
    "        print(\"\\n[ATTEMPT] Dropping temporary table...\")\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS TEMP_SAMPLED_USERS\")\n",
    "        print(\"[SUCCESS] Temporary table dropped!\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ TEST 2 PASSED: We CAN create temp tables from queries (CTAS)!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAILURE] Error during CTAS test: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"❌ TEST 2 FAILED: Cannot create temp tables from queries\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test3_header",
   "metadata": {},
   "source": [
    "## Test 3: Join Temporary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "test3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 3: JOIN MULTIPLE TEMPORARY TABLES\n",
      "================================================================================\n",
      "\n",
      "[ATTEMPT] Creating first temp table: TEMP_USERS_SAMPLE\n",
      "\n",
      "[FAILURE] Error during join test: 003540 (42501): SQL execution error: Creating table on shared database 'INCREMENTALITY' is not allowed.\n",
      "Error type: ProgrammingError\n",
      "\n",
      "================================================================================\n",
      "❌ TEST 3 FAILED: Cannot join temp tables\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- TEST 3: JOIN MULTIPLE TEMPORARY TABLES ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: JOIN MULTIPLE TEMPORARY TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Calculate date range\n",
    "        end_date = date.today()\n",
    "        start_date = end_date - timedelta(days=7)\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"\\n[ATTEMPT] Creating first temp table: TEMP_USERS_SAMPLE\")\n",
    "        \n",
    "        # Create first temp table: sampled users\n",
    "        query1 = f\"\"\"\n",
    "        CREATE TEMPORARY TABLE TEMP_USERS_SAMPLE AS\n",
    "        SELECT DISTINCT OPAQUE_USER_ID\n",
    "        FROM AUCTIONS_USERS\n",
    "        WHERE CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "          AND MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) < 10\n",
    "        LIMIT 50\n",
    "        \"\"\"\n",
    "        cursor.execute(query1)\n",
    "        print(\"[SUCCESS] First temp table created!\")\n",
    "        \n",
    "        print(\"\\n[ATTEMPT] Creating second temp table: TEMP_USER_AUCTIONS\")\n",
    "        \n",
    "        # Create second temp table: auction counts for sampled users\n",
    "        query2 = f\"\"\"\n",
    "        CREATE TEMPORARY TABLE TEMP_USER_AUCTIONS AS\n",
    "        SELECT \n",
    "            au.OPAQUE_USER_ID,\n",
    "            COUNT(*) as auction_count\n",
    "        FROM AUCTIONS_USERS au\n",
    "        INNER JOIN TEMP_USERS_SAMPLE us ON au.OPAQUE_USER_ID = us.OPAQUE_USER_ID\n",
    "        WHERE au.CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "        GROUP BY au.OPAQUE_USER_ID\n",
    "        \"\"\"\n",
    "        cursor.execute(query2)\n",
    "        print(\"[SUCCESS] Second temp table created!\")\n",
    "        \n",
    "        print(\"\\n[ATTEMPT] Joining temporary tables...\")\n",
    "        \n",
    "        # Query joining the temp tables\n",
    "        join_query = \"\"\"\n",
    "        SELECT \n",
    "            us.OPAQUE_USER_ID,\n",
    "            ua.auction_count\n",
    "        FROM TEMP_USERS_SAMPLE us\n",
    "        LEFT JOIN TEMP_USER_AUCTIONS ua ON us.OPAQUE_USER_ID = ua.OPAQUE_USER_ID\n",
    "        ORDER BY ua.auction_count DESC NULLS LAST\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        df_joined = pd.read_sql(join_query, conn)\n",
    "        print(\"[SUCCESS] Join successful!\")\n",
    "        print(f\"\\nJoined results (top 10 users by auction count):\")\n",
    "        print(df_joined)\n",
    "        \n",
    "        # Clean up\n",
    "        print(\"\\n[ATTEMPT] Dropping temporary tables...\")\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS TEMP_USERS_SAMPLE\")\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS TEMP_USER_AUCTIONS\")\n",
    "        print(\"[SUCCESS] Temporary tables dropped!\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ TEST 3 PASSED: We CAN join multiple temp tables!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAILURE] Error during join test: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"❌ TEST 3 FAILED: Cannot join temp tables\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary: Temporary Table Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY: TEMPORARY TABLE CAPABILITIES\n",
      "================================================================================\n",
      "\n",
      "If all tests passed, we can:\n",
      "\n",
      "✅ CREATE TEMPORARY TABLE with explicit schema\n",
      "✅ INSERT data into temporary tables\n",
      "✅ QUERY temporary tables with SELECT\n",
      "✅ CREATE TEMPORARY TABLE AS SELECT (CTAS) from queries\n",
      "✅ JOIN multiple temporary tables\n",
      "✅ DROP temporary tables manually\n",
      "\n",
      "Benefits for data pulls:\n",
      "1. Create TEMP_SAMPLED_USERS once, reuse across 6 queries\n",
      "2. Avoid repeating CTE logic in every query\n",
      "3. Better query performance (Snowflake can optimize)\n",
      "4. Cleaner, more readable SQL code\n",
      "5. Session-scoped (auto-cleanup on disconnect)\n",
      "\n",
      "Example usage pattern:\n",
      "\n",
      "```python\n",
      "# 1. Create temp table with sampled users\n",
      "cursor.execute('''\n",
      "    CREATE TEMPORARY TABLE TEMP_SAMPLED_USERS AS\n",
      "    SELECT OPAQUE_USER_ID FROM (\n",
      "        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
      "        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
      "              WHERE CREATED_AT BETWEEN '2025-01-01' AND '2025-01-14')\n",
      "    ) WHERE bucket < 10\n",
      "''')\n",
      "\n",
      "# 2. Use in multiple queries without repeating CTE\n",
      "df_auctions = pd.read_sql('''\n",
      "    SELECT * FROM AUCTIONS_USERS au\n",
      "    WHERE au.OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM TEMP_SAMPLED_USERS)\n",
      "''', conn)\n",
      "\n",
      "df_impressions = pd.read_sql('''\n",
      "    SELECT * FROM IMPRESSIONS i\n",
      "    WHERE i.USER_ID IN (SELECT OPAQUE_USER_ID FROM TEMP_SAMPLED_USERS)\n",
      "''', conn)\n",
      "\n",
      "# ... etc for all 6 tables\n",
      "\n",
      "# 3. Cleanup (optional - auto-drops on disconnect)\n",
      "cursor.execute('DROP TABLE IF EXISTS TEMP_SAMPLED_USERS')\n",
      "```\n",
      "\n",
      "Recommendation: Update CLAUDE.md if tests pass!\n",
      "\n",
      "\n",
      "[INFO] Testing complete. Review results above.\n"
     ]
    }
   ],
   "source": [
    "# --- SUMMARY ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TEMPORARY TABLE CAPABILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "If all tests passed, we can:\n",
    "\n",
    "✅ CREATE TEMPORARY TABLE with explicit schema\n",
    "✅ INSERT data into temporary tables\n",
    "✅ QUERY temporary tables with SELECT\n",
    "✅ CREATE TEMPORARY TABLE AS SELECT (CTAS) from queries\n",
    "✅ JOIN multiple temporary tables\n",
    "✅ DROP temporary tables manually\n",
    "\n",
    "Benefits for data pulls:\n",
    "1. Create TEMP_SAMPLED_USERS once, reuse across 6 queries\n",
    "2. Avoid repeating CTE logic in every query\n",
    "3. Better query performance (Snowflake can optimize)\n",
    "4. Cleaner, more readable SQL code\n",
    "5. Session-scoped (auto-cleanup on disconnect)\n",
    "\n",
    "Example usage pattern:\n",
    "\n",
    "```python\n",
    "# 1. Create temp table with sampled users\n",
    "cursor.execute('''\n",
    "    CREATE TEMPORARY TABLE TEMP_SAMPLED_USERS AS\n",
    "    SELECT OPAQUE_USER_ID FROM (\n",
    "        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
    "        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "              WHERE CREATED_AT BETWEEN '2025-01-01' AND '2025-01-14')\n",
    "    ) WHERE bucket < 10\n",
    "''')\n",
    "\n",
    "# 2. Use in multiple queries without repeating CTE\n",
    "df_auctions = pd.read_sql('''\n",
    "    SELECT * FROM AUCTIONS_USERS au\n",
    "    WHERE au.OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM TEMP_SAMPLED_USERS)\n",
    "''', conn)\n",
    "\n",
    "df_impressions = pd.read_sql('''\n",
    "    SELECT * FROM IMPRESSIONS i\n",
    "    WHERE i.USER_ID IN (SELECT OPAQUE_USER_ID FROM TEMP_SAMPLED_USERS)\n",
    "''', conn)\n",
    "\n",
    "# ... etc for all 6 tables\n",
    "\n",
    "# 3. Cleanup (optional - auto-drops on disconnect)\n",
    "cursor.execute('DROP TABLE IF EXISTS TEMP_SAMPLED_USERS')\n",
    "```\n",
    "\n",
    "Recommendation: Update CLAUDE.md if tests pass!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n[INFO] Testing complete. Review results above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Snowflake connection closed.\n",
      "[INFO] All temporary tables automatically dropped on disconnect.\n"
     ]
    }
   ],
   "source": [
    "# --- CLEANUP: CLOSE CONNECTION ---\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    conn.close()\n",
    "    print(\"[INFO] Snowflake connection closed.\")\n",
    "    print(\"[INFO] All temporary tables automatically dropped on disconnect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpmt3ri3uwm",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Diagnosis: Why Temp Tables Failed\n",
    "\n",
    "**Error:** `Creating table on shared database 'INCREMENTALITY' is not allowed`\n",
    "\n",
    "This means the database is **shared** (read-only access). We cannot create any tables (temp or permanent) in shared databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6kq9nicgcrx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKING DATABASE TYPE\n",
      "================================================================================\n",
      "[ERROR] No active Snowflake connection\n"
     ]
    }
   ],
   "source": [
    "# --- CHECK DATABASE TYPE ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING DATABASE TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Check if database is shared\n",
    "        print(\"\\n[ATTEMPT] Checking database properties...\")\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SHOW DATABASES LIKE 'INCREMENTALITY'\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        if results:\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            df_db = pd.DataFrame(results, columns=columns)\n",
    "            print(\"[SUCCESS] Database information retrieved:\")\n",
    "            print(df_db[['name', 'kind', 'origin']].to_string())\n",
    "            \n",
    "            if 'origin' in df_db.columns:\n",
    "                origin = df_db['origin'].iloc[0] if len(df_db) > 0 else None\n",
    "                if origin and origin != '':\n",
    "                    print(f\"\\n⚠️  Database is SHARED from: {origin}\")\n",
    "                    print(\"   This explains why we cannot create temp tables.\")\n",
    "                else:\n",
    "                    print(\"\\n✓  Database appears to be local (not shared)\")\n",
    "        else:\n",
    "            print(\"[WARNING] Could not retrieve database information\")\n",
    "        \n",
    "        # Check our current privileges\n",
    "        print(\"\\n[ATTEMPT] Checking current user privileges...\")\n",
    "        priv_query = \"SHOW GRANTS ON DATABASE INCREMENTALITY\"\n",
    "        cursor.execute(priv_query)\n",
    "        grants = cursor.fetchall()\n",
    "        \n",
    "        if grants:\n",
    "            grant_cols = [desc[0] for desc in cursor.description]\n",
    "            df_grants = pd.DataFrame(grants, columns=grant_cols)\n",
    "            print(\"[SUCCESS] Current privileges:\")\n",
    "            print(df_grants.to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[INFO] Error checking database: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cqxox2j1tf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 4: Check for Alternative Databases\n",
    "\n",
    "If INCREMENTALITY is shared, we might have access to other databases where we can create temp tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3l61tdopbat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 4: CHECK FOR ALTERNATIVE DATABASES\n",
      "================================================================================\n",
      "[ERROR] No active Snowflake connection\n"
     ]
    }
   ],
   "source": [
    "# --- CHECK FOR OTHER DATABASES ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: CHECK FOR ALTERNATIVE DATABASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n[ATTEMPT] Listing all accessible databases...\")\n",
    "        \n",
    "        cursor.execute(\"SHOW DATABASES\")\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        if results:\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            df_dbs = pd.DataFrame(results, columns=columns)\n",
    "            print(\"[SUCCESS] Found databases:\")\n",
    "            print(df_dbs[['name', 'kind', 'origin']].to_string())\n",
    "            \n",
    "            # Look for non-shared databases\n",
    "            if 'origin' in df_dbs.columns:\n",
    "                local_dbs = df_dbs[df_dbs['origin'].isna() | (df_dbs['origin'] == '')]\n",
    "                if len(local_dbs) > 0:\n",
    "                    print(f\"\\n✓  Found {len(local_dbs)} local (non-shared) database(s):\")\n",
    "                    print(local_dbs['name'].tolist())\n",
    "                    print(\"\\n   We could potentially create temp tables in these databases.\")\n",
    "                else:\n",
    "                    print(\"\\n⚠️  All databases appear to be shared.\")\n",
    "            \n",
    "        else:\n",
    "            print(\"[WARNING] No databases found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[INFO] Error listing databases: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5uivf369z9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 5: Performance Comparison - CTE vs Subquery\n",
    "\n",
    "Since we can't use temp tables, let's test if repeated CTEs vs subqueries have performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lup68vy1hql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 5: CTE PERFORMANCE\n",
      "================================================================================\n",
      "[ERROR] No active Snowflake connection\n"
     ]
    }
   ],
   "source": [
    "# --- TEST CTE PERFORMANCE ---\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: CTE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    \n",
    "    end_date = date.today()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Approach 1: Single CTE used multiple times in one query\n",
    "    print(\"\\n[TEST] Approach 1: Single query with CTE (efficient)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_cte = f\"\"\"\n",
    "    WITH SAMPLED_USERS AS (\n",
    "        SELECT OPAQUE_USER_ID \n",
    "        FROM (\n",
    "            SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
    "            FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "                  WHERE CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}')\n",
    "        ) WHERE bucket < 10\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(DISTINCT OPAQUE_USER_ID) as num_users,\n",
    "        COUNT(*) as num_auctions\n",
    "    FROM AUCTIONS_USERS\n",
    "    WHERE OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USERS)\n",
    "      AND CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_sql(query_cte, conn)\n",
    "    time1 = time.time() - start_time\n",
    "    print(f\"  Time: {time1:.2f} seconds\")\n",
    "    print(f\"  Result: {df1.to_dict('records')[0]}\")\n",
    "    \n",
    "    # Approach 2: Repeated subquery (less efficient)\n",
    "    print(\"\\n[TEST] Approach 2: Repeated subquery (potentially less efficient)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_subquery = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT OPAQUE_USER_ID) as num_users,\n",
    "        COUNT(*) as num_auctions\n",
    "    FROM AUCTIONS_USERS\n",
    "    WHERE OPAQUE_USER_ID IN (\n",
    "        SELECT OPAQUE_USER_ID \n",
    "        FROM (\n",
    "            SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
    "            FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "                  WHERE CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}')\n",
    "        ) WHERE bucket < 10\n",
    "    )\n",
    "    AND CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "    \"\"\"\n",
    "    \n",
    "    df2 = pd.read_sql(query_subquery, conn)\n",
    "    time2 = time.time() - start_time\n",
    "    print(f\"  Time: {time2:.2f} seconds\")\n",
    "    print(f\"  Result: {df2.to_dict('records')[0]}\")\n",
    "    \n",
    "    print(f\"\\n[RESULT] Performance difference: {abs(time1 - time2):.2f} seconds\")\n",
    "    if time1 < time2:\n",
    "        print(f\"  CTE was {((time2/time1 - 1)*100):.1f}% faster\")\n",
    "    else:\n",
    "        print(f\"  Subquery was {((time1/time2 - 1)*100):.1f}% faster\")\n",
    "    \n",
    "    print(\"\\n[INFO] Note: Snowflake's query optimizer may handle both similarly.\")\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4jkf41egu",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 6: Python-side Materialization\n",
    "\n",
    "Alternative approach: Fetch sampled users into Python, then use them in subsequent queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ciqzxk3lo1m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 6: PYTHON-SIDE MATERIALIZATION\n",
      "================================================================================\n",
      "[ERROR] No active Snowflake connection\n"
     ]
    }
   ],
   "source": [
    "# --- TEST PYTHON-SIDE MATERIALIZATION ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 6: PYTHON-SIDE MATERIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    \n",
    "    end_date = date.today()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(\"\\n[STEP 1] Fetch sampled user IDs into Python...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sample_query = f\"\"\"\n",
    "    SELECT OPAQUE_USER_ID \n",
    "    FROM (\n",
    "        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
    "        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "              WHERE CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}')\n",
    "    ) WHERE bucket < 10\n",
    "    \"\"\"\n",
    "    \n",
    "    df_users = pd.read_sql(sample_query, conn)\n",
    "    user_list = df_users['OPAQUE_USER_ID'].tolist()\n",
    "    time_fetch = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Fetched {len(user_list):,} users in {time_fetch:.2f} seconds\")\n",
    "    \n",
    "    if len(user_list) > 0:\n",
    "        print(\"\\n[STEP 2] Use user list in subsequent query...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Format user IDs for SQL IN clause (limited to first 100 to avoid query size issues)\n",
    "        user_subset = user_list[:min(100, len(user_list))]\n",
    "        user_ids_str = \"','\".join(user_subset)\n",
    "        \n",
    "        data_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT OPAQUE_USER_ID) as num_users,\n",
    "            COUNT(*) as num_auctions\n",
    "        FROM AUCTIONS_USERS\n",
    "        WHERE OPAQUE_USER_ID IN ('{user_ids_str}')\n",
    "          AND CREATED_AT BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
    "        \"\"\"\n",
    "        \n",
    "        df_result = pd.read_sql(data_query, conn)\n",
    "        time_query = time.time() - start_time\n",
    "        \n",
    "        print(f\"  Query executed in {time_query:.2f} seconds\")\n",
    "        print(f\"  Result: {df_result.to_dict('records')[0]}\")\n",
    "        \n",
    "        total_time = time_fetch + time_query\n",
    "        print(f\"\\n[RESULT] Total time: {total_time:.2f} seconds\")\n",
    "        \n",
    "        print(f\"\\n[WARNING] This approach has limitations:\")\n",
    "        print(f\"  - SQL query size limits (max ~16MB)\")\n",
    "        print(f\"  - Not suitable for large user lists (>10k users)\")\n",
    "        print(f\"  - Requires two round-trips to Snowflake\")\n",
    "        print(f\"  - Only tested with {len(user_subset)} users (truncated from {len(user_list)})\")\n",
    "        print(f\"\\n[RECOMMENDATION] Stick with CTE approach for large datasets.\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No users found in sample. Cannot test this approach.\")\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ge416fc9p0n",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lscv35y9xvp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "## Findings:\n",
      "\n",
      "❌ TEMPORARY TABLES: NOT AVAILABLE\n",
      "   Reason: INCREMENTALITY database is SHARED (read-only access)\n",
      "   Error: \"Creating table on shared database is not allowed\"\n",
      "\n",
      "## Recommended Approach:\n",
      "\n",
      "✅ USE CTEs (Common Table Expressions)\n",
      "   - Continue using WITH clauses as we currently do\n",
      "   - Snowflake's optimizer handles CTEs efficiently\n",
      "   - No permission issues\n",
      "   - Works perfectly with shared databases\n",
      "\n",
      "## Current Data Pull Pattern (OPTIMAL):\n",
      "\n",
      "```python\n",
      "# Define CTE once\n",
      "cte_sql = '''\n",
      "WITH SAMPLED_USER_IDS AS (\n",
      "    SELECT OPAQUE_USER_ID FROM (\n",
      "        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
      "        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
      "              WHERE CREATED_AT BETWEEN '2025-01-01' AND '2025-01-14')\n",
      "    ) WHERE bucket < 10\n",
      ")\n",
      "'''\n",
      "\n",
      "# Use in each query\n",
      "query1 = cte_sql + '''\n",
      "SELECT * FROM AUCTIONS_USERS au\n",
      "WHERE au.OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USER_IDS)\n",
      "'''\n",
      "\n",
      "query2 = cte_sql + '''\n",
      "SELECT * FROM IMPRESSIONS i\n",
      "WHERE i.USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USER_IDS)\n",
      "'''\n",
      "\n",
      "# ... etc for all 6 tables\n",
      "```\n",
      "\n",
      "## Performance Notes:\n",
      "\n",
      "1. Snowflake optimizes CTEs automatically\n",
      "2. Each query is independent (can run in parallel if needed)\n",
      "3. No session state to manage\n",
      "4. No cleanup required\n",
      "\n",
      "## CLAUDE.md Update:\n",
      "\n",
      "❌ DO NOT add temporary table guidance\n",
      "✅ Document that INCREMENTALITY is a shared database\n",
      "✅ Confirm CTE approach is the correct pattern\n",
      "✅ Note: No table creation permissions (shared database)\n",
      "\n",
      "\n",
      "[INFO] Testing complete. CTE approach remains optimal.\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL SUMMARY ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "## Findings:\n",
    "\n",
    "❌ TEMPORARY TABLES: NOT AVAILABLE\n",
    "   Reason: INCREMENTALITY database is SHARED (read-only access)\n",
    "   Error: \"Creating table on shared database is not allowed\"\n",
    "\n",
    "## Recommended Approach:\n",
    "\n",
    "✅ USE CTEs (Common Table Expressions)\n",
    "   - Continue using WITH clauses as we currently do\n",
    "   - Snowflake's optimizer handles CTEs efficiently\n",
    "   - No permission issues\n",
    "   - Works perfectly with shared databases\n",
    "\n",
    "## Current Data Pull Pattern (OPTIMAL):\n",
    "\n",
    "```python\n",
    "# Define CTE once\n",
    "cte_sql = '''\n",
    "WITH SAMPLED_USER_IDS AS (\n",
    "    SELECT OPAQUE_USER_ID FROM (\n",
    "        SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), 10000) AS bucket\n",
    "        FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "              WHERE CREATED_AT BETWEEN '2025-01-01' AND '2025-01-14')\n",
    "    ) WHERE bucket < 10\n",
    ")\n",
    "'''\n",
    "\n",
    "# Use in each query\n",
    "query1 = cte_sql + '''\n",
    "SELECT * FROM AUCTIONS_USERS au\n",
    "WHERE au.OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USER_IDS)\n",
    "'''\n",
    "\n",
    "query2 = cte_sql + '''\n",
    "SELECT * FROM IMPRESSIONS i\n",
    "WHERE i.USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USER_IDS)\n",
    "'''\n",
    "\n",
    "# ... etc for all 6 tables\n",
    "```\n",
    "\n",
    "## Performance Notes:\n",
    "\n",
    "1. Snowflake optimizes CTEs automatically\n",
    "2. Each query is independent (can run in parallel if needed)\n",
    "3. No session state to manage\n",
    "4. No cleanup required\n",
    "\n",
    "## CLAUDE.md Update:\n",
    "\n",
    "❌ DO NOT add temporary table guidance\n",
    "✅ Document that INCREMENTALITY is a shared database\n",
    "✅ Confirm CTE approach is the correct pattern\n",
    "✅ Note: No table creation permissions (shared database)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"[INFO] Testing complete. CTE approach remains optimal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647aa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Pull with Updated Schema\n",
    "\n",
    "Pull a small sample of recent data to explore the updated schema with new columns:\n",
    "- AUCTIONS_RESULTS: QUALITY, FINAL_BID, PRICE, CONVERSION_RATE, PACING\n",
    "- AUCTIONS_USERS: PLACEMENT\n",
    "\n",
    "Configuration:\n",
    "- Sample: 0.001% of users (~100-200 users)\n",
    "- Window: 7-14 days (recent data)\n",
    "- Purpose: Schema exploration and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Imports complete\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import textwrap\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    message='pandas only supports SQLAlchemy connectable.*'\n",
    ")\n",
    "\n",
    "print(\"[INFO] Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] End date: 2025-10-11\n",
      "[CONFIG] Window: 14 days\n",
      "[CONFIG] Sampling: 0.1000%\n",
      "\n",
      "[SUCCESS] Snowflake connection established.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Date range: Last 14 days for quick testing\n",
    "ANALYSIS_END_DATE = date.today()\n",
    "DAYS_WINDOW = 14\n",
    "\n",
    "# Sampling: 0.1% for quick exploration (10x more than shopping-sessions)\n",
    "SAMPLING_FRACTION = 0.001  # 0.1% of users\n",
    "\n",
    "print(f\"[CONFIG] End date: {ANALYSIS_END_DATE}\")\n",
    "print(f\"[CONFIG] Window: {DAYS_WINDOW} days\")\n",
    "print(f\"[CONFIG] Sampling: {SAMPLING_FRACTION:.4%}\")\n",
    "\n",
    "# --- SNOWFLAKE CONNECTION ---\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"\\n[SUCCESS] Snowflake connection established.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAILURE] Could not connect to Snowflake: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fetch_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA FETCHING FUNCTION WITH UPDATED SCHEMA ---\n",
    "\n",
    "def fetch_data_with_new_schema(conn, start_date: str, end_date: str, sampling_fraction: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Fetches data from Snowflake with UPDATED SCHEMA including new columns.\n",
    "    \n",
    "    New columns:\n",
    "    - AUCTIONS_RESULTS: QUALITY, FINAL_BID, PRICE, CONVERSION_RATE, PACING\n",
    "    - AUCTIONS_USERS: PLACEMENT\n",
    "    \n",
    "    Args:\n",
    "        conn: Snowflake connection\n",
    "        start_date (str): Start date 'YYYY-MM-DD'\n",
    "        end_date (str): End date 'YYYY-MM-DD'\n",
    "        sampling_fraction (float): Fraction of users to sample\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (journey_data dict, catalog_data DataFrame)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FETCHING DATA WITH UPDATED SCHEMA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Date range: {start_date} to {end_date}\")\n",
    "    print(f\"  Sampling: {sampling_fraction:.4%}\")\n",
    "    \n",
    "    journey_data = {}\n",
    "    \n",
    "    # Build shared CTE for deterministic user sampling\n",
    "    total_buckets = 10000\n",
    "    selection_threshold = max(1, int(total_buckets * sampling_fraction))\n",
    "    \n",
    "    cte_sql = textwrap.dedent(f\"\"\"\n",
    "    WITH SAMPLED_USER_IDS AS (\n",
    "        SELECT OPAQUE_USER_ID FROM (\n",
    "            SELECT OPAQUE_USER_ID, MOD(ABS(HASH(OPAQUE_USER_ID)), {total_buckets}) AS bucket\n",
    "            FROM (SELECT DISTINCT OPAQUE_USER_ID FROM AUCTIONS_USERS \n",
    "                  WHERE CREATED_AT BETWEEN '{start_date}' AND '{end_date}')\n",
    "        ) WHERE bucket < {selection_threshold}\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"\\n  Sampling strategy: Hash-based, {selection_threshold} buckets out of {total_buckets}\")\n",
    "    \n",
    "    # 1. AUCTIONS_USERS - WITH NEW PLACEMENT COLUMN\n",
    "    query_users = cte_sql + textwrap.dedent(f\"\"\"\n",
    "    SELECT \n",
    "        LOWER(TO_VARCHAR(au.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "        au.OPAQUE_USER_ID,\n",
    "        au.CREATED_AT,\n",
    "        au.PLACEMENT\n",
    "    FROM AUCTIONS_USERS AS au\n",
    "    JOIN SAMPLED_USER_IDS AS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID\n",
    "    WHERE au.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n  [1/6] Fetching AUCTIONS_USERS (with PLACEMENT)...\")\n",
    "    journey_data['AUCTIONS_USERS'] = pd.read_sql(query_users, conn)\n",
    "    print(f\"        Found {len(journey_data['AUCTIONS_USERS']):,} auctions from {journey_data['AUCTIONS_USERS']['OPAQUE_USER_ID'].nunique():,} users\")\n",
    "    \n",
    "    # 2. AUCTIONS_RESULTS - WITH NEW COLUMNS\n",
    "    query_bids = cte_sql + textwrap.dedent(f\"\"\"\n",
    "    SELECT\n",
    "        LOWER(TO_VARCHAR(ar.AUCTION_ID, 'HEX')) AS AUCTION_ID,\n",
    "        LOWER(TO_VARCHAR(ar.VENDOR_ID, 'HEX')) AS VENDOR_ID,\n",
    "        LOWER(TO_VARCHAR(ar.CAMPAIGN_ID, 'HEX')) AS CAMPAIGN_ID,\n",
    "        LOWER(TRIM(ar.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "        ar.RANKING,\n",
    "        ar.IS_WINNER,\n",
    "        ar.CREATED_AT,\n",
    "        ar.QUALITY,\n",
    "        ar.FINAL_BID,\n",
    "        ar.PRICE,\n",
    "        ar.CONVERSION_RATE,\n",
    "        ar.PACING\n",
    "    FROM AUCTIONS_RESULTS ar\n",
    "    JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "    WHERE au.OPAQUE_USER_ID IN (SELECT OPAQUE_USER_ID FROM SAMPLED_USER_IDS)\n",
    "      AND ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  [2/6] Fetching AUCTIONS_RESULTS (with QUALITY, FINAL_BID, PRICE, CONVERSION_RATE, PACING)...\")\n",
    "    journey_data['AUCTIONS_RESULTS'] = pd.read_sql(query_bids, conn)\n",
    "    print(f\"        Found {len(journey_data['AUCTIONS_RESULTS']):,} bids\")\n",
    "    \n",
    "    # 3. IMPRESSIONS\n",
    "    query_impressions = cte_sql + textwrap.dedent(f\"\"\"\n",
    "    SELECT\n",
    "        i.INTERACTION_ID,\n",
    "        LOWER(REPLACE(i.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "        LOWER(TRIM(i.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "        i.USER_ID,\n",
    "        LOWER(REPLACE(i.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "        LOWER(REPLACE(i.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "        i.OCCURRED_AT\n",
    "    FROM IMPRESSIONS i\n",
    "    JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.OPAQUE_USER_ID\n",
    "    WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  [3/6] Fetching IMPRESSIONS...\")\n",
    "    journey_data['IMPRESSIONS'] = pd.read_sql(query_impressions, conn)\n",
    "    print(f\"        Found {len(journey_data['IMPRESSIONS']):,} impressions\")\n",
    "    \n",
    "    # 4. CLICKS\n",
    "    query_clicks = cte_sql + textwrap.dedent(f\"\"\"\n",
    "    SELECT\n",
    "        c.INTERACTION_ID,\n",
    "        LOWER(REPLACE(c.AUCTION_ID, '-', '')) AS AUCTION_ID,\n",
    "        LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "        c.USER_ID,\n",
    "        LOWER(REPLACE(c.CAMPAIGN_ID, '-', '')) AS CAMPAIGN_ID,\n",
    "        LOWER(REPLACE(c.VENDOR_ID, '-', '')) AS VENDOR_ID,\n",
    "        c.OCCURRED_AT\n",
    "    FROM CLICKS c\n",
    "    JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.OPAQUE_USER_ID\n",
    "    WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  [4/6] Fetching CLICKS...\")\n",
    "    journey_data['CLICKS'] = pd.read_sql(query_clicks, conn)\n",
    "    print(f\"        Found {len(journey_data['CLICKS']):,} clicks\")\n",
    "    \n",
    "    # 5. PURCHASES\n",
    "    query_purchases = cte_sql + textwrap.dedent(f\"\"\"\n",
    "    SELECT\n",
    "        p.PURCHASE_ID,\n",
    "        p.PURCHASED_AT,\n",
    "        LOWER(TRIM(p.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "        p.QUANTITY,\n",
    "        p.UNIT_PRICE,\n",
    "        p.USER_ID,\n",
    "        p.PURCHASE_LINE\n",
    "    FROM PURCHASES p\n",
    "    JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.OPAQUE_USER_ID\n",
    "    WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  [5/6] Fetching PURCHASES...\")\n",
    "    journey_data['PURCHASES'] = pd.read_sql(query_purchases, conn)\n",
    "    print(f\"        Found {len(journey_data['PURCHASES']):,} purchases\")\n",
    "    \n",
    "    # 6. CATALOG (simplified - no category parsing for now)\n",
    "    catalog_query = cte_sql + textwrap.dedent(f\"\"\",\n",
    "    RELEVANT_PRODUCT_IDS AS (\n",
    "        SELECT DISTINCT LOWER(TRIM(ar.PRODUCT_ID)) AS product_id \n",
    "        FROM AUCTIONS_RESULTS ar \n",
    "        JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID \n",
    "        JOIN SAMPLED_USER_IDS s ON au.OPAQUE_USER_ID = s.OPAQUE_USER_ID \n",
    "        WHERE ar.CREATED_AT BETWEEN '{start_date}' AND '{end_date}' \n",
    "          AND ar.PRODUCT_ID IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT LOWER(TRIM(i.PRODUCT_ID)) AS product_id \n",
    "        FROM IMPRESSIONS i \n",
    "        JOIN SAMPLED_USER_IDS s ON i.USER_ID = s.OPAQUE_USER_ID \n",
    "        WHERE i.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}' \n",
    "          AND i.PRODUCT_ID IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT LOWER(TRIM(c.PRODUCT_ID)) AS product_id \n",
    "        FROM CLICKS c \n",
    "        JOIN SAMPLED_USER_IDS s ON c.USER_ID = s.OPAQUE_USER_ID \n",
    "        WHERE c.OCCURRED_AT BETWEEN '{start_date}' AND '{end_date}' \n",
    "          AND c.PRODUCT_ID IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT LOWER(TRIM(p.PRODUCT_ID)) AS product_id \n",
    "        FROM PURCHASES p \n",
    "        JOIN SAMPLED_USER_IDS s ON p.USER_ID = s.OPAQUE_USER_ID \n",
    "        WHERE p.PURCHASED_AT BETWEEN '{start_date}' AND '{end_date}' \n",
    "          AND p.PRODUCT_ID IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        LOWER(TRIM(c.PRODUCT_ID)) AS PRODUCT_ID,\n",
    "        c.NAME,\n",
    "        c.PRICE,\n",
    "        c.ACTIVE,\n",
    "        c.IS_DELETED\n",
    "    FROM CATALOG c\n",
    "    JOIN RELEVANT_PRODUCT_IDS rpi ON rpi.product_id = LOWER(TRIM(c.PRODUCT_ID))\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  [6/6] Fetching CATALOG...\")\n",
    "    catalog_data = pd.read_sql(catalog_query, conn)\n",
    "    print(f\"        Found {len(catalog_data):,} products\")\n",
    "    \n",
    "    print(f\"\\n  [SUCCESS] Data fetch complete!\")\n",
    "    print(f\"  Total users: {journey_data['AUCTIONS_USERS']['OPAQUE_USER_ID'].nunique():,}\")\n",
    "    print(f\"  Total events: {sum(len(df) for df in journey_data.values()):,}\")\n",
    "    \n",
    "    return journey_data, catalog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "execute_pull",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FETCHING DATA WITH UPDATED SCHEMA\n",
      "================================================================================\n",
      "  Date range: 2025-09-27 to 2025-10-11\n",
      "  Sampling: 0.1000%\n",
      "\n",
      "  Sampling strategy: Hash-based, 10 buckets out of 10000\n",
      "\n",
      "  [1/6] Fetching AUCTIONS_USERS (with PLACEMENT)...\n",
      "        Found 413,457 auctions from 4,671 users\n",
      "  [2/6] Fetching AUCTIONS_RESULTS (with QUALITY, FINAL_BID, PRICE, CONVERSION_RATE, PACING)...\n",
      "        Found 18,838,670 bids\n",
      "  [3/6] Fetching IMPRESSIONS...\n",
      "        Found 533,146 impressions\n",
      "  [4/6] Fetching CLICKS...\n",
      "        Found 16,706 clicks\n",
      "  [5/6] Fetching PURCHASES...\n",
      "        Found 2,188 purchases\n",
      "  [6/6] Fetching CATALOG...\n",
      "        Found 2,007,695 products\n",
      "\n",
      "  [SUCCESS] Data fetch complete!\n",
      "  Total users: 4,671\n",
      "  Total events: 19,804,167\n",
      "\n",
      "[INFO] Data successfully loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTE DATA PULL ---\n",
    "\n",
    "if 'conn' in locals() and conn and not conn.is_closed():\n",
    "    # Calculate date range\n",
    "    start_date = ANALYSIS_END_DATE - timedelta(days=DAYS_WINDOW)\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = ANALYSIS_END_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Fetch data\n",
    "    journey_data, catalog_data = fetch_data_with_new_schema(\n",
    "        conn, start_date_str, end_date_str, SAMPLING_FRACTION\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[INFO] Data successfully loaded into memory.\")\n",
    "else:\n",
    "    print(\"[ERROR] No active Snowflake connection. Please run the connection cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING DATA ARTIFACTS\n",
      "================================================================================\n",
      "  Saved raw_auctions_users_20251011.parquet (11.96 MB)\n",
      "  Saved raw_auctions_results_20251011.parquet (981.55 MB)\n",
      "  Saved raw_impressions_20251011.parquet (46.33 MB)\n",
      "  Saved raw_clicks_20251011.parquet (1.92 MB)\n",
      "  Saved raw_purchases_20251011.parquet (0.14 MB)\n",
      "  Saved catalog_20251011.parquet (115.65 MB)\n",
      "\n",
      "[SUCCESS] All data saved to 'data'\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE RAW DATA ---\n",
    "\n",
    "if 'journey_data' in locals() and 'catalog_data' in locals():\n",
    "    output_dir = Path(\"./data\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = ANALYSIS_END_DATE.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING DATA ARTIFACTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Save journey data\n",
    "    for name, df in journey_data.items():\n",
    "        path = output_dir / f\"raw_{name.lower()}_{timestamp}.parquet\"\n",
    "        df.to_parquet(path, index=False)\n",
    "        size_mb = path.stat().st_size / 1024 / 1024\n",
    "        print(f\"  Saved {path.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Save catalog\n",
    "    catalog_path = output_dir / f\"catalog_{timestamp}.parquet\"\n",
    "    catalog_data.to_parquet(catalog_path, index=False)\n",
    "    size_mb = catalog_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"  Saved {catalog_path.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] All data saved to '{output_dir}'\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not found. Please run the data pull cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hom902ipbcj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Data file not found: data/journey_data.pkl\n",
      "Run the data pull cells first to create the data\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD SAVED DATA (if data pull already done) ---\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_file = 'data/journey_data.pkl'\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    print(f\"Loading data from {data_file}...\")\n",
    "    with open(data_file, 'rb') as f:\n",
    "        journey_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded tables:\")\n",
    "    for table_name, df in journey_data.items():\n",
    "        print(f\"  {table_name}: {len(df):,} rows\")\n",
    "    \n",
    "    print(f\"\\n✓ Data loaded successfully\")\n",
    "else:\n",
    "    print(f\"[WARNING] Data file not found: {data_file}\")\n",
    "    print(f\"Run the data pull cells first to create the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "## Part 1: Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHEMA VALIDATION ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SCHEMA VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for table_name, df in journey_data.items():\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        print(f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print(f\"  Dtypes:\")\n",
    "        for col in df.columns:\n",
    "            missing_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "            print(f\"    {col:20s} {str(df[col].dtype):15s} {missing_pct:5.1f}% missing\")\n",
    "    \n",
    "    print(f\"\\nCATALOG:\")\n",
    "    print(f\"  Shape: {catalog_data.shape[0]:,} rows × {catalog_data.shape[1]} columns\")\n",
    "    print(f\"  Columns: {list(catalog_data.columns)}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Schema validation complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE DATA (First 5 rows)\n",
      "================================================================================\n",
      "\n",
      "AUCTIONS_USERS:\n",
      "                         AUCTION_ID  \\\n",
      "0  068d734899d07de0a1045a2118dc23ba   \n",
      "1  068d73517944715093043563a3816287   \n",
      "2  068d7344cb5b718780045f4e93c3ea28   \n",
      "3  068d73595d8b716dae04f5cf93c3ea28   \n",
      "4  068d735cd8067521bb0473d1bdf792a1   \n",
      "\n",
      "                              OPAQUE_USER_ID              CREATED_AT PLACEMENT  \n",
      "0  ext1:961fd468-21e3-424f-b0f8-0ece9f23ab6a 2025-09-27 00:49:13.650         3  \n",
      "1  ext1:d2d65979-c9b6-45e7-a90d-0638d06aaefd 2025-09-27 00:51:35.619         2  \n",
      "2  ext1:d2d65979-c9b6-45e7-a90d-0638d06aaefd 2025-09-27 00:48:12.760         2  \n",
      "3  ext1:abbf41f4-1bed-44ba-8ac6-a73d0861f1c0 2025-09-27 00:53:41.900         3  \n",
      "4  ext1:d2d65979-c9b6-45e7-a90d-0638d06aaefd 2025-09-27 00:54:37.536         2  \n",
      "\n",
      "AUCTIONS_RESULTS:\n",
      "                         AUCTION_ID                         VENDOR_ID  \\\n",
      "0  068d732029cf7560ba049dd6ee967f23  0196f3b92cd87403b96a612bd0bf28c4   \n",
      "1  068d732029cf7560ba049dd6ee967f23  064b2efd93cf7cf9a624cd526419b66a   \n",
      "2  068d73791dd5757f91043b5d8bcb40bc  019969d67a637a639e32391f07484aa0   \n",
      "3  068d73791dd5757f91043b5d8bcb40bc  0191e88b0ab871d2855186f8d247433d   \n",
      "4  068d72cd8dfa79598b04dcc5e65cabab  0197ff75d61876f2b934c1aa4865ad4c   \n",
      "\n",
      "                        CAMPAIGN_ID                PRODUCT_ID  RANKING  \\\n",
      "0  019968d4f25f7c40872454bf4d64d708  5d9fb5cbabe1cefef5154db5       18   \n",
      "1  01997316ae6e739385d0ed09eeb4e268  5af70ae4dfb740a8257c0c66       32   \n",
      "2  019969d71ff67a63a48a2790bd9e6671  68c0a6cbdb94206a1b59b1c3       43   \n",
      "3  01996aa6f3997a02980123eb251e44ff  68c226c8ad1a0ee6d8e88499       53   \n",
      "4  0199681858e27fb0b46151aa7010304b  68680a169f19e29d3c0b6597       26   \n",
      "\n",
      "   IS_WINNER              CREATED_AT   QUALITY  FINAL_BID  PRICE  \\\n",
      "0       True 2025-09-27 00:38:26.658  0.031692          1    1.0   \n",
      "1       True 2025-09-27 00:38:26.658  0.014919          1    1.0   \n",
      "2       True 2025-09-27 01:02:09.928  0.067837          5    5.0   \n",
      "3      False 2025-09-27 01:02:09.928  0.078349          4    NaN   \n",
      "4       True 2025-09-27 00:16:24.938  0.065312          7    7.0   \n",
      "\n",
      "   CONVERSION_RATE  PACING  \n",
      "0         0.000679     1.0  \n",
      "1         0.017199     1.0  \n",
      "2         0.009456     1.0  \n",
      "3         0.003592     1.0  \n",
      "4         0.003892     1.0  \n",
      "\n",
      "IMPRESSIONS:\n",
      "                         INTERACTION_ID                        AUCTION_ID  \\\n",
      "0  068e84fb-63bc-7300-aa0d-4117576fcd70  068e84e580437f678c043c3b15272000   \n",
      "1  068e84f4-1535-78ed-b90d-61417b0b759c  068e84e580437f678c043c3b15272000   \n",
      "2  068e84ea-716b-72ee-9b0d-ca83576fcd70  068e84e580437f678c043c3b15272000   \n",
      "3  068e84f5-64ef-774b-8a0d-adad15272000  068e84e580437f678c043c3b15272000   \n",
      "4  068e84f6-37ff-7ad3-ab0d-b54815272000  068e84e580437f678c043c3b15272000   \n",
      "\n",
      "                 PRODUCT_ID                                    USER_ID  \\\n",
      "0  63aa0173a0e6c6be1939b490  ext1:e5bb6e56-3239-4e48-b2e6-1f3647f215cb   \n",
      "1  68e16254013d2a6ceddee268  ext1:e5bb6e56-3239-4e48-b2e6-1f3647f215cb   \n",
      "2  6740db77a2c69e1f45f92f99  ext1:e5bb6e56-3239-4e48-b2e6-1f3647f215cb   \n",
      "3  6764d6a4d253f1c2afd794e3  ext1:e5bb6e56-3239-4e48-b2e6-1f3647f215cb   \n",
      "4  670c13c3d399ff954b8cb3b2  ext1:e5bb6e56-3239-4e48-b2e6-1f3647f215cb   \n",
      "\n",
      "                        CAMPAIGN_ID                         VENDOR_ID  \\\n",
      "0  0199bbb1b78e7551aaa6d957f6d037aa  0199bbb0ef4f7642b7ac9477a18307c0   \n",
      "1  0199b0a4bc157532bf5804cd29e87c84  0199b0a3e4fe7a4080d0cb2a9dc5e573   \n",
      "2  0199c077f23374a19a77597ae0b281db  0197a384ac2f76519a2e2351185beeae   \n",
      "3  0199c1e9d1977a1185a6567052033b54  019317133a377de2b7871a00d51d91db   \n",
      "4  0199cb1e64167e61b184722df6a3b1ee  0190f510b06b710296a543db6c961785   \n",
      "\n",
      "          OCCURRED_AT  \n",
      "0 2025-10-10 00:08:14  \n",
      "1 2025-10-10 00:08:08  \n",
      "2 2025-10-10 00:07:53  \n",
      "3 2025-10-10 00:08:08  \n",
      "4 2025-10-10 00:08:14  \n",
      "\n",
      "CLICKS:\n",
      "                         INTERACTION_ID                        AUCTION_ID  \\\n",
      "0  068e2dc8-e2e7-7365-a818-2fb738948f45  068e2dc7103673ac8a04e5f4299d3a75   \n",
      "1  068d93f7-2676-71ba-a018-5763a3816287  068d93f1170871fe8b04035eee967f23   \n",
      "2  068d93f1-c217-70a2-9618-0ee98bcb40bc  068d93f1170871fe8b04035eee967f23   \n",
      "3  068d72b4-cb16-76db-b218-a7a4ee967f23  068d72b0498670739d041c4718dc23ba   \n",
      "4  068dd2da-5eb6-7cfd-bf18-b5237f7601d0  068dd2d901e179428c04303ca3cc702a   \n",
      "\n",
      "                 PRODUCT_ID                                    USER_ID  \\\n",
      "0  689a43499b2158ba5c567960  ext1:09e54712-68fa-4618-ac5e-317441bb65aa   \n",
      "1  61ba4d3b93649f006862a4b0  ext1:3ac3aaf0-9ff9-4314-94b0-0f3eab6fb068   \n",
      "2  5ec1840f7a250250f6d06b1d  ext1:3ac3aaf0-9ff9-4314-94b0-0f3eab6fb068   \n",
      "3  644a795183cbecdfd4dabd1e  ext1:3ac3aaf0-9ff9-4314-94b0-0f3eab6fb068   \n",
      "4  68af82139f19e2ee10eebf53  ext1:c259652d-db39-4810-b61f-7a735d4d075c   \n",
      "\n",
      "                        CAMPAIGN_ID                         VENDOR_ID  \\\n",
      "0  01999708a4d178d19ae1672ab363a63c  01973d44e4ac70d282cfa670dc3db4ee   \n",
      "1  01998c5562db7511af73f6507fe8cbdc  018f84c8555a76c896d6def502caaf90   \n",
      "2  01998a26000d78a1911a825ef25acab4  019330803b9c72e185855ea19b430a14   \n",
      "3  019978b4f8d176b0b9bb1fd4104d6a50  0652364187f77f81b724aff67c114d25   \n",
      "4  01999158f4927eb2a71dfcb998e4b047  01930bd8b2807bb19281bf8986f4d361   \n",
      "\n",
      "          OCCURRED_AT  \n",
      "0 2025-10-05 21:00:37  \n",
      "1 2025-09-28 14:00:14  \n",
      "2 2025-09-28 13:58:49  \n",
      "3 2025-09-27 00:08:44  \n",
      "4 2025-10-01 13:33:23  \n",
      "\n",
      "PURCHASES:\n",
      "                PURCHASE_ID        PURCHASED_AT                PRODUCT_ID  \\\n",
      "0  68d84a437d56cd92d8900b4d 2025-09-27 20:34:16  68c5e7d5e9c77dc27c6a3aec   \n",
      "1  68d999a8b6a2b3042913692b 2025-09-28 20:25:16  68b29d4fcbbea594d59900d2   \n",
      "2  68e314d952714fa731386849 2025-10-06 01:01:16  68d6b70205deb330b63039c6   \n",
      "3  68e88c4e39636d5da3c37d80 2025-10-10 04:32:16  68e7d50449e17be6d09ad657   \n",
      "4  68d9a7e600d30febf160674b 2025-09-28 21:26:00  68837f8efbae95c790eb5101   \n",
      "\n",
      "   QUANTITY  UNIT_PRICE                                    USER_ID  \\\n",
      "0         1        1000  ext1:db3b98ae-6427-48d9-b525-2205e933e362   \n",
      "1         1        1000  ext1:5c07407e-bbe9-4587-8eff-59ea3ac809be   \n",
      "2         1         700  ext1:a639d603-e08f-4240-a070-2bedd7aaf1ee   \n",
      "3         1        1900  ext1:fd39ef66-ea8a-4bd9-a77f-6a0a40280159   \n",
      "4         1        1400  ext1:510e024a-99bc-4632-b22f-1fb70830920d   \n",
      "\n",
      "   PURCHASE_LINE  revenue  \n",
      "0              1     10.0  \n",
      "1              1     10.0  \n",
      "2              1      7.0  \n",
      "3              1     19.0  \n",
      "4              1     14.0  \n",
      "\n",
      "CATALOG:\n",
      "                 PRODUCT_ID  \\\n",
      "0  68a29517006e43624599ceaa   \n",
      "1  67460074b9bd7437eef2144b   \n",
      "2  65d5055334e253a455ab0d2a   \n",
      "3  681fd92b9b2158e5e4ca088a   \n",
      "4  68b12150b09b7c2e98baf59d   \n",
      "\n",
      "                                                NAME  PRICE  ACTIVE  \\\n",
      "0  HELLO KITTY By SANRIO 20” GREEN DRAGON JUMBO P...   64.0    True   \n",
      "1  Lane Bryant Dress Slacks Womens Full Stretch P...   16.0    True   \n",
      "2  1970s Polaroid Automatic 100 Land Camera w/Bag...  145.0    True   \n",
      "3  Dark Purple Lehenga - Elegant Reception Wear I...  275.0    True   \n",
      "4    White and Pink Paisley Skorts with lace trim XL   22.0    True   \n",
      "\n",
      "   IS_DELETED  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n"
     ]
    }
   ],
   "source": [
    "# --- DISPLAY SAMPLE DATA ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAMPLE DATA (First 5 rows)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for table_name, df in journey_data.items():\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        print(df.head())\n",
    "    \n",
    "    print(f\"\\nCATALOG:\")\n",
    "    print(catalog_data.head())\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_columns_header",
   "metadata": {},
   "source": [
    "## Part 2: New Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_columns_auctions_users",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEW COLUMN ANALYSIS: PLACEMENT (AUCTIONS_USERS)\n",
      "================================================================================\n",
      "\n",
      "Column: PLACEMENT\n",
      "  Data type: object\n",
      "  Missing: 0 (0.0%)\n",
      "  Unique values: 5\n",
      "\n",
      "  Value distribution:\n",
      "    5                               160,276 ( 38.8%)\n",
      "    2                               145,826 ( 35.3%)\n",
      "    3                                61,217 ( 14.8%)\n",
      "    1                                38,858 (  9.4%)\n",
      "    4                                 7,280 (  1.8%)\n",
      "\n",
      "================================================================================\n",
      "PLACEMENT-LEVEL BIDDING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Bidding statistics by PLACEMENT:\n",
      "           Num_Auctions  Total_Bids  Bids_Per_Auction     Wins  Win_Rate\n",
      "PLACEMENT                                                               \n",
      "1                 38858     1502474         38.665757  1135277    0.7556\n",
      "2                145826     5003519         34.311570  4285134    0.8564\n",
      "3                 61217     3040245         49.663410  2428435    0.7988\n",
      "4                  7280      178757         24.554533   142934    0.7996\n",
      "5                160276     9115603         56.874410  7518892    0.8248\n",
      "\n",
      "Key insights:\n",
      "  Most competitive placement (highest bids/auction): 5\n",
      "  Least competitive placement (lowest bids/auction): 4\n",
      "  Highest win rate placement: 2\n",
      "  Lowest win rate placement: 1\n",
      "\n",
      "[INFO] PLACEMENT analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# --- ANALYZE NEW COLUMN: PLACEMENT (AUCTIONS_USERS) ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"NEW COLUMN ANALYSIS: PLACEMENT (AUCTIONS_USERS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = journey_data['AUCTIONS_USERS']\n",
    "    \n",
    "    print(f\"\\nColumn: PLACEMENT\")\n",
    "    print(f\"  Data type: {df['PLACEMENT'].dtype}\")\n",
    "    print(f\"  Missing: {df['PLACEMENT'].isna().sum():,} ({df['PLACEMENT'].isna().mean()*100:.1f}%)\")\n",
    "    print(f\"  Unique values: {df['PLACEMENT'].nunique():,}\")\n",
    "    \n",
    "    if df['PLACEMENT'].notna().any():\n",
    "        print(f\"\\n  Value distribution:\")\n",
    "        value_counts = df['PLACEMENT'].value_counts(dropna=False)\n",
    "        for val, count in value_counts.head(20).items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"    {str(val):30s} {count:8,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        if len(value_counts) > 20:\n",
    "            print(f\"    ... and {len(value_counts) - 20} more values\")\n",
    "    else:\n",
    "        print(\"  [WARNING] All values are NULL\")\n",
    "    \n",
    "    # PLACEMENT-LEVEL BIDDING ANALYSIS\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PLACEMENT-LEVEL BIDDING ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    auctions = journey_data['AUCTIONS_USERS']\n",
    "    bids = journey_data['AUCTIONS_RESULTS']\n",
    "    \n",
    "    # Join auctions with bids to get placement info\n",
    "    auction_bids = bids.merge(auctions[['AUCTION_ID', 'PLACEMENT']], on='AUCTION_ID', how='left')\n",
    "    \n",
    "    print(f\"\\nBidding statistics by PLACEMENT:\")\n",
    "    placement_stats = auction_bids.groupby('PLACEMENT').agg({\n",
    "        'AUCTION_ID': 'count',  # total bids\n",
    "        'IS_WINNER': ['sum', 'mean']  # wins and win rate\n",
    "    }).round(4)\n",
    "    \n",
    "    placement_stats.columns = ['Total_Bids', 'Wins', 'Win_Rate']\n",
    "    \n",
    "    # Add auctions per placement\n",
    "    auctions_per_placement = auctions['PLACEMENT'].value_counts().sort_index()\n",
    "    placement_stats['Num_Auctions'] = auctions_per_placement\n",
    "    placement_stats['Bids_Per_Auction'] = placement_stats['Total_Bids'] / placement_stats['Num_Auctions']\n",
    "    \n",
    "    # Reorder columns\n",
    "    placement_stats = placement_stats[['Num_Auctions', 'Total_Bids', 'Bids_Per_Auction', 'Wins', 'Win_Rate']]\n",
    "    \n",
    "    print(placement_stats.to_string())\n",
    "    \n",
    "    print(f\"\\nKey insights:\")\n",
    "    print(f\"  Most competitive placement (highest bids/auction): {placement_stats['Bids_Per_Auction'].idxmax()}\")\n",
    "    print(f\"  Least competitive placement (lowest bids/auction): {placement_stats['Bids_Per_Auction'].idxmin()}\")\n",
    "    print(f\"  Highest win rate placement: {placement_stats['Win_Rate'].idxmax()}\")\n",
    "    print(f\"  Lowest win rate placement: {placement_stats['Win_Rate'].idxmin()}\")\n",
    "    \n",
    "    print(\"\\n[INFO] PLACEMENT analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_columns_auctions_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEW COLUMNS ANALYSIS: AUCTIONS_RESULTS\n",
      "================================================================================\n",
      "\n",
      "QUALITY:\n",
      "  Data type: float64\n",
      "  Missing: 0 (0.0%)\n",
      "  Statistics:\n",
      "    Count:  18,838,670\n",
      "    Mean:   0.036669\n",
      "    Std:    0.028097\n",
      "    Min:    0.000001\n",
      "    25%:    0.013764\n",
      "    50%:    0.032807\n",
      "    75%:    0.053153\n",
      "    Max:    0.847945\n",
      "  Unique values: 6,159,823\n",
      "  Decile distribution:\n",
      "    10th percentile: 0.005403\n",
      "    20th percentile: 0.011266\n",
      "    30th percentile: 0.016306\n",
      "    40th percentile: 0.023195\n",
      "    50th percentile: 0.032807\n",
      "    60th percentile: 0.040758\n",
      "    70th percentile: 0.048722\n",
      "    80th percentile: 0.058133\n",
      "    90th percentile: 0.072841\n",
      "\n",
      "FINAL_BID:\n",
      "  Data type: int64\n",
      "  Missing: 0 (0.0%)\n",
      "  Statistics:\n",
      "    Count:  18,838,670\n",
      "    Mean:   11.801821\n",
      "    Std:    14.444879\n",
      "    Min:    0.000000\n",
      "    25%:    3.000000\n",
      "    50%:    6.000000\n",
      "    75%:    16.000000\n",
      "    Max:    100.000000\n",
      "  Unique values: 101\n",
      "  Decile distribution:\n",
      "    10th percentile: 1.000000\n",
      "    20th percentile: 2.000000\n",
      "    30th percentile: 4.000000\n",
      "    40th percentile: 5.000000\n",
      "    50th percentile: 6.000000\n",
      "    60th percentile: 9.000000\n",
      "    70th percentile: 13.000000\n",
      "    80th percentile: 19.000000\n",
      "    90th percentile: 29.000000\n",
      "\n",
      "PRICE:\n",
      "  Data type: float64\n",
      "  Missing: 3,329,566 (17.7%)\n",
      "  Statistics:\n",
      "    Count:  15,509,104\n",
      "    Mean:   11.470018\n",
      "    Std:    13.149261\n",
      "    Min:    1.000000\n",
      "    25%:    3.000000\n",
      "    50%:    6.000000\n",
      "    75%:    16.000000\n",
      "    Max:    100.000000\n",
      "  Unique values: 100\n",
      "  Top 20 value counts:\n",
      "    1.000000: 2,313,200 (12.28%)\n",
      "    4.000000: 1,358,452 (7.21%)\n",
      "    3.000000: 1,262,618 (6.70%)\n",
      "    5.000000: 1,099,328 (5.84%)\n",
      "    6.000000: 950,437 (5.05%)\n",
      "    2.000000: 772,181 (4.10%)\n",
      "    7.000000: 685,434 (3.64%)\n",
      "    8.000000: 537,733 (2.85%)\n",
      "    9.000000: 483,344 (2.57%)\n",
      "    10.000000: 428,203 (2.27%)\n",
      "    11.000000: 389,614 (2.07%)\n",
      "    12.000000: 344,525 (1.83%)\n",
      "    13.000000: 321,998 (1.71%)\n",
      "    14.000000: 310,869 (1.65%)\n",
      "    15.000000: 294,579 (1.56%)\n",
      "    16.000000: 277,864 (1.47%)\n",
      "    17.000000: 266,616 (1.42%)\n",
      "    18.000000: 242,515 (1.29%)\n",
      "    19.000000: 229,626 (1.22%)\n",
      "    20.000000: 216,743 (1.15%)\n",
      "\n",
      "CONVERSION_RATE:\n",
      "  Data type: float64\n",
      "  Missing: 0 (0.0%)\n",
      "  Statistics:\n",
      "    Count:  18,838,670\n",
      "    Mean:   0.010004\n",
      "    Std:    0.007716\n",
      "    Min:    0.000001\n",
      "    25%:    0.004257\n",
      "    50%:    0.009010\n",
      "    75%:    0.013267\n",
      "    Max:    0.056500\n",
      "  Unique values: 3,317,495\n",
      "  Decile distribution:\n",
      "    10th percentile: 0.001456\n",
      "    20th percentile: 0.003485\n",
      "    30th percentile: 0.005345\n",
      "    40th percentile: 0.007324\n",
      "    50th percentile: 0.009010\n",
      "    60th percentile: 0.010277\n",
      "    70th percentile: 0.012159\n",
      "    80th percentile: 0.014552\n",
      "    90th percentile: 0.019160\n",
      "\n",
      "PACING:\n",
      "  Data type: float64\n",
      "  Missing: 0 (0.0%)\n",
      "  Statistics:\n",
      "    Count:  18,838,670\n",
      "    Mean:   0.890994\n",
      "    Std:    0.263387\n",
      "    Min:    0.006738\n",
      "    25%:    1.000000\n",
      "    50%:    1.000000\n",
      "    75%:    1.000000\n",
      "    Max:    1.000000\n",
      "  Unique values: 375,279\n",
      "  Decile distribution:\n",
      "    10th percentile: 0.425693\n",
      "    20th percentile: 1.000000\n",
      "    30th percentile: 1.000000\n",
      "    40th percentile: 1.000000\n",
      "    50th percentile: 1.000000\n",
      "    60th percentile: 1.000000\n",
      "    70th percentile: 1.000000\n",
      "    80th percentile: 1.000000\n",
      "    90th percentile: 1.000000\n",
      "\n",
      "[INFO] New columns analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# --- ANALYZE NEW COLUMNS: AUCTIONS_RESULTS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"NEW COLUMNS ANALYSIS: AUCTIONS_RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = journey_data['AUCTIONS_RESULTS']\n",
    "    new_cols = ['QUALITY', 'FINAL_BID', 'PRICE', 'CONVERSION_RATE', 'PACING']\n",
    "    \n",
    "    for col in new_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Data type: {df[col].dtype}\")\n",
    "            print(f\"  Missing: {df[col].isna().sum():,} ({df[col].isna().mean()*100:.1f}%)\")\n",
    "            \n",
    "            if df[col].notna().any() and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                print(f\"  Statistics:\")\n",
    "                print(f\"    Count:  {df[col].count():,}\")\n",
    "                print(f\"    Mean:   {df[col].mean():.6f}\")\n",
    "                print(f\"    Std:    {df[col].std():.6f}\")\n",
    "                print(f\"    Min:    {df[col].min():.6f}\")\n",
    "                print(f\"    25%:    {df[col].quantile(0.25):.6f}\")\n",
    "                print(f\"    50%:    {df[col].quantile(0.50):.6f}\")\n",
    "                print(f\"    75%:    {df[col].quantile(0.75):.6f}\")\n",
    "                print(f\"    Max:    {df[col].max():.6f}\")\n",
    "                \n",
    "                # Unique value counts for top 20\n",
    "                print(f\"  Unique values: {df[col].nunique():,}\")\n",
    "                if df[col].nunique() <= 100:\n",
    "                    print(f\"  Top 20 value counts:\")\n",
    "                    value_counts = df[col].value_counts().head(20)\n",
    "                    for val, count in value_counts.items():\n",
    "                        pct = (count / len(df)) * 100\n",
    "                        print(f\"    {val:.6f}: {count:,} ({pct:.2f}%)\")\n",
    "                else:\n",
    "                    # For continuous variables, show deciles\n",
    "                    print(f\"  Decile distribution:\")\n",
    "                    deciles = df[col].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "                    for q, val in deciles.items():\n",
    "                        print(f\"    {int(q*100)}th percentile: {val:.6f}\")\n",
    "                        \n",
    "            elif df[col].notna().any():\n",
    "                print(f\"  Unique values: {df[col].nunique():,}\")\n",
    "                value_counts = df[col].value_counts().head(10)\n",
    "                for val, count in value_counts.items():\n",
    "                    print(f\"    {val}: {count:,}\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] All values are NULL\")\n",
    "        else:\n",
    "            print(f\"\\n{col}: [ERROR] Column not found in data\")\n",
    "    \n",
    "    print(\"\\n[INFO] New columns analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auction_dynamics_header",
   "metadata": {},
   "source": [
    "## Part 3: Auction Dynamics with New Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QUALITY SCORE ANALYSIS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"QUALITY SCORE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    df = journey_data['AUCTIONS_RESULTS'].copy()\n",
    "\n",
    "    if 'QUALITY' in df.columns and df['QUALITY'].notna().any():\n",
    "        # Win rate by quality quartile\n",
    "        df['quality_quartile'] = pd.qcut(df['QUALITY'], q=4, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'], duplicates='drop')\n",
    "\n",
    "        win_by_quality = df.groupby('quality_quartile', observed=True)['IS_WINNER'].agg(['sum', 'count', 'mean'])\n",
    "        win_by_quality.columns = ['Wins', 'Total Bids', 'Win Rate']\n",
    "\n",
    "        print(\"\\nWin Rate by Quality Quartile:\")\n",
    "        print(win_by_quality)\n",
    "\n",
    "        # Correlation with ranking\n",
    "        if df['QUALITY'].notna().any() and df['RANKING'].notna().any():\n",
    "            corr_quality_rank = df[['QUALITY', 'RANKING']].corr().loc['QUALITY', 'RANKING']\n",
    "            print(f\"\\nCorrelation (QUALITY vs RANKING): {corr_quality_rank:.4f}\")\n",
    "            print(\"  (Negative correlation expected: higher quality → lower rank number → better position)\")\n",
    "\n",
    "            # Add BID correlation with ranking\n",
    "            if 'FINAL_BID' in df.columns and df['FINAL_BID'].notna().any():\n",
    "                corr_bid_rank = df[['FINAL_BID', 'RANKING']].corr().loc['FINAL_BID', 'RANKING']\n",
    "                print(f\"\\nCorrelation (FINAL_BID vs RANKING): {corr_bid_rank:.4f}\")\n",
    "                print(\"  (Negative correlation expected: higher bid → lower rank number → better position)\")\n",
    "\n",
    "                # Compare magnitudes\n",
    "                print(f\"\\nRelative importance (correlation magnitude):\")\n",
    "                print(f\"  QUALITY:   {abs(corr_quality_rank):.4f}\")\n",
    "                print(f\"  FINAL_BID: {abs(corr_bid_rank):.4f}\")\n",
    "                print(f\"  Bid is {abs(corr_bid_rank)/abs(corr_quality_rank):.1f}x more correlated with rank than quality\")\n",
    "\n",
    "        # LOG-LOG REGRESSION: DECOMPOSE RANK DRIVERS\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RANK DECOMPOSITION: LOG-LOG REGRESSION\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        print(\"\\nUnit of Analysis: Individual bid (18.8M observations)\")\n",
    "        print(\"Model: log(RANKING) ~ log(QUALITY) + log(FINAL_BID) + log(PACING) + PLACEMENT FE\")\n",
    "        print(\"Purpose: Estimate elasticities - % change in rank for % change in inputs\")\n",
    "        print(\"Interpretation: Coefficients = elasticities in log-log specification\")\n",
    "        print(\"Error captures: Vendor effects, product effects, time effects, stochastic variation\")\n",
    "\n",
    "        # Prepare regression data\n",
    "        auctions = journey_data['AUCTIONS_USERS']\n",
    "        reg_df = df[['AUCTION_ID', 'RANKING', 'QUALITY', 'FINAL_BID', 'PACING']].merge(\n",
    "            auctions[['AUCTION_ID', 'PLACEMENT']],\n",
    "            on='AUCTION_ID',\n",
    "            how='left'\n",
    "        )\n",
    "        reg_df = reg_df[(reg_df[['RANKING', 'QUALITY', 'FINAL_BID', 'PACING']] > 0).all(axis=1)]\n",
    "\n",
    "        print(f\"\\nRegression sample: {len(reg_df):,} bids (after dropping zeros/nulls)\")\n",
    "\n",
    "        # Take log transforms\n",
    "        reg_df['log_rank'] = np.log(reg_df['RANKING'])\n",
    "        reg_df['log_quality'] = np.log(reg_df['QUALITY'])\n",
    "        reg_df['log_bid'] = np.log(reg_df['FINAL_BID'])\n",
    "        reg_df['log_pacing'] = np.log(reg_df['PACING'])\n",
    "\n",
    "        # Create placement dummies\n",
    "        placement_dummies = pd.get_dummies(reg_df['PLACEMENT'], prefix='placement', drop_first=True)\n",
    "        reg_df = pd.concat([reg_df, placement_dummies], axis=1)\n",
    "\n",
    "        # Run OLS regression\n",
    "        import statsmodels.api as sm\n",
    "\n",
    "        # Model 1: Just quality and bid\n",
    "        X1 = reg_df[['log_quality', 'log_bid']]\n",
    "        X1 = sm.add_constant(X1)\n",
    "        y = reg_df['log_rank']\n",
    "\n",
    "        model1 = sm.OLS(y, X1).fit()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MODEL 1: log(RANK) ~ log(QUALITY) + log(BID)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"N = {len(reg_df):,}, R² = {model1.rsquared:.4f}\")\n",
    "        print(\"\\nCoefficients (elasticities):\")\n",
    "        print(f\"  Intercept:    {model1.params['const']:8.4f}  (se: {model1.bse['const']:.4f}, t: {model1.tvalues['const']:6.2f})\")\n",
    "        print(f\"  log(QUALITY): {model1.params['log_quality']:8.4f}  (se: {model1.bse['log_quality']:.4f}, t: {model1.tvalues['log_quality']:6.2f})\")\n",
    "        print(f\"  log(BID):     {model1.params['log_bid']:8.4f}  (se: {model1.bse['log_bid']:.4f}, t: {model1.tvalues['log_bid']:6.2f})\")\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(f\"  1% increase in QUALITY → {model1.params['log_quality']:.2f}% change in RANK\")\n",
    "        print(f\"  1% increase in BID     → {model1.params['log_bid']:.2f}% change in RANK\")\n",
    "        print(f\"  BID elasticity is {abs(model1.params['log_bid'])/abs(model1.params['log_quality']):.1f}x larger than QUALITY elasticity\")\n",
    "\n",
    "        # Model 2: Add pacing\n",
    "        X2 = reg_df[['log_quality', 'log_bid', 'log_pacing']]\n",
    "        X2 = sm.add_constant(X2)\n",
    "\n",
    "        model2 = sm.OLS(y, X2).fit()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MODEL 2: log(RANK) ~ log(QUALITY) + log(BID) + log(PACING)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"N = {len(reg_df):,}, R² = {model2.rsquared:.4f}, ΔR² = {model2.rsquared - model1.rsquared:.4f}\")\n",
    "        print(\"\\nCoefficients (elasticities):\")\n",
    "        print(f\"  Intercept:    {model2.params['const']:8.4f}  (se: {model2.bse['const']:.4f}, t: {model2.tvalues['const']:6.2f})\")\n",
    "        print(f\"  log(QUALITY): {model2.params['log_quality']:8.4f}  (se: {model2.bse['log_quality']:.4f}, t: {model2.tvalues['log_quality']:6.2f})\")\n",
    "        print(f\"  log(BID):     {model2.params['log_bid']:8.4f}  (se: {model2.bse['log_bid']:.4f}, t: {model2.tvalues['log_bid']:6.2f})\")\n",
    "        print(f\"  log(PACING):  {model2.params['log_pacing']:8.4f}  (se: {model2.bse['log_pacing']:.4f}, t: {model2.tvalues['log_pacing']:6.2f})\")\n",
    "\n",
    "        # Model 3: Add placement fixed effects\n",
    "        placement_cols = [col for col in reg_df.columns if col.startswith('placement_')]\n",
    "        X3 = reg_df[['log_quality', 'log_bid', 'log_pacing'] + placement_cols]\n",
    "        X3 = sm.add_constant(X3)\n",
    "\n",
    "        model3 = sm.OLS(y, X3).fit()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MODEL 3: log(RANK) ~ log(QUALITY) + log(BID) + log(PACING) + PLACEMENT FE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"N = {len(reg_df):,}, R² = {model3.rsquared:.4f}, ΔR² = {model3.rsquared - model2.rsquared:.4f}\")\n",
    "        print(\"\\nCoefficients (elasticities):\")\n",
    "        print(f\"  Intercept:    {model3.params['const']:8.4f}  (se: {model3.bse['const']:.4f}, t: {model3.tvalues['const']:6.2f})\")\n",
    "        print(f\"  log(QUALITY): {model3.params['log_quality']:8.4f}  (se: {model3.bse['log_quality']:.4f}, t: {model3.tvalues['log_quality']:6.2f})\")\n",
    "        print(f\"  log(BID):     {model3.params['log_bid']:8.4f}  (se: {model3.bse['log_bid']:.4f}, t: {model3.tvalues['log_bid']:6.2f})\")\n",
    "        print(f\"  log(PACING):  {model3.params['log_pacing']:8.4f}  (se: {model3.bse['log_pacing']:.4f}, t: {model3.tvalues['log_pacing']:6.2f})\")\n",
    "        print(f\"\\nPlacement fixed effects:\")\n",
    "        for col in placement_cols:\n",
    "            print(f\"  {col:15s}: {model3.params[col]:8.4f}  (se: {model3.bse[col]:.4f}, t: {model3.tvalues[col]:6.2f})\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SUMMARY: WHAT MOVES RANK?\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"From Model 3 (full specification):\")\n",
    "        print(f\"  1% ↑ BID     → {model3.params['log_bid']:.3f}% change in RANK\")\n",
    "        print(f\"  1% ↑ QUALITY → {model3.params['log_quality']:.3f}% change in RANK\")\n",
    "        print(f\"  1% ↑ PACING  → {model3.params['log_pacing']:.3f}% change in RANK\")\n",
    "        print(f\"\\nBID is {abs(model3.params['log_bid'])/abs(model3.params['log_quality']):.1f}x more important than QUALITY\")\n",
    "        print(f\"R² = {model3.rsquared:.4f} → {(1-model3.rsquared)*100:.1f}% of rank variation unexplained\")\n",
    "        print(f\"  (Unexplained variation due to: vendor effects, product effects, time effects, tie-breaking)\")\n",
    "\n",
    "        # QUALITY vs ACTUAL CTR ANALYSIS\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"QUALITY vs ACTUAL CTR ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        impressions = journey_data['IMPRESSIONS']\n",
    "        clicks = journey_data['CLICKS']\n",
    "\n",
    "        # Join bids with impressions\n",
    "        bid_imp = df[df['IS_WINNER']==True].merge(\n",
    "            impressions[['AUCTION_ID', 'PRODUCT_ID', 'INTERACTION_ID']], \n",
    "            on=['AUCTION_ID', 'PRODUCT_ID'], \n",
    "            how='left',\n",
    "            suffixes=('', '_imp')\n",
    "        )\n",
    "\n",
    "        # Mark if impression led to click\n",
    "        bid_imp['had_impression'] = bid_imp['INTERACTION_ID'].notna()\n",
    "        bid_imp_with_imp = bid_imp[bid_imp['had_impression']].copy()\n",
    "\n",
    "        # Join with clicks to see which impressions got clicked\n",
    "        bid_imp_with_imp = bid_imp_with_imp.merge(\n",
    "            clicks[['AUCTION_ID', 'PRODUCT_ID']],\n",
    "            on=['AUCTION_ID', 'PRODUCT_ID'],\n",
    "            how='left',\n",
    "            indicator='clicked'\n",
    "        )\n",
    "        bid_imp_with_imp['was_clicked'] = (bid_imp_with_imp['clicked'] == 'both').astype(int)\n",
    "\n",
    "        print(f\"\\nBids with impressions tracked: {len(bid_imp_with_imp):,}\")\n",
    "        print(f\"Bids that led to clicks: {bid_imp_with_imp['was_clicked'].sum():,}\")\n",
    "        print(f\"Overall observed CTR: {bid_imp_with_imp['was_clicked'].mean():.4f}\")\n",
    "\n",
    "        # Group by quality buckets and calculate actual CTR\n",
    "        bid_imp_with_imp['quality_bucket'] = pd.qcut(bid_imp_with_imp['QUALITY'], q=10, labels=False, duplicates='drop') + 1\n",
    "\n",
    "        quality_ctr = bid_imp_with_imp.groupby('quality_bucket').agg({\n",
    "            'QUALITY': 'mean',\n",
    "            'was_clicked': ['sum', 'count', 'mean']\n",
    "        })\n",
    "        quality_ctr.columns = ['Avg_Quality', 'Clicks', 'Impressions', 'Actual_CTR']\n",
    "\n",
    "        print(f\"\\nActual CTR by Quality Decile:\")\n",
    "        print(quality_ctr.to_string())\n",
    "\n",
    "        # Correlation between QUALITY and actual CTR\n",
    "        corr_quality_ctr = bid_imp_with_imp[['QUALITY', 'was_clicked']].corr().loc['QUALITY', 'was_clicked']\n",
    "        print(f\"\\nCorrelation (QUALITY score vs Actual CTR): {corr_quality_ctr:.4f}\")\n",
    "\n",
    "        # CONVERSION_RATE vs ACTUAL CONVERSION ANALYSIS\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CONVERSION_RATE vs ACTUAL CONVERSION\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        purchases = journey_data['PURCHASES']\n",
    "\n",
    "        # For clicks, check if they led to purchase\n",
    "        clicks_with_bids = clicks.merge(\n",
    "            df[df['IS_WINNER']==True][['AUCTION_ID', 'PRODUCT_ID', 'CONVERSION_RATE']],\n",
    "            on=['AUCTION_ID', 'PRODUCT_ID'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Join with purchases\n",
    "        clicks_with_bids = clicks_with_bids.merge(\n",
    "            purchases[['USER_ID', 'PRODUCT_ID']],\n",
    "            on=['USER_ID', 'PRODUCT_ID'],\n",
    "            how='left',\n",
    "            indicator='converted'\n",
    "        )\n",
    "        clicks_with_bids['did_convert'] = (clicks_with_bids['converted'] == 'both').astype(int)\n",
    "\n",
    "        print(f\"\\nClicks tracked: {len(clicks_with_bids):,}\")\n",
    "        print(f\"Clicks that converted: {clicks_with_bids['did_convert'].sum():,}\")\n",
    "        print(f\"Overall observed conversion rate: {clicks_with_bids['did_convert'].mean():.4f}\")\n",
    "\n",
    "        # Group by predicted conversion rate and see actual conversion\n",
    "        clicks_with_bids['cr_bucket'] = pd.qcut(clicks_with_bids['CONVERSION_RATE'], q=10, labels=False, duplicates='drop') + 1\n",
    "\n",
    "        cr_actual = clicks_with_bids.groupby('cr_bucket').agg({\n",
    "            'CONVERSION_RATE': 'mean',\n",
    "            'did_convert': ['sum', 'count', 'mean']\n",
    "        })\n",
    "        cr_actual.columns = ['Predicted_CR', 'Conversions', 'Clicks', 'Actual_CR']\n",
    "\n",
    "        print(f\"\\nActual Conversion Rate by Predicted CR Decile:\")\n",
    "        print(cr_actual.to_string())\n",
    "\n",
    "        # Correlation\n",
    "        corr_cr = clicks_with_bids[['CONVERSION_RATE', 'did_convert']].corr().loc['CONVERSION_RATE', 'did_convert']\n",
    "        print(f\"\\nCorrelation (Predicted CONVERSION_RATE vs Actual Conversion): {corr_cr:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[WARNING] QUALITY column has no data\")\n",
    "\n",
    "    print(\"\\n[INFO] Quality score analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bidding_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BIDDING DYNAMICS: FINAL_BID vs PRICE ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BIDDING DYNAMICS: FINAL_BID vs PRICE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = journey_data['AUCTIONS_RESULTS'].copy()\n",
    "    \n",
    "    if 'FINAL_BID' in df.columns and 'PRICE' in df.columns:\n",
    "        # Compare winners vs losers\n",
    "        print(\"\\nWinning Bids:\")\n",
    "        winning_bids = df[df['IS_WINNER'] == True]\n",
    "        if len(winning_bids) > 0 and winning_bids['FINAL_BID'].notna().any():\n",
    "            print(f\"  Count: {len(winning_bids):,}\")\n",
    "            print(f\"  Mean FINAL_BID: {winning_bids['FINAL_BID'].mean():.2f}\")\n",
    "            print(f\"  Mean PRICE: {winning_bids['PRICE'].mean():.2f}\")\n",
    "            if winning_bids['PRICE'].notna().any() and winning_bids['FINAL_BID'].notna().any():\n",
    "                print(f\"  Mean FINAL_BID/PRICE ratio: {(winning_bids['FINAL_BID'] / winning_bids['PRICE']).mean():.4f}\")\n",
    "        else:\n",
    "            print(\"  [WARNING] No data for winning bids\")\n",
    "        \n",
    "        print(\"\\nLosing Bids:\")\n",
    "        losing_bids = df[df['IS_WINNER'] == False]\n",
    "        if len(losing_bids) > 0 and losing_bids['FINAL_BID'].notna().any():\n",
    "            print(f\"  Count: {len(losing_bids):,}\")\n",
    "            print(f\"  Mean FINAL_BID: {losing_bids['FINAL_BID'].mean():.2f}\")\n",
    "            print(f\"  Mean PRICE: {losing_bids['PRICE'].mean():.2f}\")\n",
    "            if losing_bids['PRICE'].notna().any() and losing_bids['FINAL_BID'].notna().any():\n",
    "                print(f\"  Mean FINAL_BID/PRICE ratio: {(losing_bids['FINAL_BID'] / losing_bids['PRICE']).mean():.4f}\")\n",
    "        else:\n",
    "            print(\"  [WARNING] No data for losing bids\")\n",
    "        \n",
    "        # Correlation\n",
    "        if df['FINAL_BID'].notna().any() and df['PRICE'].notna().any():\n",
    "            corr = df[['FINAL_BID', 'PRICE']].corr().loc['FINAL_BID', 'PRICE']\n",
    "            print(f\"\\nCorrelation (FINAL_BID vs PRICE): {corr:.4f}\")\n",
    "        \n",
    "        # BID × QUALITY SCORE ANALYSIS (4×4 MATRIX)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"BID × QUALITY SCORE ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create quartiles\n",
    "        df['bid_quartile'] = pd.qcut(df['FINAL_BID'], q=4, labels=['Bid_Q1_Low', 'Bid_Q2', 'Bid_Q3', 'Bid_Q4_High'], duplicates='drop')\n",
    "        df['quality_quartile'] = pd.qcut(df['QUALITY'], q=4, labels=['Qual_Q1_Low', 'Qual_Q2', 'Qual_Q3', 'Qual_Q4_High'], duplicates='drop')\n",
    "        \n",
    "        # Create combined score\n",
    "        df['score'] = df['FINAL_BID'] * df['QUALITY']\n",
    "        \n",
    "        # Build 4×4 matrix of win rates\n",
    "        matrix = df.groupby(['bid_quartile', 'quality_quartile'], observed=True)['IS_WINNER'].agg(['sum', 'count', 'mean'])\n",
    "        matrix.columns = ['Wins', 'Total', 'Win_Rate']\n",
    "        \n",
    "        print(\"\\nWin Rate by Bid Quartile × Quality Quartile:\")\n",
    "        print(matrix.to_string())\n",
    "        \n",
    "        # Pivot for easier reading\n",
    "        win_rate_pivot = df.pivot_table(\n",
    "            values='IS_WINNER',\n",
    "            index='bid_quartile',\n",
    "            columns='quality_quartile',\n",
    "            aggfunc='mean',\n",
    "            observed=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nWin Rate Matrix (rows=bid, cols=quality):\")\n",
    "        print(win_rate_pivot.to_string())\n",
    "        \n",
    "        # Test if score predicts wins better than bid alone\n",
    "        from scipy.stats import spearmanr\n",
    "        corr_bid_win = df[['FINAL_BID', 'IS_WINNER']].corr(method='spearman').loc['FINAL_BID', 'IS_WINNER']\n",
    "        corr_score_win = df[['score', 'IS_WINNER']].corr(method='spearman').loc['score', 'IS_WINNER']\n",
    "        print(f\"\\nSpearman correlation (BID vs WIN): {corr_bid_win:.4f}\")\n",
    "        print(f\"Spearman correlation (BID×QUALITY vs WIN): {corr_score_win:.4f}\")\n",
    "        \n",
    "        # FIRST-PRICE vs SECOND-PRICE AUCTION DETECTION (EXPANDED)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AUCTION MECHANISM DETECTION (EXPANDED)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        winners_with_price = winning_bids[winning_bids['PRICE'].notna()].copy()\n",
    "        \n",
    "        # Detect auction types with finer granularity\n",
    "        winners_with_price['price_diff'] = winners_with_price['FINAL_BID'] - winners_with_price['PRICE']\n",
    "        winners_with_price['auction_type'] = 'Unknown'\n",
    "        winners_with_price.loc[winners_with_price['price_diff'].abs() < 0.01, 'auction_type'] = 'First-Price'\n",
    "        winners_with_price.loc[winners_with_price['price_diff'] > 0.01, 'auction_type'] = 'Second-Price'\n",
    "        winners_with_price.loc[winners_with_price['price_diff'] < -0.01, 'auction_type'] = 'Anomaly (PRICE > BID)'\n",
    "        \n",
    "        auction_type_dist = winners_with_price['auction_type'].value_counts()\n",
    "        print(f\"\\nAuction type distribution (winners with pricing, N={len(winners_with_price):,}):\")\n",
    "        for atype, count in auction_type_dist.items():\n",
    "            pct = (count / len(winners_with_price)) * 100\n",
    "            print(f\"  {atype:25s}: {count:9,} ({pct:5.2f}%)\")\n",
    "        \n",
    "        # Check for 3rd auction type: Hybrid by placement\n",
    "        auctions = journey_data['AUCTIONS_USERS']\n",
    "        winners_with_placement = winners_with_price.merge(\n",
    "            auctions[['AUCTION_ID', 'PLACEMENT']],\n",
    "            on='AUCTION_ID',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        auction_by_placement = winners_with_placement.groupby(['PLACEMENT', 'auction_type']).size().unstack(fill_value=0)\n",
    "        auction_by_placement_pct = auction_by_placement.div(auction_by_placement.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        print(f\"\\nAuction type distribution by PLACEMENT (%):\")\n",
    "        print(auction_by_placement_pct.to_string())\n",
    "        \n",
    "        # Check temporal patterns (hour of day)\n",
    "        winners_with_placement['hour'] = pd.to_datetime(winners_with_placement['CREATED_AT']).dt.hour\n",
    "        auction_by_hour = winners_with_placement.groupby(['hour', 'auction_type']).size().unstack(fill_value=0)\n",
    "        auction_by_hour_pct = auction_by_hour.div(auction_by_hour.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        print(f\"\\nAuction type stability over time (sample hours):\")\n",
    "        sample_hours = [0, 6, 12, 18]\n",
    "        for h in sample_hours:\n",
    "            if h in auction_by_hour_pct.index:\n",
    "                first_pct = auction_by_hour_pct.loc[h, 'First-Price'] if 'First-Price' in auction_by_hour_pct.columns else 0\n",
    "                second_pct = auction_by_hour_pct.loc[h, 'Second-Price'] if 'Second-Price' in auction_by_hour_pct.columns else 0\n",
    "                print(f\"  Hour {h:02d}: First={first_pct:5.1f}%, Second={second_pct:5.1f}%\")\n",
    "        \n",
    "        # Second-price analysis\n",
    "        second_price = winners_with_price[winners_with_price['auction_type'] == 'Second-Price']\n",
    "        if len(second_price) > 0:\n",
    "            print(f\"\\nSecond-price auction statistics:\")\n",
    "            print(f\"  Count: {len(second_price):,}\")\n",
    "            print(f\"  Mean (FINAL_BID - PRICE): ${second_price['price_diff'].mean():.2f}\")\n",
    "            print(f\"  Median discount: ${second_price['price_diff'].median():.2f}\")\n",
    "            print(f\"  Max discount: ${second_price['price_diff'].max():.2f}\")\n",
    "            print(f\"  Mean savings rate: {(second_price['price_diff'] / second_price['FINAL_BID']).mean():.2%}\")\n",
    "        \n",
    "        # Anomaly investigation\n",
    "        anomaly = winners_with_price[winners_with_price['auction_type'] == 'Anomaly (PRICE > BID)']\n",
    "        if len(anomaly) > 0:\n",
    "            print(f\"\\nAnomaly investigation (PRICE > BID):\")\n",
    "            print(f\"  Count: {len(anomaly):,} ({len(anomaly)/len(winners_with_price)*100:.2f}%)\")\n",
    "            print(f\"  Mean PRICE excess: ${abs(anomaly['price_diff'].mean()):.2f}\")\n",
    "            print(f\"  Max PRICE excess: ${abs(anomaly['price_diff'].min()):.2f}\")\n",
    "            print(f\"  Interpretation: May indicate reserve prices or minimum bid floors\")\n",
    "        \n",
    "        print(f\"\\nConclusion:\")\n",
    "        if auction_type_dist.get('First-Price', 0) > auction_type_dist.get('Second-Price', 0):\n",
    "            print(f\"  Platform runs predominantly FIRST-PRICE auctions ({auction_type_dist.get('First-Price', 0)/len(winners_with_price)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  Platform runs predominantly SECOND-PRICE auctions ({auction_type_dist.get('Second-Price', 0)/len(winners_with_price)*100:.1f}%)\")\n",
    "        \n",
    "        if auction_by_placement_pct.std(axis=0).max() > 5:\n",
    "            print(f\"  ⚠ Auction type varies significantly by PLACEMENT (hybrid system)\")\n",
    "        else:\n",
    "            print(f\"  ✓ Auction type is consistent across placements\")\n",
    "        \n",
    "        # CPC PRICING MODEL VALIDATION\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CPC PRICING MODEL VALIDATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        clicks = journey_data['CLICKS']\n",
    "        \n",
    "        # Check if PRICE is null for winners without clicks\n",
    "        winners_no_click = winning_bids.merge(\n",
    "            clicks[['AUCTION_ID', 'PRODUCT_ID']],\n",
    "            on=['AUCTION_ID', 'PRODUCT_ID'],\n",
    "            how='left',\n",
    "            indicator='has_click'\n",
    "        )\n",
    "        winners_no_click['clicked'] = (winners_no_click['has_click'] == 'both')\n",
    "        \n",
    "        print(f\"\\nWinning bids analysis:\")\n",
    "        print(f\"  Total winners: {len(winners_no_click):,}\")\n",
    "        print(f\"  Winners with clicks: {winners_no_click['clicked'].sum():,}\")\n",
    "        print(f\"  Winners without clicks: {(~winners_no_click['clicked']).sum():,}\")\n",
    "        \n",
    "        # Check PRICE population by click status\n",
    "        clicked_winners = winners_no_click[winners_no_click['clicked']]\n",
    "        no_click_winners = winners_no_click[~winners_no_click['clicked']]\n",
    "        \n",
    "        print(f\"\\nPRICE populated for clicked winners: {clicked_winners['PRICE'].notna().sum():,} / {len(clicked_winners):,} ({clicked_winners['PRICE'].notna().mean():.2%})\")\n",
    "        print(f\"PRICE populated for non-clicked winners: {no_click_winners['PRICE'].notna().sum():,} / {len(no_click_winners):,} ({no_click_winners['PRICE'].notna().mean():.2%})\")\n",
    "        \n",
    "        print(f\"\\nConclusion:\")\n",
    "        if no_click_winners['PRICE'].notna().mean() < 0.1:\n",
    "            print(f\"  ✓ CPC model confirmed: PRICE is mostly null when there's no click\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Unexpected: PRICE populated even without clicks\")\n",
    "            print(f\"     → PRICE likely represents clearing price (not actual charge)\")\n",
    "            print(f\"     → Actual billing happens separately in payment system\")\n",
    "        \n",
    "        # PRICE DISAMBIGUATION: CLEARING PRICE vs CATALOG PRICE\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PRICE FIELD DISAMBIGUATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(\"\\nQuestion: Does PRICE = ad clearing price OR commodity catalog price?\")\n",
    "        \n",
    "        catalog = journey_data['CATALOG']\n",
    "        \n",
    "        # Join winners with catalog\n",
    "        winners_with_catalog = winning_bids.merge(\n",
    "            catalog[['PRODUCT_ID', 'PRICE']],\n",
    "            on='PRODUCT_ID',\n",
    "            how='left',\n",
    "            suffixes=('_bid', '_catalog')\n",
    "        )\n",
    "        \n",
    "        winners_with_both = winners_with_catalog[\n",
    "            winners_with_catalog['PRICE_bid'].notna() & \n",
    "            winners_with_catalog['PRICE_catalog'].notna()\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSample size: {len(winners_with_both):,} winners with both PRICE fields populated\")\n",
    "        \n",
    "        if len(winners_with_both) > 0:\n",
    "            # Test 1: Are they identical?\n",
    "            winners_with_both['price_match'] = (winners_with_both['PRICE_bid'] - winners_with_both['PRICE_catalog']).abs() < 0.01\n",
    "            match_rate = winners_with_both['price_match'].mean()\n",
    "            \n",
    "            print(f\"\\nTest 1: Exact match rate (bid PRICE = catalog PRICE):\")\n",
    "            print(f\"  Match rate: {match_rate:.2%}\")\n",
    "            \n",
    "            if match_rate > 0.9:\n",
    "                print(f\"  → PRICE appears to be CATALOG PRICE (commodity price)\")\n",
    "            else:\n",
    "                print(f\"  → PRICE appears to be CLEARING PRICE (auction outcome)\")\n",
    "            \n",
    "            # Test 2: Correlation\n",
    "            corr_prices = winners_with_both[['PRICE_bid', 'PRICE_catalog']].corr().iloc[0,1]\n",
    "            print(f\"\\nTest 2: Correlation between bid PRICE and catalog PRICE:\")\n",
    "            print(f\"  Correlation: {corr_prices:.4f}\")\n",
    "            \n",
    "            # Test 3: Variation within product\n",
    "            # For same product across multiple auctions, does PRICE vary?\n",
    "            product_price_var = winners_with_catalog.groupby('PRODUCT_ID')['PRICE_bid'].agg(['count', 'std', 'mean'])\n",
    "            product_price_var = product_price_var[product_price_var['count'] >= 5]  # Products with 5+ wins\n",
    "            \n",
    "            if len(product_price_var) > 0:\n",
    "                avg_cv = (product_price_var['std'] / product_price_var['mean']).mean()\n",
    "                print(f\"\\nTest 3: Within-product PRICE variation (N={len(product_price_var):,} products with 5+ wins):\")\n",
    "                print(f\"  Average coefficient of variation: {avg_cv:.4f}\")\n",
    "                \n",
    "                if avg_cv < 0.05:\n",
    "                    print(f\"  → PRICE is nearly constant for same product → CATALOG PRICE\")\n",
    "                else:\n",
    "                    print(f\"  → PRICE varies significantly for same product → CLEARING PRICE\")\n",
    "            \n",
    "            # Test 4: PRICE vs FINAL_BID relationship\n",
    "            winners_with_both['bid_over_catalog'] = winners_with_both['FINAL_BID'] / winners_with_both['PRICE_catalog']\n",
    "            winners_with_both['price_over_catalog'] = winners_with_both['PRICE_bid'] / winners_with_both['PRICE_catalog']\n",
    "            \n",
    "            print(f\"\\nTest 4: Relationship to catalog price:\")\n",
    "            print(f\"  Mean FINAL_BID / catalog_PRICE: {winners_with_both['bid_over_catalog'].mean():.4f}\")\n",
    "            print(f\"  Mean auction_PRICE / catalog_PRICE: {winners_with_both['price_over_catalog'].mean():.4f}\")\n",
    "            \n",
    "            # Test 5: Extreme examples\n",
    "            print(f\"\\nTest 5: Extreme mismatches (bid PRICE >> catalog PRICE):\")\n",
    "            big_diff = winners_with_both[winners_with_both['PRICE_bid'] > winners_with_both['PRICE_catalog'] * 2]\n",
    "            if len(big_diff) > 0:\n",
    "                print(f\"  Cases where bid PRICE > 2x catalog PRICE: {len(big_diff):,} ({len(big_diff)/len(winners_with_both)*100:.2f}%)\")\n",
    "                print(f\"  Mean ratio: {(big_diff['PRICE_bid'] / big_diff['PRICE_catalog']).mean():.2f}x\")\n",
    "                print(f\"  → Suggests PRICE fields measure different things\")\n",
    "            else:\n",
    "                print(f\"  No extreme mismatches found\")\n",
    "                print(f\"  → Suggests PRICE fields may be related\")\n",
    "        \n",
    "        print(f\"\\nFINAL DETERMINATION:\")\n",
    "        if len(winners_with_both) > 0:\n",
    "            if match_rate > 0.9:\n",
    "                print(f\"  PRICE in AUCTIONS_RESULTS = CATALOG PRICE (commodity price)\")\n",
    "                print(f\"  Evidence: {match_rate:.1%} exact match rate\")\n",
    "            elif corr_prices > 0.95:\n",
    "                print(f\"  PRICE in AUCTIONS_RESULTS = CATALOG PRICE (with small markup/fees)\")\n",
    "                print(f\"  Evidence: {corr_prices:.4f} correlation, match_rate={match_rate:.1%}\")\n",
    "            else:\n",
    "                print(f\"  PRICE in AUCTIONS_RESULTS = CLEARING PRICE (auction outcome)\")\n",
    "                print(f\"  Evidence: Low match rate ({match_rate:.1%}), correlation={corr_prices:.4f}\")\n",
    "                print(f\"  Note: PRICE = what advertiser pays for THIS auction\")\n",
    "        else:\n",
    "            print(f\"  INSUFFICIENT DATA: Need products in both AUCTIONS_RESULTS and CATALOG\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n[WARNING] FINAL_BID or PRICE columns not available\")\n",
    "    \n",
    "    print(\"\\n[INFO] Bidding dynamics analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacing_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PACING IMPACT ON BIDS\n",
      "================================================================================\n",
      "\n",
      "Pacing distribution:\n",
      "  Count: 18,838,670\n",
      "  Mean: 0.8910\n",
      "  Std: 0.2634\n",
      "  Min: 0.0067\n",
      "  Max: 1.0000\n",
      "\n",
      "Bids by pacing level:\n",
      "  Low (<0.5)           2,096,661 ( 11.1%)\n",
      "  Medium (0.5-0.9)     1,169,643 (  6.2%)\n",
      "  High (0.9-1.0)       15,572,366 ( 82.7%)\n",
      "  Over (>1.0)                 0 (  0.0%)\n",
      "\n",
      "Win rate by pacing level:\n",
      "                      Wins     Total  Win Rate\n",
      "pacing_bin                                    \n",
      "Low (<0.5)         1224937   2096661  0.584232\n",
      "Medium (0.5-0.9)    953453   1169643  0.815166\n",
      "High (0.9-1.0)    13330714  15572366  0.856049\n",
      "\n",
      "Correlation (PACING vs FINAL_BID): -0.0798\n",
      "\n",
      "[INFO] Pacing analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# --- PACING IMPACT ANALYSIS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PACING IMPACT ON BIDS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = journey_data['AUCTIONS_RESULTS'].copy()\n",
    "    \n",
    "    if 'PACING' in df.columns and df['PACING'].notna().any():\n",
    "        print(f\"\\nPacing distribution:\")\n",
    "        print(f\"  Count: {df['PACING'].count():,}\")\n",
    "        print(f\"  Mean: {df['PACING'].mean():.4f}\")\n",
    "        print(f\"  Std: {df['PACING'].std():.4f}\")\n",
    "        print(f\"  Min: {df['PACING'].min():.4f}\")\n",
    "        print(f\"  Max: {df['PACING'].max():.4f}\")\n",
    "        \n",
    "        # Pacing bins\n",
    "        df['pacing_bin'] = pd.cut(df['PACING'], bins=[0, 0.5, 0.9, 1.0, np.inf], \n",
    "                                   labels=['Low (<0.5)', 'Medium (0.5-0.9)', 'High (0.9-1.0)', 'Over (>1.0)'])\n",
    "        \n",
    "        print(\"\\nBids by pacing level:\")\n",
    "        pacing_dist = df['pacing_bin'].value_counts(sort=False)\n",
    "        for bin_name, count in pacing_dist.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {bin_name:20s} {count:8,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Win rate by pacing\n",
    "        win_by_pacing = df.groupby('pacing_bin', observed=True)['IS_WINNER'].agg(['sum', 'count', 'mean'])\n",
    "        win_by_pacing.columns = ['Wins', 'Total', 'Win Rate']\n",
    "        print(\"\\nWin rate by pacing level:\")\n",
    "        print(win_by_pacing)\n",
    "        \n",
    "        # Correlation with final bid\n",
    "        if 'FINAL_BID' in df.columns and df['FINAL_BID'].notna().any():\n",
    "            corr = df[['PACING', 'FINAL_BID']].corr().loc['PACING', 'FINAL_BID']\n",
    "            print(f\"\\nCorrelation (PACING vs FINAL_BID): {corr:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] PACING column has no data\")\n",
    "    \n",
    "    print(\"\\n[INFO] Pacing analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversion_rate_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSION_RATE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Conversion rate distribution:\n",
      "  Count: 18,838,670\n",
      "  Mean: 0.010004\n",
      "  Std: 0.007716\n",
      "  Min: 0.000001\n",
      "  Median: 0.009010\n",
      "  Max: 0.056500\n",
      "\n",
      "Correlation (CONVERSION_RATE vs QUALITY): 0.1578\n",
      "\n",
      "Conversion rate by bid outcome:\n",
      "  Winners:  Mean CR = 0.010032\n",
      "  Losers:   Mean CR = 0.009875\n",
      "\n",
      "[INFO] Conversion rate analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# --- CONVERSION_RATE ANALYSIS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONVERSION_RATE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = journey_data['AUCTIONS_RESULTS'].copy()\n",
    "    \n",
    "    if 'CONVERSION_RATE' in df.columns and df['CONVERSION_RATE'].notna().any():\n",
    "        print(f\"\\nConversion rate distribution:\")\n",
    "        print(f\"  Count: {df['CONVERSION_RATE'].count():,}\")\n",
    "        print(f\"  Mean: {df['CONVERSION_RATE'].mean():.6f}\")\n",
    "        print(f\"  Std: {df['CONVERSION_RATE'].std():.6f}\")\n",
    "        print(f\"  Min: {df['CONVERSION_RATE'].min():.6f}\")\n",
    "        print(f\"  Median: {df['CONVERSION_RATE'].median():.6f}\")\n",
    "        print(f\"  Max: {df['CONVERSION_RATE'].max():.6f}\")\n",
    "        \n",
    "        # Relationship with quality\n",
    "        if 'QUALITY' in df.columns and df['QUALITY'].notna().any():\n",
    "            corr = df[['CONVERSION_RATE', 'QUALITY']].corr().loc['CONVERSION_RATE', 'QUALITY']\n",
    "            print(f\"\\nCorrelation (CONVERSION_RATE vs QUALITY): {corr:.4f}\")\n",
    "        \n",
    "        # Winners vs losers\n",
    "        print(\"\\nConversion rate by bid outcome:\")\n",
    "        print(f\"  Winners:  Mean CR = {df[df['IS_WINNER']==True]['CONVERSION_RATE'].mean():.6f}\")\n",
    "        print(f\"  Losers:   Mean CR = {df[df['IS_WINNER']==False]['CONVERSION_RATE'].mean():.6f}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] CONVERSION_RATE column has no data\")\n",
    "    \n",
    "    print(\"\\n[INFO] Conversion rate analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funnel_header",
   "metadata": {},
   "source": [
    "## Part 4: Funnel Analysis with Updated Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funnel_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FUNNEL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Funnel stages:\n",
      "  Auctions:      413,457\n",
      "  Bids:         18,838,670\n",
      "  Winners:      15,509,104\n",
      "  Impressions:   533,146\n",
      "  Clicks:         16,706\n",
      "  Purchases:       2,188\n",
      "\n",
      "  Click-through rate: 0.0313 (3.13%)\n",
      "  Click-to-purchase: 0.1310 (13.10%)\n",
      "\n",
      "User-level:\n",
      "  Total users: 4,671\n",
      "  Buyers: 835\n",
      "  Conversion rate: 0.1788 (17.88%)\n",
      "\n",
      "Revenue:\n",
      "  Total: $74,659.00\n",
      "  Per buyer: $89.41\n",
      "  Per user: $15.98\n",
      "\n",
      "================================================================================\n",
      "ENHANCED FUNNEL METRICS WITH PACING & WIN RATES\n",
      "================================================================================\n",
      "\n",
      "Ad Win Rate:\n",
      "  Total bids: 18,838,670\n",
      "  Winning bids: 15,509,104\n",
      "  Overall win rate: 0.8233 (82.33%)\n",
      "\n",
      "Win Rate by Placement:\n",
      "              Wins     Bids  Win_Rate\n",
      "PLACEMENT                            \n",
      "1          1135277  1502474  0.755605\n",
      "2          4285134  5003519  0.856424\n",
      "3          2428435  3040245  0.798763\n",
      "4           142934   178757  0.799599\n",
      "5          7518892  9115603  0.824838\n",
      "\n",
      "Pacing Distribution:\n",
      "count    1.883867e+07\n",
      "mean     8.909944e-01\n",
      "std      2.633870e-01\n",
      "min      6.737947e-03\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      1.000000e+00\n",
      "max      1.000000e+00\n",
      "\n",
      "Average Pacing:\n",
      "  Winners: 0.9207\n",
      "  Losers: 0.7526\n",
      "  Difference: 0.1681\n",
      "\n",
      "Funnel by Pacing Level:\n",
      "                      Bids      Wins  Win_Rate\n",
      "pacing_level                                  \n",
      "Low (<0.5)         2096661   1224937  0.584232\n",
      "Medium (0.5-0.9)   1169643    953453  0.815166\n",
      "High (0.9-1.0)    15572366  13330714  0.856049\n",
      "\n",
      "Impression Delivery Rate by Placement:\n",
      "           Winners  Impressions  Delivery_Rate\n",
      "PLACEMENT                                     \n",
      "1          1135277       280347       0.246941\n",
      "2          4285134       146548       0.034199\n",
      "3          2428435        87694       0.036111\n",
      "4           142934         2898       0.020275\n",
      "5          7518892        15665       0.002083\n",
      "\n",
      "================================================================================\n",
      "COMPLETE FUNNEL BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "Stage-by-stage conversion:\n",
      "  Auctions → Bids: 45.56x (avg bids per auction)\n",
      "  Bids → Winners: 0.8233 (82.33%)\n",
      "  Winners → Impressions: 0.0344 (3.44%)\n",
      "  Impressions → Clicks: 0.0313 (3.13%)\n",
      "  Clicks → Purchases: 0.1310 (13.10%)\n",
      "  Auctions → Purchases (end-to-end): 0.0053 (0.53%)\n",
      "\n",
      "[INFO] Funnel analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# --- FUNNEL ANALYSIS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FUNNEL ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    auctions = journey_data['AUCTIONS_USERS']\n",
    "    bids = journey_data['AUCTIONS_RESULTS']\n",
    "    impressions = journey_data['IMPRESSIONS']\n",
    "    clicks = journey_data['CLICKS']\n",
    "    purchases = journey_data['PURCHASES']\n",
    "    \n",
    "    # Basic funnel metrics\n",
    "    print(\"\\nFunnel stages:\")\n",
    "    print(f\"  Auctions:     {len(auctions):8,}\")\n",
    "    print(f\"  Bids:         {len(bids):8,}\")\n",
    "    print(f\"  Winners:      {bids['IS_WINNER'].sum():8,}\")\n",
    "    print(f\"  Impressions:  {len(impressions):8,}\")\n",
    "    print(f\"  Clicks:       {len(clicks):8,}\")\n",
    "    print(f\"  Purchases:    {len(purchases):8,}\")\n",
    "    \n",
    "    # Conversion rates\n",
    "    if len(impressions) > 0:\n",
    "        ctr = len(clicks) / len(impressions)\n",
    "        print(f\"\\n  Click-through rate: {ctr:.4f} ({ctr*100:.2f}%)\")\n",
    "    \n",
    "    if len(clicks) > 0:\n",
    "        cvr = len(purchases) / len(clicks)\n",
    "        print(f\"  Click-to-purchase: {cvr:.4f} ({cvr*100:.2f}%)\")\n",
    "    \n",
    "    # User-level metrics\n",
    "    n_users = auctions['OPAQUE_USER_ID'].nunique()\n",
    "    n_buyers = purchases['USER_ID'].nunique()\n",
    "    print(f\"\\nUser-level:\")\n",
    "    print(f\"  Total users: {n_users:,}\")\n",
    "    print(f\"  Buyers: {n_buyers:,}\")\n",
    "    print(f\"  Conversion rate: {n_buyers/n_users:.4f} ({n_buyers/n_users*100:.2f}%)\")\n",
    "    \n",
    "    # Revenue\n",
    "    purchases['revenue'] = (purchases['UNIT_PRICE'] * purchases['QUANTITY']) / 100\n",
    "    total_revenue = purchases['revenue'].sum()\n",
    "    print(f\"\\nRevenue:\")\n",
    "    print(f\"  Total: ${total_revenue:,.2f}\")\n",
    "    print(f\"  Per buyer: ${total_revenue/n_buyers:,.2f}\")\n",
    "    print(f\"  Per user: ${total_revenue/n_users:,.2f}\")\n",
    "    \n",
    "    # ENHANCED FUNNEL METRICS\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ENHANCED FUNNEL METRICS WITH PACING & WIN RATES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Ad win rate overall\n",
    "    total_bids = len(bids)\n",
    "    total_winners = bids['IS_WINNER'].sum()\n",
    "    print(f\"\\nAd Win Rate:\")\n",
    "    print(f\"  Total bids: {total_bids:,}\")\n",
    "    print(f\"  Winning bids: {total_winners:,}\")\n",
    "    print(f\"  Overall win rate: {total_winners/total_bids:.4f} ({total_winners/total_bids*100:.2f}%)\")\n",
    "    \n",
    "    # Win rate by placement\n",
    "    auction_bids = bids.merge(auctions[['AUCTION_ID', 'PLACEMENT']], on='AUCTION_ID', how='left')\n",
    "    win_by_placement = auction_bids.groupby('PLACEMENT')['IS_WINNER'].agg(['sum', 'count', 'mean'])\n",
    "    win_by_placement.columns = ['Wins', 'Bids', 'Win_Rate']\n",
    "    print(f\"\\nWin Rate by Placement:\")\n",
    "    print(win_by_placement.to_string())\n",
    "    \n",
    "    # Pacing distribution in funnel context\n",
    "    print(f\"\\nPacing Distribution:\")\n",
    "    pacing_dist = bids['PACING'].describe()\n",
    "    print(pacing_dist.to_string())\n",
    "    \n",
    "    # Average pacing for winners vs losers\n",
    "    avg_pacing_winners = bids[bids['IS_WINNER']==True]['PACING'].mean()\n",
    "    avg_pacing_losers = bids[bids['IS_WINNER']==False]['PACING'].mean()\n",
    "    print(f\"\\nAverage Pacing:\")\n",
    "    print(f\"  Winners: {avg_pacing_winners:.4f}\")\n",
    "    print(f\"  Losers: {avg_pacing_losers:.4f}\")\n",
    "    print(f\"  Difference: {avg_pacing_winners - avg_pacing_losers:.4f}\")\n",
    "    \n",
    "    # Pacing level breakdown\n",
    "    bids['pacing_level'] = pd.cut(bids['PACING'], bins=[0, 0.5, 0.9, 1.0, np.inf], \n",
    "                                   labels=['Low (<0.5)', 'Medium (0.5-0.9)', 'High (0.9-1.0)', 'Over (>1.0)'])\n",
    "    pacing_funnel = bids.groupby('pacing_level', observed=True).agg({\n",
    "        'AUCTION_ID': 'count',\n",
    "        'IS_WINNER': ['sum', 'mean']\n",
    "    })\n",
    "    pacing_funnel.columns = ['Bids', 'Wins', 'Win_Rate']\n",
    "    print(f\"\\nFunnel by Pacing Level:\")\n",
    "    print(pacing_funnel.to_string())\n",
    "    \n",
    "    # Impression delivery rate by placement\n",
    "    print(f\"\\nImpression Delivery Rate by Placement:\")\n",
    "    winners_by_placement = auction_bids[auction_bids['IS_WINNER']==True].groupby('PLACEMENT').size()\n",
    "    \n",
    "    impressions_with_placement = impressions.merge(\n",
    "        auctions[['AUCTION_ID', 'PLACEMENT']], \n",
    "        on='AUCTION_ID', \n",
    "        how='left'\n",
    "    )\n",
    "    impressions_by_placement = impressions_with_placement.groupby('PLACEMENT').size()\n",
    "    \n",
    "    delivery_rate = pd.DataFrame({\n",
    "        'Winners': winners_by_placement,\n",
    "        'Impressions': impressions_by_placement\n",
    "    })\n",
    "    delivery_rate['Delivery_Rate'] = delivery_rate['Impressions'] / delivery_rate['Winners']\n",
    "    print(delivery_rate.to_string())\n",
    "    \n",
    "    # Full funnel breakdown\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPLETE FUNNEL BREAKDOWN\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nStage-by-stage conversion:\")\n",
    "    print(f\"  Auctions → Bids: {total_bids/len(auctions):.2f}x (avg bids per auction)\")\n",
    "    print(f\"  Bids → Winners: {total_winners/total_bids:.4f} ({total_winners/total_bids*100:.2f}%)\")\n",
    "    print(f\"  Winners → Impressions: {len(impressions)/total_winners:.4f} ({len(impressions)/total_winners*100:.2f}%)\")\n",
    "    print(f\"  Impressions → Clicks: {len(clicks)/len(impressions):.4f} ({len(clicks)/len(impressions)*100:.2f}%)\")\n",
    "    print(f\"  Clicks → Purchases: {len(purchases)/len(clicks):.4f} ({len(purchases)/len(clicks)*100:.2f}%)\")\n",
    "    print(f\"  Auctions → Purchases (end-to-end): {len(purchases)/len(auctions):.4f} ({len(purchases)/len(auctions)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n[INFO] Funnel analysis complete.\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Part 5: Summary & Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY & DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "Data pull configuration:\n",
      "  Date range: 2025-09-27 to 2025-10-11 (14 days)\n",
      "  Sampling fraction: 0.1000%\n",
      "  Users sampled: 4,671\n",
      "\n",
      "New schema fields coverage:\n",
      "  AUCTIONS_USERS.PLACEMENT: 100.0% populated\n",
      "  AUCTIONS_RESULTS.QUALITY: 100.0% populated\n",
      "  AUCTIONS_RESULTS.FINAL_BID: 100.0% populated\n",
      "  AUCTIONS_RESULTS.PRICE: 82.3% populated\n",
      "  AUCTIONS_RESULTS.CONVERSION_RATE: 100.0% populated\n",
      "  AUCTIONS_RESULTS.PACING: 100.0% populated\n",
      "\n",
      "Data quality flags:\n",
      "  ⚠ Issues detected:\n",
      "    - Low impression-to-bid match rate: 3.44%\n",
      "\n",
      "[SUCCESS] Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# --- SUMMARY STATISTICS ---\n",
    "\n",
    "if 'journey_data' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY & DATA QUALITY ASSESSMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nData pull configuration:\")\n",
    "    print(f\"  Date range: {start_date_str} to {end_date_str} ({DAYS_WINDOW} days)\")\n",
    "    print(f\"  Sampling fraction: {SAMPLING_FRACTION:.4%}\")\n",
    "    print(f\"  Users sampled: {journey_data['AUCTIONS_USERS']['OPAQUE_USER_ID'].nunique():,}\")\n",
    "    \n",
    "    print(f\"\\nNew schema fields coverage:\")\n",
    "    \n",
    "    # AUCTIONS_USERS: PLACEMENT\n",
    "    placement_coverage = (1 - journey_data['AUCTIONS_USERS']['PLACEMENT'].isna().mean()) * 100\n",
    "    print(f\"  AUCTIONS_USERS.PLACEMENT: {placement_coverage:.1f}% populated\")\n",
    "    \n",
    "    # AUCTIONS_RESULTS: new columns\n",
    "    df_bids = journey_data['AUCTIONS_RESULTS']\n",
    "    for col in ['QUALITY', 'FINAL_BID', 'PRICE', 'CONVERSION_RATE', 'PACING']:\n",
    "        if col in df_bids.columns:\n",
    "            coverage = (1 - df_bids[col].isna().mean()) * 100\n",
    "            print(f\"  AUCTIONS_RESULTS.{col}: {coverage:.1f}% populated\")\n",
    "        else:\n",
    "            print(f\"  AUCTIONS_RESULTS.{col}: NOT FOUND\")\n",
    "    \n",
    "    print(f\"\\nData quality flags:\")\n",
    "    \n",
    "    # Check for obvious issues\n",
    "    issues = []\n",
    "    \n",
    "    # Check if new columns are mostly empty\n",
    "    if 'PLACEMENT' in journey_data['AUCTIONS_USERS'].columns:\n",
    "        if journey_data['AUCTIONS_USERS']['PLACEMENT'].isna().mean() > 0.9:\n",
    "            issues.append(\"PLACEMENT column >90% null\")\n",
    "    \n",
    "    for col in ['QUALITY', 'FINAL_BID', 'PRICE', 'CONVERSION_RATE', 'PACING']:\n",
    "        if col in df_bids.columns:\n",
    "            if df_bids[col].isna().mean() > 0.9:\n",
    "                issues.append(f\"{col} column >90% null\")\n",
    "    \n",
    "    # Check funnel integrity\n",
    "    winning_bids = df_bids['IS_WINNER'].sum()\n",
    "    impressions = len(journey_data['IMPRESSIONS'])\n",
    "    if impressions > 0:\n",
    "        match_rate = impressions / winning_bids if winning_bids > 0 else 0\n",
    "        if match_rate < 0.1:\n",
    "            issues.append(f\"Low impression-to-bid match rate: {match_rate:.2%}\")\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"  ✓ No major issues detected\")\n",
    "    else:\n",
    "        print(\"  ⚠ Issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"    - {issue}\")\n",
    "    \n",
    "    print(\"\\n[SUCCESS] Analysis complete!\")\n",
    "else:\n",
    "    print(\"[ERROR] Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8e2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b28de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6fa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

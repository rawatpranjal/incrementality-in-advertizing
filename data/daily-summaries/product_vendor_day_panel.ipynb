{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af342e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY',\n",
    "    schema='INCREMENTALITY_RESEARCH'\n",
    ")\n",
    "\n",
    "print(\"Connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b171d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: SETUP ===\n",
      "Analysis Period: 2025-03-14 to 2025-09-07\n",
      "Total days to process: 178\n",
      "Output directory: /Users/pranjal/Code/marketplace-incrementality/daily_summaries/data/panels\n",
      "\n",
      "Creating master catalog (Product -> Campaign -> Vendor mapping)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a401032fbe477f97cf965b79062bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Master Catalog:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master catalog created: 8,490,131 unique product-vendor-campaign mappings\n",
      "\n",
      "=== PHASE 2: PRODUCT-DAY PANEL ASSEMBLY ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bd85074e244fae82d39d9b5e3acfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Product-Day Data:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No product-day data to save\n",
      "\n",
      "=== PHASE 3: VENDOR-DAY PANEL ASSEMBLY ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa2e32c316f4b9bbdca0a53f07cf5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Vendor-Day Data:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No vendor-day data to save\n",
      "\n",
      "=== PANEL CREATION COMPLETE ===\n",
      "Output directory: /Users/pranjal/Code/marketplace-incrementality/daily_summaries/data/panels\n",
      "Files created:\n",
      "  - /Users/pranjal/Code/marketplace-incrementality/daily_summaries/data/panels/product_day_panel.parquet\n",
      "  - /Users/pranjal/Code/marketplace-incrementality/daily_summaries/data/panels/vendor_day_panel.parquet\n",
      "  - master_catalog.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PHASE 1: ONE-TIME SETUP\n",
    "print(\"=== PHASE 1: SETUP ===\")\n",
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path(\"/Users/pranjal/Code/marketplace-incrementality/daily_summaries/data\")\n",
    "AUCTIONS_DIR = BASE_PATH / \"product_daily_auctions_dataset\"\n",
    "IMPRESSIONS_DIR = BASE_PATH / \"product_daily_impressions_dataset\"\n",
    "CLICKS_DIR = BASE_PATH / \"product_daily_clicks_dataset\"\n",
    "PURCHASES_DIR = BASE_PATH / \"product_daily_purchases_dataset\"\n",
    "\n",
    "# Create panels output directory\n",
    "PANELS_DIR = BASE_PATH / \"panels\"\n",
    "PANELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Define final output paths\n",
    "PRODUCT_PANEL_PATH = PANELS_DIR / \"product_day_panel.parquet\"\n",
    "VENDOR_PANEL_PATH = PANELS_DIR / \"vendor_day_panel.parquet\"\n",
    "\n",
    "# Analysis period\n",
    "ANALYSIS_START = date(2025, 3, 14)\n",
    "ANALYSIS_END = date(2025, 9, 7)\n",
    "date_list = pd.date_range(start=ANALYSIS_START, end=ANALYSIS_END, freq='D')\n",
    "\n",
    "print(f\"Analysis Period: {ANALYSIS_START} to {ANALYSIS_END}\")\n",
    "print(f\"Total days to process: {len(date_list)}\")\n",
    "print(f\"Output directory: {PANELS_DIR}\")\n",
    "\n",
    "# Create master catalog from impressions data\n",
    "print(\"\\nCreating master catalog (Product -> Campaign -> Vendor mapping)...\")\n",
    "master_catalog_rows = []\n",
    "\n",
    "for current_date in tqdm(date_list[:10], desc=\"Building Master Catalog\"):  # Sample from first 10 days\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    impressions_file = IMPRESSIONS_DIR / f\"data_{date_str}.parquet\"\n",
    "\n",
    "    if impressions_file.exists():\n",
    "        df_imp = pd.read_parquet(impressions_file)\n",
    "        catalog_day = df_imp[['PRODUCT_ID', 'VENDOR_ID', 'CAMPAIGN_ID']].drop_duplicates()\n",
    "        master_catalog_rows.append(catalog_day)\n",
    "\n",
    "if master_catalog_rows:\n",
    "    master_catalog = pd.concat(master_catalog_rows, ignore_index=True).drop_duplicates()\n",
    "    master_catalog.to_parquet(PANELS_DIR / \"master_catalog.parquet\", index=False)\n",
    "    print(f\"Master catalog created: {len(master_catalog):,} unique product-vendor-campaign mappings\")\n",
    "else:\n",
    "    print(\"No impressions data found for master catalog\")\n",
    "    master_catalog = pd.DataFrame(columns=['PRODUCT_ID', 'VENDOR_ID', 'CAMPAIGN_ID'])\n",
    "\n",
    "# PHASE 2: DAY-BY-DAY ASSEMBLY OF PRODUCT-DAY PANEL\n",
    "print(\"\\n=== PHASE 2: PRODUCT-DAY PANEL ASSEMBLY ===\")\n",
    "\n",
    "product_day_dfs = []\n",
    "\n",
    "for current_date in tqdm(date_list, desc=\"Processing Product-Day Data\"):\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Load daily files (create empty if missing)\n",
    "    def load_or_empty(file_path, expected_columns):\n",
    "        if file_path.exists():\n",
    "            return pd.read_parquet(file_path)\n",
    "        else:\n",
    "            empty_df = pd.DataFrame(columns=expected_columns)\n",
    "            return empty_df\n",
    "\n",
    "    # Load all four datasets for this day\n",
    "    auctions_file = AUCTIONS_DIR / f\"data_{date_str}.parquet\"\n",
    "    impressions_file = IMPRESSIONS_DIR / f\"data_{date_str}.parquet\"\n",
    "    clicks_file = CLICKS_DIR / f\"data_{date_str}.parquet\"\n",
    "    purchases_file = PURCHASES_DIR / f\"data_{date_str}.parquet\"\n",
    "\n",
    "    # Expected columns for empty DataFrames\n",
    "    auctions_cols = ['PRODUCT_ID', 'DATE', 'VENDOR_ID', 'CAMPAIGN_ID', 'PRODUCT_AUCTIONS_COUNT',\n",
    "                    'TOTAL_BIDS_FOR_PRODUCT', 'TOTAL_WINS_FOR_PRODUCT', 'AVG_BID_RANK_FOR_PRODUCT',\n",
    "                    'DISTINCT_BIDDERS_FOR_PRODUCT', 'BEST_RANK_FOR_PRODUCT', 'WORST_RANK_FOR_PRODUCT']\n",
    "\n",
    "    impressions_cols = ['PRODUCT_ID', 'DATE', 'VENDOR_ID', 'CAMPAIGN_ID', 'TOTAL_IMPRESSIONS',\n",
    "                        'IMPRESSIONS', 'DISTINCT_USERS_IMPRESSED']\n",
    "\n",
    "    clicks_cols = ['PRODUCT_ID', 'DATE', 'VENDOR_ID', 'CAMPAIGN_ID', 'TOTAL_CLICKS',\n",
    "                    'CLICKS', 'DISTINCT_USERS_CLICKED']\n",
    "\n",
    "    purchases_cols = ['PRODUCT_ID', 'DATE', 'PURCHASES', 'LINES_SOLD', 'UNITS_SOLD',\n",
    "                    'REVENUE_CENTS', 'AVG_UNIT_PRICE_CENTS', 'MIN_UNIT_PRICE_CENTS',\n",
    "                    'MAX_UNIT_PRICE_CENTS', 'STDDEV_UNIT_PRICE_CENTS', 'DISTINCT_USERS_PURCHASED']\n",
    "\n",
    "    df_auctions = load_or_empty(auctions_file, auctions_cols)\n",
    "    df_impressions = load_or_empty(impressions_file, impressions_cols)\n",
    "    df_clicks = load_or_empty(clicks_file, clicks_cols)\n",
    "    df_purchases = load_or_empty(purchases_file, purchases_cols)\n",
    "\n",
    "    # Start with auctions as base (has vendor_id, campaign_id)\n",
    "    if not df_auctions.empty:\n",
    "        df_daily = df_auctions.copy()\n",
    "    else:\n",
    "        # If no auctions, create minimal structure with date\n",
    "        df_daily = pd.DataFrame({'PRODUCT_ID': [], 'DATE': pd.to_datetime(date_str)})\n",
    "        continue  # Skip days with no auction data\n",
    "\n",
    "    # Outer join with impressions\n",
    "    if not df_impressions.empty:\n",
    "        df_daily = df_daily.merge(\n",
    "            df_impressions[['PRODUCT_ID', 'TOTAL_IMPRESSIONS', 'IMPRESSIONS', 'DISTINCT_USERS_IMPRESSED']],\n",
    "            on='PRODUCT_ID', how='outer'\n",
    "        )\n",
    "    else:\n",
    "        df_daily[['TOTAL_IMPRESSIONS', 'IMPRESSIONS', 'DISTINCT_USERS_IMPRESSED']] = 0\n",
    "\n",
    "    # Outer join with clicks\n",
    "    if not df_clicks.empty:\n",
    "        df_daily = df_daily.merge(\n",
    "            df_clicks[['PRODUCT_ID', 'TOTAL_CLICKS', 'CLICKS', 'DISTINCT_USERS_CLICKED']],\n",
    "            on='PRODUCT_ID', how='outer'\n",
    "        )\n",
    "    else:\n",
    "        df_daily[['TOTAL_CLICKS', 'CLICKS', 'DISTINCT_USERS_CLICKED']] = 0\n",
    "\n",
    "    # Outer join with purchases\n",
    "    if not df_purchases.empty:\n",
    "        df_daily = df_daily.merge(df_purchases, on='PRODUCT_ID', how='outer')\n",
    "    else:\n",
    "        for col in purchases_cols[2:]:  # Skip PRODUCT_ID and DATE\n",
    "            df_daily[col] = 0\n",
    "\n",
    "    # Clean merged DataFrame\n",
    "    # Fill nulls with 0 for all metric columns\n",
    "    metric_columns = [col for col in df_daily.columns if col not in ['PRODUCT_ID', 'DATE', 'VENDOR_ID', 'CAMPAIGN_ID']]\n",
    "    df_daily[metric_columns] = df_daily[metric_columns].fillna(0)\n",
    "\n",
    "    # Ensure DATE column is properly set\n",
    "    df_daily['DATE'] = pd.to_datetime(date_str)\n",
    "\n",
    "    # Append to list\n",
    "    if not df_daily.empty:\n",
    "        product_day_dfs.append(df_daily)\n",
    "\n",
    "# Concatenate and save product-day panel\n",
    "if product_day_dfs:\n",
    "    df_product_day_final = pd.concat(product_day_dfs, ignore_index=True)\n",
    "    df_product_day_final.to_parquet(PRODUCT_PANEL_PATH, index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "    print(f\"✅ Product-Day Panel Created:\")\n",
    "    print(f\"   Shape: {df_product_day_final.shape}\")\n",
    "    print(f\"   Date range: {df_product_day_final['DATE'].min().date()} to {df_product_day_final['DATE'].max().date()}\")\n",
    "    print(f\"   Unique products: {df_product_day_final['PRODUCT_ID'].nunique():,}\")\n",
    "    print(f\"   Saved to: {PRODUCT_PANEL_PATH}\")\n",
    "else:\n",
    "    print(\"❌ No product-day data to save\")\n",
    "\n",
    "# PHASE 3: DAY-BY-DAY ASSEMBLY OF VENDOR-DAY PANEL\n",
    "print(\"\\n=== PHASE 3: VENDOR-DAY PANEL ASSEMBLY ===\")\n",
    "\n",
    "vendor_day_dfs = []\n",
    "\n",
    "# Process each day from the product-day data\n",
    "for current_date in tqdm(date_list, desc=\"Processing Vendor-Day Data\"):\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter product-day data for this specific date\n",
    "    if 'df_product_day_final' in locals():\n",
    "        df_daily_products = df_product_day_final[df_product_day_final['DATE'] == pd.to_datetime(date_str)]\n",
    "\n",
    "        if not df_daily_products.empty and 'VENDOR_ID' in df_daily_products.columns:\n",
    "            # Group by vendor_id and aggregate\n",
    "            vendor_agg = df_daily_products.groupby('VENDOR_ID').agg({\n",
    "                'PRODUCT_ID': 'nunique',  # distinct_products_active\n",
    "                'PRODUCT_AUCTIONS_COUNT': 'sum',\n",
    "                'TOTAL_BIDS_FOR_PRODUCT': 'sum',\n",
    "                'TOTAL_WINS_FOR_PRODUCT': 'sum',\n",
    "                'TOTAL_IMPRESSIONS': 'sum',\n",
    "                'IMPRESSIONS': 'sum',\n",
    "                'CLICKS': 'sum',\n",
    "                'PURCHASES': 'sum',\n",
    "                'UNITS_SOLD': 'sum',\n",
    "                'REVENUE_CENTS': 'sum',\n",
    "                'DISTINCT_USERS_IMPRESSED': 'sum',\n",
    "                'DISTINCT_USERS_CLICKED': 'sum',\n",
    "                'DISTINCT_USERS_PURCHASED': 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Rename columns\n",
    "            vendor_agg.rename(columns={'PRODUCT_ID': 'DISTINCT_PRODUCTS_ACTIVE'}, inplace=True)\n",
    "\n",
    "            # Add date\n",
    "            vendor_agg['DATE'] = pd.to_datetime(date_str)\n",
    "\n",
    "            # Calculate ratios\n",
    "            vendor_agg['WIN_RATE'] = vendor_agg['TOTAL_WINS_FOR_PRODUCT'] / vendor_agg['TOTAL_BIDS_FOR_PRODUCT'].replace(0, 1)\n",
    "            vendor_agg['REVENUE_DOLLARS'] = vendor_agg['REVENUE_CENTS'] / 100.0\n",
    "\n",
    "            vendor_day_dfs.append(vendor_agg)\n",
    "\n",
    "# Concatenate and save vendor-day panel\n",
    "if vendor_day_dfs:\n",
    "    df_vendor_day_final = pd.concat(vendor_day_dfs, ignore_index=True)\n",
    "    df_vendor_day_final.to_parquet(VENDOR_PANEL_PATH, index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "    print(f\"✅ Vendor-Day Panel Created:\")\n",
    "    print(f\"   Shape: {df_vendor_day_final.shape}\")\n",
    "    print(f\"   Date range: {df_vendor_day_final['DATE'].min().date()} to {df_vendor_day_final['DATE'].max().date()}\")\n",
    "    print(f\"   Unique vendors: {df_vendor_day_final['VENDOR_ID'].nunique():,}\")\n",
    "    print(f\"   Saved to: {VENDOR_PANEL_PATH}\")\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample of vendor-day data:\")\n",
    "    print(df_vendor_day_final.head().to_markdown(index=False))\n",
    "else:\n",
    "    print(\"❌ No vendor-day data to save\")\n",
    "\n",
    "print(f\"\\n=== PANEL CREATION COMPLETE ===\")\n",
    "print(f\"Output directory: {PANELS_DIR}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - {PRODUCT_PANEL_PATH}\")\n",
    "print(f\"  - {VENDOR_PANEL_PATH}\")\n",
    "print(f\"  - master_catalog.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: SETUP ===\n",
      "Analysis Period: 2025-03-14 to 2025-09-07\n",
      "Total days to process: 178\n",
      "\n",
      "=== PHASE 2: PRODUCT-DAY PANEL ASSEMBLY (FIXED) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990c91701b804bc9bd4ae83b6ca520af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Product-Day Data:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PHASE 1: SETUP (same as before)\n",
    "print(\"=== PHASE 1: SETUP ===\")\n",
    "BASE_PATH = Path(\"/Users/pranjal/Code/marketplace-incrementality/daily_summaries/data\")\n",
    "AUCTIONS_DIR = BASE_PATH / \"product_daily_auctions_dataset\"\n",
    "IMPRESSIONS_DIR = BASE_PATH / \"product_daily_impressions_dataset\"\n",
    "CLICKS_DIR = BASE_PATH / \"product_daily_clicks_dataset\"\n",
    "PURCHASES_DIR = BASE_PATH / \"product_daily_purchases_dataset\"\n",
    "PANELS_DIR = BASE_PATH / \"panels\"\n",
    "PANELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ANALYSIS_START = date(2025, 3, 14)\n",
    "ANALYSIS_END = date(2025, 9, 7)\n",
    "date_list = pd.date_range(start=ANALYSIS_START, end=ANALYSIS_END, freq='D')\n",
    "\n",
    "print(f\"Analysis Period: {ANALYSIS_START} to {ANALYSIS_END}\")\n",
    "print(f\"Total days to process: {len(date_list)}\")\n",
    "\n",
    "# PHASE 2: MODIFIED PRODUCT-DAY PANEL ASSEMBLY\n",
    "print(\"\\n=== PHASE 2: PRODUCT-DAY PANEL ASSEMBLY (FIXED) ===\")\n",
    "\n",
    "product_day_dfs = []\n",
    "debug_info = {\"auctions\": 0, \"impressions\": 0, \"clicks\": 0, \"purchases\": 0, \"processed\": 0}\n",
    "\n",
    "for current_date in tqdm(date_list, desc=\"Processing Product-Day Data\"):\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Check which files exist for this date\n",
    "    auctions_file = AUCTIONS_DIR / f\"data_{date_str}.parquet\"\n",
    "    impressions_file = IMPRESSIONS_DIR / f\"data_{date_str}.parquet\"\n",
    "    clicks_file = CLICKS_DIR / f\"data_{date_str}.parquet\"\n",
    "    purchases_file = PURCHASES_DIR / f\"data_{date_str}.parquet\"\n",
    "\n",
    "    files_exist = {\n",
    "        'auctions': auctions_file.exists(),\n",
    "        'impressions': impressions_file.exists(),\n",
    "        'clicks': clicks_file.exists(),\n",
    "        'purchases': purchases_file.exists()\n",
    "    }\n",
    "\n",
    "    # Update debug counters\n",
    "    for key, exists in files_exist.items():\n",
    "        if exists:\n",
    "            debug_info[key] += 1\n",
    "\n",
    "    # Strategy: Use impressions as base if auctions not available (impressions has vendor_id)\n",
    "    df_daily = None\n",
    "    base_dataset = None\n",
    "\n",
    "    # Try to establish a base dataset with PRODUCT_ID and VENDOR_ID\n",
    "    if files_exist['auctions']:\n",
    "        df_daily = pd.read_parquet(auctions_file)\n",
    "        base_dataset = \"auctions\"\n",
    "    elif files_exist['impressions']:\n",
    "        df_daily = pd.read_parquet(impressions_file)\n",
    "        base_dataset = \"impressions\"\n",
    "    elif files_exist['clicks']:\n",
    "        df_daily = pd.read_parquet(clicks_file)\n",
    "        base_dataset = \"clicks\"\n",
    "    elif files_exist['purchases']:\n",
    "        df_daily = pd.read_parquet(purchases_file)\n",
    "        base_dataset = \"purchases\"\n",
    "        # Purchases doesn't have vendor_id, we'll need to add it later\n",
    "\n",
    "    if df_daily is None or df_daily.empty:\n",
    "        continue\n",
    "\n",
    "    # Ensure we have the basic columns\n",
    "    if 'PRODUCT_ID' not in df_daily.columns:\n",
    "        continue\n",
    "\n",
    "    # Add DATE column\n",
    "    df_daily['DATE'] = pd.to_datetime(date_str)\n",
    "\n",
    "    # Merge other datasets\n",
    "    merge_datasets = [\n",
    "        (impressions_file, 'impressions', files_exist['impressions']),\n",
    "        (clicks_file, 'clicks', files_exist['clicks']),\n",
    "        (purchases_file, 'purchases', files_exist['purchases']),\n",
    "        (auctions_file, 'auctions', files_exist['auctions'])\n",
    "    ]\n",
    "\n",
    "    for file_path, dataset_name, exists in merge_datasets:\n",
    "        if exists and dataset_name != base_dataset:\n",
    "            try:\n",
    "                df_merge = pd.read_parquet(file_path)\n",
    "                if not df_merge.empty and 'PRODUCT_ID' in df_merge.columns:\n",
    "\n",
    "                    # Get columns to merge (exclude overlapping metadata columns)\n",
    "                    merge_cols = [col for col in df_merge.columns\n",
    "                                if col not in ['DATE', 'VENDOR_ID', 'CAMPAIGN_ID'] or col == 'PRODUCT_ID']\n",
    "\n",
    "                    df_daily = df_daily.merge(\n",
    "                        df_merge[merge_cols],\n",
    "                        on='PRODUCT_ID',\n",
    "                        how='outer',\n",
    "                        suffixes=('', f'_{dataset_name}')\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging {dataset_name} for {date_str}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Fill nulls with 0 for metric columns\n",
    "    numeric_columns = df_daily.select_dtypes(include=['number']).columns\n",
    "    df_daily[numeric_columns] = df_daily[numeric_columns].fillna(0)\n",
    "\n",
    "    # Add to list\n",
    "    if not df_daily.empty:\n",
    "        product_day_dfs.append(df_daily)\n",
    "        debug_info['processed'] += 1\n",
    "\n",
    "print(f\"\\nDebug Info:\")\n",
    "print(f\"  Days with auctions: {debug_info['auctions']}\")\n",
    "print(f\"  Days with impressions: {debug_info['impressions']}\")\n",
    "print(f\"  Days with clicks: {debug_info['clicks']}\")\n",
    "print(f\"  Days with purchases: {debug_info['purchases']}\")\n",
    "print(f\"  Days actually processed: {debug_info['processed']}\")\n",
    "\n",
    "# Save product-day panel\n",
    "if product_day_dfs:\n",
    "    df_product_day_final = pd.concat(product_day_dfs, ignore_index=True)\n",
    "\n",
    "    # Save to parquet\n",
    "    PRODUCT_PANEL_PATH = PANELS_DIR / \"product_day_panel.parquet\"\n",
    "    df_product_day_final.to_parquet(PRODUCT_PANEL_PATH, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Product-Day Panel Created:\")\n",
    "    print(f\"   Shape: {df_product_day_final.shape}\")\n",
    "    print(f\"   Columns: {list(df_product_day_final.columns)}\")\n",
    "    print(f\"   Date range: {df_product_day_final['DATE'].min().date()} to {df_product_day_final['DATE'].max().date()}\")\n",
    "    print(f\"   Unique products: {df_product_day_final['PRODUCT_ID'].nunique():,}\")\n",
    "    print(f\"   Saved to: {PRODUCT_PANEL_PATH}\")\n",
    "3\n",
    "    # Show sample\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df_product_day_final.head(3).to_string())\n",
    "\n",
    "else:\n",
    "    print(\"❌ No product-day data to save\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSIS COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

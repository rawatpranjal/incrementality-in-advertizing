{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY',\n",
    "    schema='INCREMENTALITY_RESEARCH'\n",
    ")\n",
    "\n",
    "print(\"Connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10 rows from PRODUCT_CATALOG\n",
      "              PRODUCT_ID                                                                         NAME  ACTIVE                                                                                                                                                                                                                                                                                                                                        CATEGORIES                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     DESCRIPTION  PRICE                                             VENDORS  IS_DELETED\n",
      "67cdf64369ef1ac7493985d4                   Lauren Merkin Black Leather Clutch Evening Bag Party Purse    True [\\n  \"domain#us\",\\n  \"brand#lauren merkin\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#00248975d97b4e80ef00a955\",\\n  \"category_feature#00ee9287d97b4e80ef00a955\",\\n  \"color#black\",\\n  \"style_tag#leatherclutch\",\\n  \"style_tag#eveningbag\",\\n  \"style_tag#partypurse\",\\n  \"size#us os\",\\n  \"created_at#2025-03-09T13:12:52-07:00\"\\n] A sleek silhouette defines this Lauren Merkin clutch, a sophisticated accessory that exudes understated elegance. Crafted in supple black leather, the ruched detailing adds a touch of texture and visual interest, while the streamlined design ensures it remains timeless. The interior, lined with a subtle striped fabric, offers just enough space for your essentials, making it perfect for evenings out or special occasions. Carry it to a cocktail party, a formal dinner, or even a chic wedding reception to elevate your ensemble with a touch of refined style.\\n\\nPattern: Solid\\nExterior Material: Leather\\nDepartment: Women\\nExterior Color: Black\\nStyle: Clutch\\nAccents: Pleated\\nFeatures: Lined, Ruched\\nFinish: Pebbled\\nClosure: Magnetic\\nModel: Clutch Evening Bag\\nAccessories: Original Dust Bag\\n\\nKeywords: Lauren Merkin, clutch, leather clutch, evening bag, party purse, black clutch, designer clutch, ruched clutch, formal bag, cocktail bag, wedding clutch, small bag, black leather, women's clutch   38.0 [\\n  \"ext1:fe925a6a-2d4d-4e05-9ac1-5683ebede822\"\\n]       False\n",
      "642d7d479464f30da6a14a90                                Victoria's Secret Pajama Long Sleeve Top sz M    True        [\\n  \"domain#us\",\\n  \"brand#victoria's secret\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#00208975d97b4e80ef00a955\",\\n  \"category_feature#00ca9287d97b4e80ef00a955\",\\n  \"color#pink\",\\n  \"style_tag#sleepover\",\\n  \"style_tag#pajama\",\\n  \"style_tag#sleepwear\",\\n  \"size#us m\",\\n  \"created_at#2023-04-05T07:17:40-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Victoria's Secret long sleeve top, pink color, striped pattern, button up, chest pocket with embroidered logo, comes with sleep mask, size medium, great conditions   18.0 [\\n  \"ext1:d7f3d0ad-f908-4246-a92a-e8ac28f4396e\"\\n]       False\n",
      "68275cc99f034c49f1031b25                            Vintage Karen Scott Sport Blue Fleece Size Medium    True                                                                                            [\\n  \"domain#us\",\\n  \"brand#karen scott\",\\n  \"department#20008c10d97b4e1245005764\",\\n  \"category#2d008c10d97b4e1245005764\",\\n  \"category_feature#9c009813d97b4e3995005764\",\\n  \"color#blue\",\\n  \"size#m\",\\n  \"created_at#2025-05-16T08:44:01-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Comfortable material and perfect for the chilly nights, or breezy days.   25.0 [\\n  \"ext1:cb0bb29d-30ae-4e71-8df2-b7239e1e027c\"\\n]       False\n",
      "66dde5121ce666bad8c488e4   COACH 56819 Chelsea SoHo Crossbody Bag Tan Silver Pebble Leather MSRP $375    True        [\\n  \"domain#us\",\\n  \"brand#coach\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#00248975d97b4e80ef00a955\",\\n  \"category_feature#00e09287d97b4e80ef00a955\",\\n  \"color#tan\",\\n  \"color#silver\",\\n  \"style_tag#hobo\",\\n  \"style_tag#soho\",\\n  \"style_tag#crossbody\",\\n  \"size#us os\",\\n  \"created_at#2024-09-05T20:08:36-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            EUC minimal wear on lining \\nPolished pebble leather\\nInside zip pocket\\nZip-top closure, fabric lining\\nHandle with 7 1/2\" drop\\nDetachable strap with 22\" drop for shoulder or crossbody wear\\nCompatible with Coach interchangeable straps\\n10.5” L) x 7 1/2\" (H) x 3\" (W)\\nStyle No. 56819\\nNo dust bag, care book or original tags\\n\\nY2K, 00’s  139.0 [\\n  \"ext1:61bac99e-104d-47ac-963b-3dad685e0209\"\\n]       False\n",
      "6860843a006e435e5f20f21f                                              Nike White and Black Sportswear    True                                                       [\\n  \"domain#us\",\\n  \"brand#nike\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#002e8975d97b4e80ef00a955\",\\n  \"color#white\",\\n  \"color#black\",\\n  \"style_tag#activewear\",\\n  \"style_tag#casual\",\\n  \"style_tag#sporty\",\\n  \"size#l\",\\n  \"created_at#2025-06-28T17:11:32-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Classic Nike sportswear in white with bold black details. Perfect for any activity. BNWT   11.0 [\\n  \"ext1:e01a38b1-35b0-4a1a-a004-0798a90c182e\"\\n]       False\n",
      "67cf0c64ac1b0480b0b4e607        Zanias - Chrysalis (CD, 2023) - NEW - Synthwave, Industrial, Darkwave    True                                                                                                                                 [\\n  \"domain#us\",\\n  \"department#583c7d134024035188906153\",\\n  \"category#488e4884402403bc7f6c6157\",\\n  \"category_feature#e0c8296f402403e02d3d615b\",\\n  \"size#us os\",\\n  \"created_at#2025-03-10T09:05:58-07:00\"\\n]                                                                                 Condition: New\\nDescription:\\nExperience the captivating sounds of Zanias' latest album, \"Chrysalis\" (CD, 2023)! This CD is brand new, factory sealed, and ready to transport you to the darker corners of synthwave, industrial, and darkwave. Immerse yourself in Zanias' powerful vocals and haunting melodies. A must-have for fans of the genre.\\n\\n- New & Sealed: Brand new, factory sealed CD.\\n\\n- 2023 Release: The latest album from Zanias.\\n\\n- Genre: Synthwave, Industrial, Darkwave.\\n\\n- Powerful Vocals: Zanias' distinctive and evocative voice.\\n\\n- Haunting Melodies: Atmospheric and immersive soundscapes.\\n\\nDon't miss out on this compelling release. Add \"Chrysalis\" to your collection today! Fast and secure shipping.\\nKeywords:\\nZanias Chrysalis CD, Zanias CD, Chrysalis CD, 2023 CD, Synthwave CD, Industrial CD, Darkwave CD, New CD, Sealed CD, Music CD, Electronic Music, Gothic Music, Alternative Music, Zanias Album.   10.0 [\\n  \"ext1:5e6a335e-ae6c-4dda-8fcc-30e6498d70a8\"\\n]       False\n",
      "6721609c93a13db747ef996e Soma Memorable Front Closure Racerback Bra 34C Women Black Adjustable Straps    True                          [\\n  \"domain#us\",\\n  \"brand#soma\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#00208975d97b4e80ef00a955\",\\n  \"category_feature#00c29287d97b4e80ef00a955\",\\n  \"color#black\",\\n  \"style_tag#soma\",\\n  \"style_tag#racerback\",\\n  \"style_tag#bra\",\\n  \"size#us 34c\",\\n  \"created_at#2024-10-29T15:24:39-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                       Soma Memorable Front Closure Racerback Bra 34C Black Women Adjustable Straps\\nWomen\\nSize-34C\\nColor-Black\\nUnderwire\\nLined\\nLightweight\\nSexy\\nSmooth\\nSoft\\nComfy\\nNight Out\\nAdjustable Straps\\nLingerie\\nFront hook closure\\nFabric-Body-Nylon/Spandex\\nCare-Hand Wash\\nCondition: Pre-owned\\nMEASUREMENTS are approximate and may vary by 2-3 cm, be sure of fit before purchasing\\nSeason: Spring, Summer, Fall, Winter\\nReview the description and pictures before purchasing as the pictures are considered part of the description. There may be minimal flaws that I missed at the time of listing and the color/shade may vary slightly on different monitors   17.0 [\\n  \"ext1:26ded45b-a2dd-4af8-8b5d-bcb4d27c5c4a\"\\n]       False\n",
      "66f4a352981d480a295b16cd      Vintage Swing Top Baby Girls Sz 3-6m Pink Turtledove 60/70s Embroidered    True                                                                    [\\n  \"domain#us\",\\n  \"brand#vintage\",\\n  \"department#20008c10d97b4e1245005764\",\\n  \"category#2e008c10d97b4e1245005764\",\\n  \"color#pink\",\\n  \"style_tag#all\",\\n  \"style_tag#baby top\",\\n  \"style_tag#baby tops\",\\n  \"size#us 3-6\",\\n  \"created_at#2024-09-25T16:57:10-07:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Condition: Good Used Condition: small pin size hole on back near buttons \\n\\nVintage pink turtledove embroidered swing top. Size 3-6m \\n\\nFlat Measurements: \\n\\nPit-pit: 10\" \\n\\nShoulder to Hem: 9.25\" \\n\\n1.3 oz \\n\\n(H)   10.0 [\\n  \"ext1:7f4f6bd5-0ad3-4dbc-8eac-83c18cd21487\"\\n]       False\n",
      "65e06413253a8cfe0308d86f          Corset beaded scalloped ruffle prom glitz pageant dress gown 6 8 10    True                                                                 [\\n  \"domain#us\",\\n  \"brand#cassandra stone\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#00108975d97b4e80ef00a955\",\\n  \"category_feature#010082cfd97b4ecc3f0056aa\",\\n  \"color#yellow\",\\n  \"color#gold\",\\n  \"size#us 10\",\\n  \"created_at#2024-02-29T03:01:57-08:00\"\\n]                                                                                                                                                                                                                                                                                                                           Gorgeous heavy bling lace sequin bead detail\\ngold with some metallic silver lace detail\\nCassanda Stone Corset beaded scalloped ruffle prom pageant dress \\nspaghetti straps or can be removed or tucked in for strapless \\nsays sz 10 but I feel it would better fit 6 8\\nNwot store sample, tried on but not worn\\nmay have some loose or missing beads but nothing noticeable \\ndress is not steamed in the photo, will look even better once you steamed it out\\nall sales final\\n\\nUnique, elegant, high end and can be yours!\\n\\ncheck out my other gown listings for Jovani terani mac duggal tarik ediz fouad sarkis Rachel allan portia and Scarlett sherri hill Tony Bowls and more\\ngold mermaidcore gown  449.0 [\\n  \"ext1:ce276098-53f5-4e24-8e4a-46bd87876fd8\"\\n]       False\n",
      "65a93b543b982a84b8491c4e                                             MOTHERHOOD:BLUE/WHITE STRIPE TOP    True                                                [\\n  \"domain#us\",\\n  \"brand#motherhood\",\\n  \"department#000e8975d97b4e80ef00a955\",\\n  \"category#002e8975d97b4e80ef00a955\",\\n  \"color#blue\",\\n  \"color#white\",\\n  \"style_tag#casual\",\\n  \"style_tag#neutral\",\\n  \"style_tag#relaxed fit\",\\n  \"size#l\",\\n  \"created_at#2024-01-18T07:00:49-08:00\"\\n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     HELLO mothers this is the comfortable top that you have been looking for!!!   15.0 [\\n  \"ext1:c91e88e1-2e9a-4b85-be09-9f81ad7dff4e\"\\n]       False\n",
      "\n",
      "Data types:\n",
      "PRODUCT_ID      object\n",
      "NAME            object\n",
      "ACTIVE            bool\n",
      "CATEGORIES      object\n",
      "DESCRIPTION     object\n",
      "PRICE          float64\n",
      "VENDORS         object\n",
      "IS_DELETED        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM CATALOG\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "columns = ['PRODUCT_ID', 'NAME', 'ACTIVE', 'CATEGORIES', 'DESCRIPTION', 'PRICE', 'VENDORS', 'IS_DELETED']\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Got {len(df)} rows from PRODUCT_CATALOG\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Product Catalog Processing Configuration ---\n",
      "Analysis Period: 2025-03-14 to 2025-09-07\n",
      "Output Directory: /Users/pranjal/Code/marketplace-incrementality/daily_summaries/data/product_catalog_processed\n",
      "=== PROCESSING PRODUCT CATALOG DATA ===\n",
      "Using SQL-based approach: Purchases + Clicks -> Catalog Join -> Process -> Save\n",
      "Resuming from checkpoint: 12,252,147 products already processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:   0%|          | 0/178 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2025-03-14 - already processed\n",
      "Skipping 2025-03-15 - already processed\n",
      "Skipping 2025-03-16 - already processed\n",
      "Skipping 2025-03-17 - already processed\n",
      "Skipping 2025-03-18 - already processed\n",
      "Skipping 2025-03-19 - already processed\n",
      "Skipping 2025-03-20 - already processed\n",
      "Skipping 2025-03-21 - already processed\n",
      "Skipping 2025-03-22 - already processed\n",
      "Skipping 2025-03-23 - already processed\n",
      "Skipping 2025-03-24 - already processed\n",
      "Skipping 2025-03-25 - already processed\n",
      "Skipping 2025-03-26 - already processed\n",
      "Skipping 2025-03-27 - already processed\n",
      "Skipping 2025-03-28 - already processed\n",
      "Skipping 2025-03-29 - already processed\n",
      "Skipping 2025-03-30 - already processed\n",
      "Skipping 2025-03-31 - already processed\n",
      "Skipping 2025-04-01 - already processed\n",
      "Skipping 2025-04-02 - already processed\n",
      "Skipping 2025-04-03 - already processed\n",
      "Skipping 2025-04-04 - already processed\n",
      "Skipping 2025-04-05 - already processed\n",
      "Skipping 2025-04-06 - already processed\n",
      "Skipping 2025-04-07 - already processed\n",
      "Skipping 2025-04-08 - already processed\n",
      "Skipping 2025-04-09 - already processed\n",
      "Skipping 2025-04-10 - already processed\n",
      "Skipping 2025-04-11 - already processed\n",
      "Skipping 2025-04-12 - already processed\n",
      "Skipping 2025-04-13 - already processed\n",
      "Skipping 2025-04-14 - already processed\n",
      "Skipping 2025-04-15 - already processed\n",
      "Skipping 2025-04-16 - already processed\n",
      "Skipping 2025-04-17 - already processed\n",
      "Skipping 2025-04-18 - already processed\n",
      "Skipping 2025-04-19 - already processed\n",
      "Skipping 2025-04-20 - already processed\n",
      "Skipping 2025-04-21 - already processed\n",
      "Skipping 2025-04-22 - already processed\n",
      "Skipping 2025-04-23 - already processed\n",
      "Skipping 2025-04-24 - already processed\n",
      "Skipping 2025-04-25 - already processed\n",
      "Skipping 2025-04-26 - already processed\n",
      "Skipping 2025-04-27 - already processed\n",
      "Skipping 2025-04-28 - already processed\n",
      "Skipping 2025-04-29 - already processed\n",
      "Skipping 2025-04-30 - already processed\n",
      "Skipping 2025-05-01 - already processed\n",
      "Skipping 2025-05-02 - already processed\n",
      "Skipping 2025-05-03 - already processed\n",
      "Skipping 2025-05-04 - already processed\n",
      "Skipping 2025-05-05 - already processed\n",
      "Skipping 2025-05-06 - already processed\n",
      "Skipping 2025-05-07 - already processed\n",
      "Skipping 2025-05-08 - already processed\n",
      "Skipping 2025-05-09 - already processed\n",
      "Skipping 2025-05-10 - already processed\n",
      "Skipping 2025-05-11 - already processed\n",
      "Skipping 2025-05-12 - already processed\n",
      "Skipping 2025-05-13 - already processed\n",
      "Skipping 2025-05-14 - already processed\n",
      "Skipping 2025-05-15 - already processed\n",
      "Skipping 2025-05-16 - already processed\n",
      "Skipping 2025-05-17 - already processed\n",
      "Skipping 2025-05-18 - already processed\n",
      "Skipping 2025-05-19 - already processed\n",
      "Skipping 2025-05-20 - already processed\n",
      "Skipping 2025-05-21 - already processed\n",
      "Skipping 2025-05-22 - already processed\n",
      "Skipping 2025-05-23 - already processed\n",
      "Skipping 2025-05-24 - already processed\n",
      "Skipping 2025-05-25 - already processed\n",
      "Skipping 2025-05-26 - already processed\n",
      "Skipping 2025-05-27 - already processed\n",
      "Skipping 2025-05-28 - already processed\n",
      "Skipping 2025-05-29 - already processed\n",
      "Skipping 2025-05-30 - already processed\n",
      "Skipping 2025-05-31 - already processed\n",
      "Skipping 2025-06-01 - already processed\n",
      "Skipping 2025-06-02 - already processed\n",
      "Skipping 2025-06-03 - already processed\n",
      "Skipping 2025-06-04 - already processed\n",
      "Skipping 2025-06-05 - already processed\n",
      "Skipping 2025-06-06 - already processed\n",
      "Skipping 2025-06-07 - already processed\n",
      "Skipping 2025-06-08 - already processed\n",
      "Skipping 2025-06-09 - already processed\n",
      "Skipping 2025-06-10 - already processed\n",
      "Skipping 2025-06-11 - already processed\n",
      "Skipping 2025-06-12 - already processed\n",
      "Skipping 2025-06-13 - already processed\n",
      "Skipping 2025-06-14 - already processed\n",
      "Skipping 2025-06-15 - already processed\n",
      "Skipping 2025-06-16 - already processed\n",
      "Skipping 2025-06-17 - already processed\n",
      "Skipping 2025-06-18 - already processed\n",
      "Skipping 2025-06-19 - already processed\n",
      "Skipping 2025-06-20 - already processed\n",
      "Skipping 2025-06-21 - already processed\n",
      "Skipping 2025-06-22 - already processed\n",
      "Skipping 2025-06-23 - already processed\n",
      "Skipping 2025-06-24 - already processed\n",
      "Skipping 2025-06-25 - already processed\n",
      "Skipping 2025-06-26 - already processed\n",
      "Skipping 2025-06-27 - already processed\n",
      "Skipping 2025-06-28 - already processed\n",
      "Skipping 2025-06-29 - already processed\n",
      "Skipping 2025-06-30 - already processed\n",
      "Skipping 2025-07-01 - already processed\n",
      "Skipping 2025-07-02 - already processed\n",
      "Skipping 2025-07-03 - already processed\n",
      "Skipping 2025-07-04 - already processed\n",
      "Skipping 2025-07-05 - already processed\n",
      "Skipping 2025-07-06 - already processed\n",
      "Skipping 2025-07-07 - already processed\n",
      "Skipping 2025-07-08 - already processed\n",
      "Skipping 2025-07-09 - already processed\n",
      "Skipping 2025-07-10 - already processed\n",
      "Skipping 2025-07-11 - already processed\n",
      "Skipping 2025-07-12 - already processed\n",
      "Skipping 2025-07-13 - already processed\n",
      "Skipping 2025-07-14 - already processed\n",
      "Skipping 2025-07-15 - already processed\n",
      "Skipping 2025-07-16 - already processed\n",
      "Skipping 2025-07-17 - already processed\n",
      "Skipping 2025-07-18 - already processed\n",
      "Skipping 2025-07-19 - already processed\n",
      "Skipping 2025-07-20 - already processed\n",
      "Skipping 2025-07-21 - already processed\n",
      "Skipping 2025-07-22 - already processed\n",
      "Skipping 2025-07-23 - already processed\n",
      "Skipping 2025-07-24 - already processed\n",
      "Skipping 2025-07-25 - already processed\n",
      "Skipping 2025-07-26 - already processed\n",
      "Skipping 2025-07-27 - already processed\n",
      "Skipping 2025-07-28 - already processed\n",
      "Skipping 2025-07-29 - already processed\n",
      "Skipping 2025-07-30 - already processed\n",
      "Skipping 2025-07-31 - already processed\n",
      "Skipping 2025-08-01 - already processed\n",
      "Skipping 2025-08-02 - already processed\n",
      "Skipping 2025-08-03 - already processed\n",
      "Skipping 2025-08-04 - already processed\n",
      "Skipping 2025-08-05 - already processed\n",
      "Skipping 2025-08-06 - already processed\n",
      "Skipping 2025-08-07 - already processed\n",
      "Skipping 2025-08-08 - already processed\n",
      "Processing 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  84%|████████▎ | 149/178 [01:43<00:20,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 541,888 products, 91,295 new\n",
      "Processing 2025-08-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  84%|████████▍ | 150/178 [03:26<00:46,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 570,136 products, 87,237 new\n",
      "Processing 2025-08-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  85%|████████▍ | 151/178 [05:04<01:19,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 521,520 products, 74,963 new\n",
      "Processing 2025-08-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  85%|████████▌ | 152/178 [06:38<02:01,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 515,236 products, 74,333 new\n",
      "Processing 2025-08-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  86%|████████▌ | 153/178 [08:05<02:50,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 522,194 products, 73,307 new\n",
      "Processing 2025-08-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  87%|████████▋ | 154/178 [09:26<03:47,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 522,600 products, 72,662 new\n",
      "Processing 2025-08-15...\n",
      "  Found 528,320 products, 73,416 new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  87%|████████▋ | 155/178 [10:44<04:54, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 12,799,360 products processed\n",
      "Processing 2025-08-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  88%|████████▊ | 156/178 [12:10<06:27, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 560,067 products, 77,963 new\n",
      "Processing 2025-08-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  88%|████████▊ | 157/178 [14:06<09:06, 26.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 582,709 products, 79,648 new\n",
      "Processing 2025-08-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  89%|████████▉ | 158/178 [18:41<17:39, 52.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 526,908 products, 70,271 new\n",
      "Processing 2025-08-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  89%|████████▉ | 159/178 [20:07<18:10, 57.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 535,157 products, 72,720 new\n",
      "Processing 2025-08-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  90%|████████▉ | 160/178 [21:37<18:48, 62.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 520,498 products, 70,121 new\n",
      "Processing 2025-08-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  90%|█████████ | 161/178 [23:06<19:08, 67.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 514,541 products, 68,864 new\n",
      "Processing 2025-08-22...\n",
      "  Found 516,580 products, 68,767 new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  91%|█████████ | 162/178 [24:41<19:35, 73.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 13,307,714 products processed\n",
      "Processing 2025-08-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  92%|█████████▏| 163/178 [27:46<24:47, 99.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 548,485 products, 75,061 new\n",
      "Processing 2025-08-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  92%|█████████▏| 164/178 [30:44<27:44, 118.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 577,124 products, 79,140 new\n",
      "Processing 2025-08-25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  93%|█████████▎| 165/178 [33:47<29:23, 135.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 521,781 products, 69,546 new\n",
      "Processing 2025-08-26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  93%|█████████▎| 166/178 [35:22<24:54, 124.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 519,026 products, 70,372 new\n",
      "Processing 2025-08-27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  94%|█████████▍| 167/178 [36:54<21:09, 115.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 519,433 products, 69,801 new\n",
      "Processing 2025-08-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  94%|█████████▍| 168/178 [38:22<17:57, 107.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 511,034 products, 68,688 new\n",
      "Processing 2025-08-29...\n",
      "  Found 534,340 products, 72,244 new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  95%|█████████▍| 169/178 [39:59<15:40, 104.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 13,812,566 products processed\n",
      "Processing 2025-08-30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  96%|█████████▌| 170/178 [41:38<13:42, 102.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 568,998 products, 77,691 new\n",
      "Processing 2025-08-31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  96%|█████████▌| 171/178 [43:12<11:42, 100.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 588,197 products, 79,572 new\n",
      "Processing 2025-09-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  97%|█████████▋| 172/178 [44:48<09:54, 99.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 588,693 products, 80,236 new\n",
      "Processing 2025-09-02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  97%|█████████▋| 173/178 [46:19<08:03, 96.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 527,059 products, 69,985 new\n",
      "Processing 2025-09-03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  98%|█████████▊| 174/178 [47:54<06:24, 96.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 533,723 products, 73,545 new\n",
      "Processing 2025-09-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  98%|█████████▊| 175/178 [49:25<04:43, 94.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 517,107 products, 73,173 new\n",
      "Processing 2025-09-05...\n",
      "  Found 531,857 products, 77,036 new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  99%|█████████▉| 176/178 [51:01<03:10, 95.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 14,343,804 products processed\n",
      "Processing 2025-09-06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data:  99%|█████████▉| 177/178 [52:37<01:35, 95.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 607,262 products, 88,407 new\n",
      "Processing 2025-09-07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Catalog Data: 100%|██████████| 178/178 [54:14<00:00, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 635,765 products, 92,794 new\n",
      "\n",
      "=== Creating Final Dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'extraction_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_1239/1428675865.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    258\u001b[39m     print(f\"Incremental update: {start_date} to {end_date}\")\n\u001b[32m    259\u001b[39m     print(f\"Starting with {len(existing_products):,} existing products\")\n\u001b[32m    260\u001b[39m \n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m\"__main__\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     main()\n",
      "\u001b[32m/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_1239/1428675865.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Process data day by day\u001b[39;00m\n\u001b[32m    237\u001b[39m     all_data, daily_stats, processed_products = process_daily_catalog_data()\n\u001b[32m    238\u001b[39m \n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Create final combined dataset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     final_df, summary = create_final_dataset(all_data, daily_stats, processed_products)\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m     print(f\"\\n=== PROCESSING COMPLETE ===\")\n\u001b[32m    243\u001b[39m     print(f\"All output saved to: {OUTPUT_DIR}\")\n",
      "\u001b[32m/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_1239/1428675865.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(all_processed_data, daily_stats, processed_products)\u001b[39m\n\u001b[32m    175\u001b[39m         \u001b[38;5;66;03m# Combine all daily data\u001b[39;00m\n\u001b[32m    176\u001b[39m         df_final = pd.concat(all_processed_data, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    177\u001b[39m \n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# Remove duplicates (keep latest extraction_date for each product)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         df_final = df_final.sort_values(\u001b[33m'extraction_date'\u001b[39m).drop_duplicates(subset=[\u001b[33m'PRODUCT_ID'\u001b[39m], keep=\u001b[33m'last'\u001b[39m)\n\u001b[32m    180\u001b[39m \n\u001b[32m    181\u001b[39m         \u001b[38;5;66;03m# Save final combined dataset\u001b[39;00m\n\u001b[32m    182\u001b[39m         final_file = OUTPUT_DIR / \u001b[33m\"product_catalog_final_processed.parquet\"\u001b[39m\n",
      "\u001b[32m~/Code/marketplace-incrementality/venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7194\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7195\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7196\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7197\u001b[39m \n\u001b[32m   7198\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7199\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/Code/marketplace-incrementality/venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'extraction_date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "import snowflake.connector\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "ANALYSIS_START = date(2025, 3, 14)\n",
    "ANALYSIS_END = date(2025, 9, 7)\n",
    "BASE_PATH = Path(\"/Users/pranjal/Code/marketplace-incrementality/daily_summaries/data\")\n",
    "OUTPUT_DIR = BASE_PATH / \"product_catalog_processed\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"--- Product Catalog Processing Configuration ---\")\n",
    "print(f\"Analysis Period: {ANALYSIS_START} to {ANALYSIS_END}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def get_daily_catalog_query(date_str: str, next_date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    SQL query that:\n",
    "    1. Gets product IDs from purchases and clicks for the day\n",
    "    2. Joins with catalog to get product data\n",
    "    3. Processes/cleans categories in SQL\n",
    "    4. Returns clean, processed product catalog data\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    WITH daily_products AS (\n",
    "        -- Get unique product IDs from purchases\n",
    "        SELECT DISTINCT PRODUCT_ID\n",
    "        FROM PURCHASES\n",
    "        WHERE PURCHASED_AT >= '{date_str}'::TIMESTAMP_NTZ \n",
    "        AND PURCHASED_AT < '{next_date_str}'::TIMESTAMP_NTZ\n",
    "        AND PRODUCT_ID IS NOT NULL\n",
    "        \n",
    "        UNION\n",
    "        \n",
    "        -- Get unique product IDs from clicks\n",
    "        SELECT DISTINCT PRODUCT_ID\n",
    "        FROM CLICKS\n",
    "        WHERE OCCURRED_AT >= '{date_str}'::TIMESTAMP_NTZ \n",
    "        AND OCCURRED_AT < '{next_date_str}'::TIMESTAMP_NTZ\n",
    "        AND PRODUCT_ID IS NOT NULL\n",
    "    ),\n",
    "    processed_catalog AS (\n",
    "        SELECT \n",
    "            c.PRODUCT_ID,\n",
    "            c.NAME,\n",
    "            c.PRICE,\n",
    "            c.ACTIVE,\n",
    "            c.IS_DELETED,\n",
    "            c.DESCRIPTION,\n",
    "            -- Extract and clean category data using higher-order functions\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'brand#%'), ''), '#', 2) AS brand,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'department#%'), ''), '#', 2) AS department_id,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category#%'), ''), '#', 2) AS category_id,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'category_feature#%'), ''), '#', 2) AS category_feature_id,\n",
    "            ARRAY_TO_STRING(TRANSFORM(FILTER(c.CATEGORIES, x -> x LIKE 'color#%'), y -> SPLIT_PART(y, '#', 2)), '|') AS colors,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'size#%'), ''), '#', 2) AS size_info,\n",
    "            ARRAY_TO_STRING(TRANSFORM(FILTER(c.CATEGORIES, x -> x LIKE 'style_tag#%'), y -> SPLIT_PART(y, '#', 2)), '|') AS style_tags,\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'domain#%'), ''), '#', 2) AS domain,\n",
    "            -- Extract created_at from categories\n",
    "            SPLIT_PART(ARRAY_TO_STRING(FILTER(c.CATEGORIES, x -> x LIKE 'created_at#%'), ''), '#', 2) AS product_created_at,\n",
    "            -- Category counts\n",
    "            ARRAY_SIZE(c.CATEGORIES) AS total_categories,\n",
    "            ARRAY_SIZE(FILTER(c.CATEGORIES, x -> x LIKE 'color#%')) AS color_count,\n",
    "            ARRAY_SIZE(FILTER(c.CATEGORIES, x -> x LIKE 'style_tag#%')) AS style_tag_count,\n",
    "            -- Data quality flags\n",
    "            CASE WHEN c.PRICE <= 0 OR c.PRICE > 10000 THEN TRUE ELSE FALSE END AS price_outlier,\n",
    "            CASE WHEN LENGTH(c.NAME) < 5 THEN TRUE ELSE FALSE END AS short_name,\n",
    "            CASE WHEN c.DESCRIPTION IS NULL OR LENGTH(c.DESCRIPTION) < 10 THEN TRUE ELSE FALSE END AS poor_description,\n",
    "            -- Extract vendors (assuming it's an array)\n",
    "            c.VENDORS,\n",
    "            -- Add extraction metadata\n",
    "            '{date_str}'::DATE AS extraction_date,\n",
    "            CURRENT_TIMESTAMP() AS processed_at\n",
    "        FROM CATALOG c\n",
    "        INNER JOIN daily_products dp ON c.PRODUCT_ID = dp.PRODUCT_ID\n",
    "    )\n",
    "    SELECT * FROM processed_catalog\n",
    "    \"\"\"\n",
    "\n",
    "def process_daily_catalog_data():\n",
    "    \"\"\"Process catalog data day by day, merging SQL operations\"\"\"\n",
    "\n",
    "    date_list = pd.date_range(start=ANALYSIS_START, end=ANALYSIS_END, freq='D')\n",
    "    all_processed_data = []\n",
    "    processed_products = set()\n",
    "    daily_stats = []\n",
    "\n",
    "    # Check for existing processed data\n",
    "    checkpoint_file = OUTPUT_DIR / \"processed_products_checkpoint.json\"\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            processed_products = set(checkpoint_data.get('processed_products', []))\n",
    "            print(f\"Resuming from checkpoint: {len(processed_products):,} products already processed\")\n",
    "\n",
    "    for current_date in tqdm(date_list, desc=\"Processing Daily Catalog Data\"):\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        next_date_str = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Check if this date was already processed\n",
    "        daily_output_file = OUTPUT_DIR / f\"catalog_processed_{date_str}.parquet\"\n",
    "        if daily_output_file.exists():\n",
    "            print(f\"Skipping {date_str} - already processed\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing {date_str}...\")\n",
    "\n",
    "            # Execute the combined SQL query\n",
    "            query = get_daily_catalog_query(date_str, next_date_str)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if results:\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                df_day = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "                # Filter out products we've already processed\n",
    "                new_products = df_day[~df_day['PRODUCT_ID'].isin(processed_products)]\n",
    "\n",
    "                if not new_products.empty:\n",
    "                    # Save daily processed data\n",
    "                    new_products.to_parquet(daily_output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "                    all_processed_data.append(new_products)\n",
    "\n",
    "                    # Update processed products set\n",
    "                    new_product_ids = set(new_products['PRODUCT_ID'].unique())\n",
    "                    processed_products.update(new_product_ids)\n",
    "\n",
    "                    # Track daily statistics\n",
    "                    daily_stats.append({\n",
    "                        'date': date_str,\n",
    "                        'products_found': len(df_day),\n",
    "                        'new_products': len(new_products),\n",
    "                        'cumulative_processed': len(processed_products),\n",
    "                        'has_purchases': len(df_day) > 0,  # Indicates if day had purchase/click data\n",
    "                        'avg_price': float(new_products['PRICE'].mean()) if 'PRICE' in new_products.columns else None,\n",
    "                        'active_products': int(new_products['ACTIVE'].sum()) if 'ACTIVE' in new_products.columns else 0\n",
    "                    })\n",
    "\n",
    "                    print(f\"  Found {len(df_day):,} products, {len(new_products):,} new\")\n",
    "                else:\n",
    "                    print(f\"  No new products found for {date_str}\")\n",
    "            else:\n",
    "                print(f\"  No data found for {date_str}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_str}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save checkpoint every 7 days\n",
    "        if len(daily_stats) % 7 == 0:\n",
    "            checkpoint_data = {\n",
    "                'processed_products': list(processed_products),\n",
    "                'last_processed_date': date_str,\n",
    "                'daily_stats': daily_stats\n",
    "            }\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint_data, f, default=str)\n",
    "            print(f\"Checkpoint saved: {len(processed_products):,} products processed\")\n",
    "\n",
    "    return all_processed_data, daily_stats, processed_products\n",
    "\n",
    "def create_final_dataset(all_processed_data, daily_stats, processed_products):\n",
    "    \"\"\"Combine all daily processed data into final dataset\"\"\"\n",
    "\n",
    "    if all_processed_data:\n",
    "        print(\"\\n=== Creating Final Dataset ===\")\n",
    "\n",
    "        # Combine all daily data\n",
    "        df_final = pd.concat(all_processed_data, ignore_index=True)\n",
    "\n",
    "        # Remove duplicates (keep latest extraction_date for each product)\n",
    "        df_final = df_final.sort_values('extraction_date').drop_duplicates(subset=['PRODUCT_ID'], keep='last')\n",
    "\n",
    "        # Save final combined dataset\n",
    "        final_file = OUTPUT_DIR / \"product_catalog_final_processed.parquet\"\n",
    "        df_final.to_parquet(final_file, index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "        # Create summary statistics\n",
    "        summary_stats = {\n",
    "            'total_unique_products': len(df_final),\n",
    "            'total_processed_products': len(processed_products),\n",
    "            'active_products': int(df_final['ACTIVE'].sum()),\n",
    "            'deleted_products': int(df_final['IS_DELETED'].sum()),\n",
    "            'price_statistics': df_final['PRICE'].describe().to_dict(),\n",
    "            'coverage_by_category': {\n",
    "                'with_brand': int(df_final['BRAND'].notna().sum()),\n",
    "                'with_colors': int(df_final['COLORS'].notna().sum()),\n",
    "                'with_style_tags': int(df_final['STYLE_TAGS'].notna().sum()),\n",
    "                'with_size': int(df_final['SIZE_INFO'].notna().sum())\n",
    "            },\n",
    "            'data_quality': {\n",
    "                'price_outliers': int(df_final['PRICE_OUTLIER'].sum()),\n",
    "                'short_names': int(df_final['SHORT_NAME'].sum()),\n",
    "                'poor_descriptions': int(df_final['POOR_DESCRIPTION'].sum())\n",
    "            },\n",
    "            'daily_processing_stats': daily_stats,\n",
    "            'processing_period': f\"{ANALYSIS_START} to {ANALYSIS_END}\",\n",
    "            'final_processing_timestamp': time.time()\n",
    "        }\n",
    "\n",
    "        # Save summary\n",
    "        with open(OUTPUT_DIR / \"processing_summary.json\", 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "        print(f\"Final dataset created: {df_final.shape[0]:,} products\")\n",
    "        print(f\"Saved to: {final_file}\")\n",
    "\n",
    "        # Display key statistics\n",
    "        print(f\"\\n=== FINAL STATISTICS ===\")\n",
    "        print(f\"Total unique products: {len(df_final):,}\")\n",
    "        print(f\"Active products: {df_final['ACTIVE'].sum():,}\")\n",
    "        print(f\"Products with brands: {df_final['BRAND'].notna().sum():,}\")\n",
    "        print(f\"Products with colors: {df_final['COLORS'].notna().sum():,}\")\n",
    "        print(f\"Average price: ${df_final['PRICE'].mean():.2f}\")\n",
    "        print(f\"Price outliers: {df_final['PRICE_OUTLIER'].sum():,}\")\n",
    "\n",
    "        return df_final, summary_stats\n",
    "\n",
    "    else:\n",
    "        print(\"No data to process\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "\n",
    "    print(\"=== PROCESSING PRODUCT CATALOG DATA ===\")\n",
    "    print(\"Using SQL-based approach: Purchases + Clicks -> Catalog Join -> Process -> Save\")\n",
    "\n",
    "    # Process data day by day\n",
    "    all_data, daily_stats, processed_products = process_daily_catalog_data()\n",
    "\n",
    "    # Create final combined dataset\n",
    "    final_df, summary = create_final_dataset(all_data, daily_stats, processed_products)\n",
    "\n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"All output saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# For incremental updates (new data)\n",
    "def process_incremental_update(start_date: date, end_date: date):\n",
    "    \"\"\"Process only new dates that haven't been processed before\"\"\"\n",
    "\n",
    "    # Load existing processed products\n",
    "    checkpoint_file = OUTPUT_DIR / \"processed_products_checkpoint.json\"\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            existing_products = set(checkpoint_data.get('processed_products', []))\n",
    "    else:\n",
    "        existing_products = set()\n",
    "\n",
    "    print(f\"Incremental update: {start_date} to {end_date}\")\n",
    "    print(f\"Starting with {len(existing_products):,} existing products\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Complete the incomplete process_incremental_update function\n",
    "def process_incremental_update(start_date: date, end_date: date):\n",
    "    \"\"\"Process only new dates that haven't been processed before\"\"\"\n",
    "\n",
    "    # Load existing processed products from checkpoint\n",
    "    checkpoint_file = OUTPUT_DIR / \"processed_products_checkpoint.json\"\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            existing_products = set(checkpoint_data.get('processed_products', []))\n",
    "            existing_daily_stats = checkpoint_data.get('daily_stats', [])\n",
    "    else:\n",
    "        existing_products = set()\n",
    "        existing_daily_stats = []\n",
    "\n",
    "    print(f\"Incremental update: {start_date} to {end_date}\")\n",
    "    print(f\"Starting with {len(existing_products):,} existing products\")\n",
    "\n",
    "    # Process only the new date range\n",
    "    date_list = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    new_processed_data = []\n",
    "    updated_stats = existing_daily_stats.copy()\n",
    "\n",
    "    for current_date in tqdm(date_list, desc=\"Processing Remaining Days\"):\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        next_date_str = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Check if this date was already processed\n",
    "        daily_output_file = OUTPUT_DIR / f\"catalog_processed_{date_str}.parquet\"\n",
    "        if daily_output_file.exists():\n",
    "            print(f\"Skipping {date_str} - already processed\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing {date_str}...\")\n",
    "\n",
    "            # Execute the combined SQL query\n",
    "            query = get_daily_catalog_query(date_str, next_date_str)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if results:\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                df_day = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "                # Filter out products we've already processed\n",
    "                new_products = df_day[~df_day['PRODUCT_ID'].isin(existing_products)]\n",
    "\n",
    "                if not new_products.empty:\n",
    "                    # Save daily processed data\n",
    "                    new_products.to_parquet(daily_output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "                    new_processed_data.append(new_products)\n",
    "\n",
    "                    # Update processed products set\n",
    "                    new_product_ids = set(new_products['PRODUCT_ID'].unique())\n",
    "                    existing_products.update(new_product_ids)\n",
    "\n",
    "                    # Track daily statistics\n",
    "                    updated_stats.append({\n",
    "                        'date': date_str,\n",
    "                        'products_found': len(df_day),\n",
    "                        'new_products': len(new_products),\n",
    "                        'cumulative_processed': len(existing_products),\n",
    "                        'has_purchases': len(df_day) > 0,\n",
    "                        'avg_price': float(new_products['PRICE'].mean()) if 'PRICE' in new_products.columns else None,\n",
    "                        'active_products': int(new_products['ACTIVE'].sum()) if 'ACTIVE' in new_products.columns else 0\n",
    "                    })\n",
    "\n",
    "                    print(f\"  Found {len(df_day):,} products, {len(new_products):,} new\")\n",
    "                else:\n",
    "                    print(f\"  No new products found for {date_str}\")\n",
    "            else:\n",
    "                print(f\"  No data found for {date_str}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_str}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save checkpoint every 7 days\n",
    "        if len([s for s in updated_stats if s['date'] >= start_date.strftime('%Y-%m-%d')]) % 7 == 0:\n",
    "            checkpoint_data = {\n",
    "                'processed_products': list(existing_products),\n",
    "                'last_processed_date': date_str,\n",
    "                'daily_stats': updated_stats\n",
    "            }\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint_data, f, default=str)\n",
    "            print(f\"Checkpoint saved: {len(existing_products):,} products processed\")\n",
    "\n",
    "    # Final checkpoint update\n",
    "    checkpoint_data = {\n",
    "        'processed_products': list(existing_products),\n",
    "        'last_processed_date': end_date.strftime('%Y-%m-%d'),\n",
    "        'daily_stats': updated_stats\n",
    "    }\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint_data, f, default=str)\n",
    "\n",
    "    print(f\"\\nIncremental update complete!\")\n",
    "    print(f\"Total products now: {len(existing_products):,}\")\n",
    "    print(f\"New products added: {sum(len(df) for df in new_processed_data):,}\")\n",
    "\n",
    "    # Create updated final dataset if we processed new data\n",
    "    if new_processed_data:\n",
    "        create_final_dataset(new_processed_data, updated_stats, existing_products)\n",
    "\n",
    "    return new_processed_data, updated_stats, existing_products\n",
    "\n",
    "# Resume processing from where you left off (2025-08-09 to 2025-09-07)\n",
    "process_incremental_update(date(2025, 8, 9), date(2025, 9, 7))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
